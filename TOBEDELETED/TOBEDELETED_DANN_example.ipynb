{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import entropy, kurtosis\n",
    "from config.transformers import *\n",
    "from config.models import *\n",
    "from config.loss_functions import *\n",
    "from config.validation import *\n",
    "from config.regressors import *\n",
    "import numpy as np\n",
    "import pyriemann\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "DATASET = 'freemoves' # change this to guided/freemoves if needed\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "tw_extractor = TimeWindowTransformer(size = 500, step = 50)\n",
    "label_extractor = LabelWindowExtractor(size = 500, step = 50)\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 | Train on [1, 2, 3, 4], Validate on 0 ===\n",
      "Epoch 01 | Train Loss: 656.3550 | Val RMSE: 24.1060 | Dom Train Acc: 38.71%\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train Loss: 601.7190 | Val RMSE: 23.0802 | Dom Train Acc: 41.36%\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train Loss: 535.3066 | Val RMSE: 21.8700 | Dom Train Acc: 47.94%\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train Loss: 469.7959 | Val RMSE: 19.8016 | Dom Train Acc: 48.97%\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train Loss: 405.1439 | Val RMSE: 19.7694 | Dom Train Acc: 50.60%\n",
      "           λ = 0.500\n",
      "Epoch 06 | Train Loss: 344.8084 | Val RMSE: 18.4050 | Dom Train Acc: 50.66%\n",
      "           λ = 0.600\n",
      "Epoch 07 | Train Loss: 291.3523 | Val RMSE: 18.3262 | Dom Train Acc: 52.48%\n",
      "           λ = 0.700\n",
      "Epoch 08 | Train Loss: 247.0739 | Val RMSE: 15.8564 | Dom Train Acc: 53.44%\n",
      "           λ = 0.800\n",
      "Epoch 09 | Train Loss: 208.1713 | Val RMSE: 14.9946 | Dom Train Acc: 52.37%\n",
      "           λ = 0.900\n",
      "Epoch 10 | Train Loss: 175.3392 | Val RMSE: 13.0051 | Dom Train Acc: 56.10%\n",
      "           λ = 1.000\n",
      "Epoch 11 | Train Loss: 151.4051 | Val RMSE: 12.2957 | Dom Train Acc: 54.36%\n",
      "           λ = 1.000\n",
      "Epoch 12 | Train Loss: 132.1827 | Val RMSE: 12.2616 | Dom Train Acc: 56.33%\n",
      "           λ = 1.000\n",
      "Epoch 13 | Train Loss: 120.1838 | Val RMSE: 11.7817 | Dom Train Acc: 56.62%\n",
      "           λ = 1.000\n",
      "Epoch 14 | Train Loss: 109.2697 | Val RMSE: 11.3638 | Dom Train Acc: 54.63%\n",
      "           λ = 1.000\n",
      "Epoch 15 | Train Loss: 101.8198 | Val RMSE: 11.2873 | Dom Train Acc: 55.57%\n",
      "           λ = 1.000\n",
      "Epoch 16 | Train Loss: 94.7456 | Val RMSE: 11.4460 | Dom Train Acc: 58.76%\n",
      "           λ = 1.000\n",
      "Epoch 17 | Train Loss: 91.8834 | Val RMSE: 10.7923 | Dom Train Acc: 56.57%\n",
      "           λ = 1.000\n",
      "Epoch 18 | Train Loss: 87.1628 | Val RMSE: 10.9608 | Dom Train Acc: 57.69%\n",
      "           λ = 1.000\n",
      "Epoch 19 | Train Loss: 83.7989 | Val RMSE: 11.8871 | Dom Train Acc: 51.09%\n",
      "           λ = 1.000\n",
      "Epoch 20 | Train Loss: 83.6173 | Val RMSE: 10.2549 | Dom Train Acc: 58.12%\n",
      "           λ = 1.000\n",
      "Epoch 21 | Train Loss: 78.7767 | Val RMSE: 10.4922 | Dom Train Acc: 57.49%\n",
      "           λ = 1.000\n",
      "Epoch 22 | Train Loss: 76.7842 | Val RMSE: 10.5558 | Dom Train Acc: 58.07%\n",
      "           λ = 1.000\n",
      "Epoch 23 | Train Loss: 75.4436 | Val RMSE: 10.4231 | Dom Train Acc: 56.14%\n",
      "           λ = 1.000\n",
      "Epoch 24 | Train Loss: 73.9746 | Val RMSE: 10.5181 | Dom Train Acc: 58.53%\n",
      "           λ = 1.000\n",
      "Epoch 25 | Train Loss: 72.3011 | Val RMSE: 10.5396 | Dom Train Acc: 58.53%\n",
      "           λ = 1.000\n",
      "Epoch 26 | Train Loss: 71.2616 | Val RMSE: 9.9400 | Dom Train Acc: 58.81%\n",
      "           λ = 1.000\n",
      "Epoch 27 | Train Loss: 69.7716 | Val RMSE: 10.5498 | Dom Train Acc: 53.52%\n",
      "           λ = 1.000\n",
      "Epoch 28 | Train Loss: 68.8286 | Val RMSE: 10.1980 | Dom Train Acc: 60.62%\n",
      "           λ = 1.000\n",
      "Epoch 29 | Train Loss: 68.0509 | Val RMSE: 10.4970 | Dom Train Acc: 58.29%\n",
      "           λ = 1.000\n",
      "Epoch 30 | Train Loss: 66.6801 | Val RMSE: 9.9956 | Dom Train Acc: 59.83%\n",
      "           λ = 1.000\n",
      "Epoch 31 | Train Loss: 65.2471 | Val RMSE: 10.4031 | Dom Train Acc: 60.22%\n",
      "           λ = 1.000\n",
      "Epoch 32 | Train Loss: 65.4692 | Val RMSE: 10.3082 | Dom Train Acc: 60.31%\n",
      "           λ = 1.000\n",
      "Epoch 33 | Train Loss: 63.5790 | Val RMSE: 10.2527 | Dom Train Acc: 60.55%\n",
      "           λ = 1.000\n",
      "Epoch 34 | Train Loss: 63.0458 | Val RMSE: 10.4168 | Dom Train Acc: 59.53%\n",
      "           λ = 1.000\n",
      "Epoch 35 | Train Loss: 63.7937 | Val RMSE: 11.2931 | Dom Train Acc: 57.29%\n",
      "           λ = 1.000\n",
      "Epoch 36 | Train Loss: 61.9682 | Val RMSE: 10.4011 | Dom Train Acc: 58.38%\n",
      "           λ = 1.000\n",
      "Epoch 37 | Train Loss: 61.4124 | Val RMSE: 10.2957 | Dom Train Acc: 60.87%\n",
      "           λ = 1.000\n",
      "Epoch 38 | Train Loss: 60.1035 | Val RMSE: 10.5880 | Dom Train Acc: 58.03%\n",
      "           λ = 1.000\n",
      "Epoch 39 | Train Loss: 59.0963 | Val RMSE: 10.3322 | Dom Train Acc: 57.42%\n",
      "           λ = 1.000\n",
      "Epoch 40 | Train Loss: 58.9886 | Val RMSE: 10.3220 | Dom Train Acc: 60.99%\n",
      "           λ = 1.000\n",
      "Epoch 41 | Train Loss: 58.3054 | Val RMSE: 10.4756 | Dom Train Acc: 59.04%\n",
      "           λ = 1.000\n",
      "Epoch 42 | Train Loss: 58.5173 | Val RMSE: 10.4775 | Dom Train Acc: 63.01%\n",
      "           λ = 1.000\n",
      "Epoch 43 | Train Loss: 57.5930 | Val RMSE: 10.2292 | Dom Train Acc: 60.37%\n",
      "           λ = 1.000\n",
      "Epoch 44 | Train Loss: 56.5500 | Val RMSE: 9.9741 | Dom Train Acc: 58.43%\n",
      "           λ = 1.000\n",
      "Epoch 45 | Train Loss: 57.2718 | Val RMSE: 10.5503 | Dom Train Acc: 57.60%\n",
      "           λ = 1.000\n",
      "Epoch 46 | Train Loss: 55.8387 | Val RMSE: 10.2048 | Dom Train Acc: 62.13%\n",
      "           λ = 1.000\n",
      "Early stopping triggered.\n",
      "\n",
      "=== Fold 2 | Train on [0, 2, 3, 4], Validate on 1 ===\n",
      "Epoch 01 | Train Loss: 563.7230 | Val RMSE: 30.4615 | Dom Train Acc: 34.34%\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train Loss: 510.0769 | Val RMSE: 28.6313 | Dom Train Acc: 39.01%\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train Loss: 451.9041 | Val RMSE: 27.1594 | Dom Train Acc: 41.15%\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train Loss: 395.6772 | Val RMSE: 25.6441 | Dom Train Acc: 43.82%\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train Loss: 338.0476 | Val RMSE: 24.0963 | Dom Train Acc: 42.87%\n",
      "           λ = 0.500\n",
      "Epoch 06 | Train Loss: 286.3251 | Val RMSE: 21.9918 | Dom Train Acc: 42.22%\n",
      "           λ = 0.600\n",
      "Epoch 07 | Train Loss: 242.5214 | Val RMSE: 20.2207 | Dom Train Acc: 42.14%\n",
      "           λ = 0.700\n",
      "Epoch 08 | Train Loss: 204.7817 | Val RMSE: 18.6282 | Dom Train Acc: 42.12%\n",
      "           λ = 0.800\n",
      "Epoch 09 | Train Loss: 173.7154 | Val RMSE: 17.0279 | Dom Train Acc: 41.23%\n",
      "           λ = 0.900\n",
      "Epoch 10 | Train Loss: 149.9293 | Val RMSE: 16.8070 | Dom Train Acc: 44.03%\n",
      "           λ = 1.000\n",
      "Epoch 11 | Train Loss: 131.4161 | Val RMSE: 16.3892 | Dom Train Acc: 45.01%\n",
      "           λ = 1.000\n",
      "Epoch 12 | Train Loss: 117.4843 | Val RMSE: 14.9481 | Dom Train Acc: 42.39%\n",
      "           λ = 1.000\n",
      "Epoch 13 | Train Loss: 106.2272 | Val RMSE: 15.1262 | Dom Train Acc: 44.78%\n",
      "           λ = 1.000\n",
      "Epoch 14 | Train Loss: 96.5164 | Val RMSE: 14.6320 | Dom Train Acc: 45.97%\n",
      "           λ = 1.000\n",
      "Epoch 15 | Train Loss: 88.9851 | Val RMSE: 14.1217 | Dom Train Acc: 43.93%\n",
      "           λ = 1.000\n",
      "Epoch 16 | Train Loss: 84.0560 | Val RMSE: 14.9462 | Dom Train Acc: 46.66%\n",
      "           λ = 1.000\n",
      "Epoch 17 | Train Loss: 79.8966 | Val RMSE: 13.2866 | Dom Train Acc: 44.55%\n",
      "           λ = 1.000\n",
      "Epoch 18 | Train Loss: 76.7351 | Val RMSE: 13.9065 | Dom Train Acc: 46.64%\n",
      "           λ = 1.000\n",
      "Epoch 19 | Train Loss: 73.7622 | Val RMSE: 14.0945 | Dom Train Acc: 45.84%\n",
      "           λ = 1.000\n",
      "Epoch 20 | Train Loss: 72.1515 | Val RMSE: 15.4960 | Dom Train Acc: 46.74%\n",
      "           λ = 1.000\n",
      "Epoch 21 | Train Loss: 69.2983 | Val RMSE: 13.8920 | Dom Train Acc: 47.41%\n",
      "           λ = 1.000\n",
      "Epoch 22 | Train Loss: 67.1945 | Val RMSE: 14.3471 | Dom Train Acc: 46.82%\n",
      "           λ = 1.000\n",
      "Epoch 23 | Train Loss: 65.4132 | Val RMSE: 14.2367 | Dom Train Acc: 48.80%\n",
      "           λ = 1.000\n",
      "Epoch 24 | Train Loss: 64.2702 | Val RMSE: 14.7885 | Dom Train Acc: 49.54%\n",
      "           λ = 1.000\n",
      "Epoch 25 | Train Loss: 63.0892 | Val RMSE: 15.7015 | Dom Train Acc: 49.32%\n",
      "           λ = 1.000\n",
      "Epoch 26 | Train Loss: 62.3831 | Val RMSE: 14.3238 | Dom Train Acc: 50.00%\n",
      "           λ = 1.000\n",
      "Epoch 27 | Train Loss: 61.6979 | Val RMSE: 14.9548 | Dom Train Acc: 47.83%\n",
      "           λ = 1.000\n",
      "Epoch 28 | Train Loss: 60.3932 | Val RMSE: 14.4218 | Dom Train Acc: 50.26%\n",
      "           λ = 1.000\n",
      "Epoch 29 | Train Loss: 58.7809 | Val RMSE: 14.7154 | Dom Train Acc: 51.00%\n",
      "           λ = 1.000\n",
      "Epoch 30 | Train Loss: 58.4453 | Val RMSE: 14.8971 | Dom Train Acc: 51.48%\n",
      "           λ = 1.000\n",
      "Epoch 31 | Train Loss: 58.2777 | Val RMSE: 13.5834 | Dom Train Acc: 47.26%\n",
      "           λ = 1.000\n",
      "Epoch 32 | Train Loss: 58.1615 | Val RMSE: 14.6970 | Dom Train Acc: 50.14%\n",
      "           λ = 1.000\n",
      "Epoch 33 | Train Loss: 56.3147 | Val RMSE: 13.6282 | Dom Train Acc: 50.25%\n",
      "           λ = 1.000\n",
      "Epoch 34 | Train Loss: 55.2254 | Val RMSE: 15.6729 | Dom Train Acc: 52.03%\n",
      "           λ = 1.000\n",
      "Epoch 35 | Train Loss: 55.3457 | Val RMSE: 14.3505 | Dom Train Acc: 52.64%\n",
      "           λ = 1.000\n",
      "Epoch 36 | Train Loss: 55.0745 | Val RMSE: 14.4534 | Dom Train Acc: 49.84%\n",
      "           λ = 1.000\n",
      "Epoch 37 | Train Loss: 54.7029 | Val RMSE: 13.9667 | Dom Train Acc: 54.23%\n",
      "           λ = 1.000\n",
      "Early stopping triggered.\n",
      "\n",
      "=== Fold 3 | Train on [0, 1, 3, 4], Validate on 2 ===\n",
      "Epoch 01 | Train Loss: 642.9187 | Val RMSE: 25.4986 | Dom Train Acc: 39.72%\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train Loss: 584.8029 | Val RMSE: 22.4836 | Dom Train Acc: 45.85%\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train Loss: 519.9186 | Val RMSE: 23.7408 | Dom Train Acc: 46.26%\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train Loss: 452.9855 | Val RMSE: 19.0165 | Dom Train Acc: 42.91%\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train Loss: 395.0206 | Val RMSE: 18.3685 | Dom Train Acc: 44.20%\n",
      "           λ = 0.500\n",
      "Epoch 06 | Train Loss: 336.7671 | Val RMSE: 19.0735 | Dom Train Acc: 48.75%\n",
      "           λ = 0.600\n",
      "Epoch 07 | Train Loss: 285.4036 | Val RMSE: 17.1651 | Dom Train Acc: 50.51%\n",
      "           λ = 0.700\n",
      "Epoch 08 | Train Loss: 243.3700 | Val RMSE: 15.7746 | Dom Train Acc: 51.96%\n",
      "           λ = 0.800\n",
      "Epoch 09 | Train Loss: 207.6716 | Val RMSE: 16.6652 | Dom Train Acc: 53.98%\n",
      "           λ = 0.900\n",
      "Epoch 10 | Train Loss: 178.4454 | Val RMSE: 15.6384 | Dom Train Acc: 51.67%\n",
      "           λ = 1.000\n",
      "Epoch 11 | Train Loss: 153.3350 | Val RMSE: 12.7532 | Dom Train Acc: 53.39%\n",
      "           λ = 1.000\n",
      "Epoch 12 | Train Loss: 134.4600 | Val RMSE: 12.5146 | Dom Train Acc: 55.62%\n",
      "           λ = 1.000\n",
      "Epoch 13 | Train Loss: 118.6165 | Val RMSE: 12.0609 | Dom Train Acc: 56.84%\n",
      "           λ = 1.000\n",
      "Epoch 14 | Train Loss: 107.7183 | Val RMSE: 11.7542 | Dom Train Acc: 56.54%\n",
      "           λ = 1.000\n",
      "Epoch 15 | Train Loss: 99.8870 | Val RMSE: 13.0627 | Dom Train Acc: 56.08%\n",
      "           λ = 1.000\n",
      "Epoch 16 | Train Loss: 94.3583 | Val RMSE: 11.1711 | Dom Train Acc: 57.07%\n",
      "           λ = 1.000\n",
      "Epoch 17 | Train Loss: 88.1222 | Val RMSE: 11.4857 | Dom Train Acc: 55.22%\n",
      "           λ = 1.000\n",
      "Epoch 18 | Train Loss: 84.6308 | Val RMSE: 11.3579 | Dom Train Acc: 57.41%\n",
      "           λ = 1.000\n",
      "Epoch 19 | Train Loss: 79.7940 | Val RMSE: 10.9811 | Dom Train Acc: 57.10%\n",
      "           λ = 1.000\n",
      "Epoch 20 | Train Loss: 78.8161 | Val RMSE: 11.1961 | Dom Train Acc: 58.42%\n",
      "           λ = 1.000\n",
      "Epoch 21 | Train Loss: 76.2337 | Val RMSE: 10.6414 | Dom Train Acc: 56.87%\n",
      "           λ = 1.000\n",
      "Epoch 22 | Train Loss: 74.4596 | Val RMSE: 11.1421 | Dom Train Acc: 58.53%\n",
      "           λ = 1.000\n",
      "Epoch 23 | Train Loss: 71.9668 | Val RMSE: 10.7674 | Dom Train Acc: 59.27%\n",
      "           λ = 1.000\n",
      "Epoch 24 | Train Loss: 70.1749 | Val RMSE: 10.6757 | Dom Train Acc: 61.70%\n",
      "           λ = 1.000\n",
      "Epoch 25 | Train Loss: 69.8836 | Val RMSE: 11.8120 | Dom Train Acc: 60.26%\n",
      "           λ = 1.000\n",
      "Epoch 26 | Train Loss: 68.9614 | Val RMSE: 11.6604 | Dom Train Acc: 58.34%\n",
      "           λ = 1.000\n",
      "Epoch 27 | Train Loss: 66.8291 | Val RMSE: 10.5151 | Dom Train Acc: 60.20%\n",
      "           λ = 1.000\n",
      "Epoch 28 | Train Loss: 65.8480 | Val RMSE: 10.6639 | Dom Train Acc: 61.72%\n",
      "           λ = 1.000\n",
      "Epoch 29 | Train Loss: 65.1464 | Val RMSE: 10.5351 | Dom Train Acc: 60.23%\n",
      "           λ = 1.000\n",
      "Epoch 30 | Train Loss: 66.0580 | Val RMSE: 10.6109 | Dom Train Acc: 60.03%\n",
      "           λ = 1.000\n",
      "Epoch 31 | Train Loss: 63.9648 | Val RMSE: 10.6915 | Dom Train Acc: 58.87%\n",
      "           λ = 1.000\n",
      "Epoch 32 | Train Loss: 63.1145 | Val RMSE: 10.3376 | Dom Train Acc: 61.40%\n",
      "           λ = 1.000\n",
      "Epoch 33 | Train Loss: 61.6378 | Val RMSE: 10.1886 | Dom Train Acc: 63.63%\n",
      "           λ = 1.000\n",
      "Epoch 34 | Train Loss: 61.2800 | Val RMSE: 11.0567 | Dom Train Acc: 62.87%\n",
      "           λ = 1.000\n",
      "Epoch 35 | Train Loss: 62.1620 | Val RMSE: 10.5462 | Dom Train Acc: 57.32%\n",
      "           λ = 1.000\n",
      "Epoch 36 | Train Loss: 59.6909 | Val RMSE: 10.8114 | Dom Train Acc: 63.47%\n",
      "           λ = 1.000\n",
      "Epoch 37 | Train Loss: 60.7008 | Val RMSE: 10.0162 | Dom Train Acc: 60.72%\n",
      "           λ = 1.000\n",
      "Epoch 38 | Train Loss: 58.5153 | Val RMSE: 10.9950 | Dom Train Acc: 62.72%\n",
      "           λ = 1.000\n",
      "Epoch 39 | Train Loss: 58.4732 | Val RMSE: 10.6131 | Dom Train Acc: 62.70%\n",
      "           λ = 1.000\n",
      "Epoch 40 | Train Loss: 58.3663 | Val RMSE: 10.3012 | Dom Train Acc: 63.30%\n",
      "           λ = 1.000\n",
      "Epoch 41 | Train Loss: 58.5886 | Val RMSE: 10.5732 | Dom Train Acc: 62.08%\n",
      "           λ = 1.000\n",
      "Epoch 42 | Train Loss: 58.7430 | Val RMSE: 10.3100 | Dom Train Acc: 60.21%\n",
      "           λ = 1.000\n",
      "Epoch 43 | Train Loss: 57.4772 | Val RMSE: 10.2244 | Dom Train Acc: 64.92%\n",
      "           λ = 1.000\n",
      "Epoch 44 | Train Loss: 55.4511 | Val RMSE: 10.5864 | Dom Train Acc: 63.84%\n",
      "           λ = 1.000\n",
      "Epoch 45 | Train Loss: 55.3209 | Val RMSE: 10.3458 | Dom Train Acc: 64.75%\n",
      "           λ = 1.000\n",
      "Epoch 46 | Train Loss: 55.3543 | Val RMSE: 10.4320 | Dom Train Acc: 63.35%\n",
      "           λ = 1.000\n",
      "Epoch 47 | Train Loss: 54.4542 | Val RMSE: 11.1050 | Dom Train Acc: 61.30%\n",
      "           λ = 1.000\n",
      "Epoch 48 | Train Loss: 54.2208 | Val RMSE: 10.3106 | Dom Train Acc: 64.75%\n",
      "           λ = 1.000\n",
      "Epoch 49 | Train Loss: 53.7686 | Val RMSE: 10.2033 | Dom Train Acc: 61.00%\n",
      "           λ = 1.000\n",
      "Epoch 50 | Train Loss: 55.1383 | Val RMSE: 10.1904 | Dom Train Acc: 63.88%\n",
      "           λ = 1.000\n",
      "\n",
      "=== Fold 4 | Train on [0, 1, 2, 4], Validate on 3 ===\n",
      "Epoch 01 | Train Loss: 674.2762 | Val RMSE: 22.5735 | Dom Train Acc: 46.16%\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train Loss: 616.6618 | Val RMSE: 21.0394 | Dom Train Acc: 46.29%\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train Loss: 550.6705 | Val RMSE: 19.0411 | Dom Train Acc: 46.57%\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train Loss: 481.9343 | Val RMSE: 19.7897 | Dom Train Acc: 48.40%\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train Loss: 413.8621 | Val RMSE: 17.9646 | Dom Train Acc: 49.81%\n",
      "           λ = 0.500\n",
      "Epoch 06 | Train Loss: 354.5892 | Val RMSE: 16.6368 | Dom Train Acc: 50.71%\n",
      "           λ = 0.600\n",
      "Epoch 07 | Train Loss: 299.8648 | Val RMSE: 15.7872 | Dom Train Acc: 52.46%\n",
      "           λ = 0.700\n",
      "Epoch 08 | Train Loss: 251.4122 | Val RMSE: 14.7615 | Dom Train Acc: 53.03%\n",
      "           λ = 0.800\n",
      "Epoch 09 | Train Loss: 213.2465 | Val RMSE: 13.2312 | Dom Train Acc: 51.53%\n",
      "           λ = 0.900\n",
      "Epoch 10 | Train Loss: 179.9347 | Val RMSE: 12.6647 | Dom Train Acc: 52.44%\n",
      "           λ = 1.000\n",
      "Epoch 11 | Train Loss: 156.6159 | Val RMSE: 12.5345 | Dom Train Acc: 54.65%\n",
      "           λ = 1.000\n",
      "Epoch 12 | Train Loss: 137.0023 | Val RMSE: 11.8977 | Dom Train Acc: 51.87%\n",
      "           λ = 1.000\n",
      "Epoch 13 | Train Loss: 122.4894 | Val RMSE: 11.0719 | Dom Train Acc: 53.73%\n",
      "           λ = 1.000\n",
      "Epoch 14 | Train Loss: 111.2166 | Val RMSE: 11.0517 | Dom Train Acc: 55.45%\n",
      "           λ = 1.000\n",
      "Epoch 15 | Train Loss: 102.8792 | Val RMSE: 10.0747 | Dom Train Acc: 56.62%\n",
      "           λ = 1.000\n",
      "Epoch 16 | Train Loss: 95.7427 | Val RMSE: 10.9084 | Dom Train Acc: 55.22%\n",
      "           λ = 1.000\n",
      "Epoch 17 | Train Loss: 91.1851 | Val RMSE: 9.6489 | Dom Train Acc: 57.93%\n",
      "           λ = 1.000\n",
      "Epoch 18 | Train Loss: 87.3801 | Val RMSE: 9.8217 | Dom Train Acc: 56.45%\n",
      "           λ = 1.000\n",
      "Epoch 19 | Train Loss: 84.6322 | Val RMSE: 10.2349 | Dom Train Acc: 57.03%\n",
      "           λ = 1.000\n",
      "Epoch 20 | Train Loss: 80.9172 | Val RMSE: 9.2291 | Dom Train Acc: 59.63%\n",
      "           λ = 1.000\n",
      "Epoch 21 | Train Loss: 79.3515 | Val RMSE: 9.3698 | Dom Train Acc: 57.28%\n",
      "           λ = 1.000\n",
      "Epoch 22 | Train Loss: 77.0520 | Val RMSE: 9.3714 | Dom Train Acc: 56.28%\n",
      "           λ = 1.000\n",
      "Epoch 23 | Train Loss: 75.3510 | Val RMSE: 9.3448 | Dom Train Acc: 54.83%\n",
      "           λ = 1.000\n",
      "Epoch 24 | Train Loss: 73.1051 | Val RMSE: 9.0624 | Dom Train Acc: 57.61%\n",
      "           λ = 1.000\n",
      "Epoch 25 | Train Loss: 73.1829 | Val RMSE: 11.0358 | Dom Train Acc: 52.58%\n",
      "           λ = 1.000\n",
      "Epoch 26 | Train Loss: 73.0798 | Val RMSE: 9.3769 | Dom Train Acc: 57.23%\n",
      "           λ = 1.000\n",
      "Epoch 27 | Train Loss: 70.4939 | Val RMSE: 8.8354 | Dom Train Acc: 60.71%\n",
      "           λ = 1.000\n",
      "Epoch 28 | Train Loss: 69.3855 | Val RMSE: 9.1579 | Dom Train Acc: 61.07%\n",
      "           λ = 1.000\n",
      "Epoch 29 | Train Loss: 69.0383 | Val RMSE: 9.3809 | Dom Train Acc: 58.54%\n",
      "           λ = 1.000\n",
      "Epoch 30 | Train Loss: 68.2024 | Val RMSE: 8.9765 | Dom Train Acc: 59.37%\n",
      "           λ = 1.000\n",
      "Epoch 31 | Train Loss: 67.6635 | Val RMSE: 8.9311 | Dom Train Acc: 57.85%\n",
      "           λ = 1.000\n",
      "Epoch 32 | Train Loss: 66.1137 | Val RMSE: 9.0233 | Dom Train Acc: 60.93%\n",
      "           λ = 1.000\n",
      "Epoch 33 | Train Loss: 65.4649 | Val RMSE: 8.8451 | Dom Train Acc: 61.10%\n",
      "           λ = 1.000\n",
      "Epoch 34 | Train Loss: 64.4272 | Val RMSE: 8.6762 | Dom Train Acc: 60.29%\n",
      "           λ = 1.000\n",
      "Epoch 35 | Train Loss: 64.9247 | Val RMSE: 9.2814 | Dom Train Acc: 61.37%\n",
      "           λ = 1.000\n",
      "Epoch 36 | Train Loss: 64.4377 | Val RMSE: 8.9696 | Dom Train Acc: 60.46%\n",
      "           λ = 1.000\n",
      "Epoch 37 | Train Loss: 63.0085 | Val RMSE: 8.7884 | Dom Train Acc: 62.47%\n",
      "           λ = 1.000\n",
      "Epoch 38 | Train Loss: 65.1019 | Val RMSE: 9.0687 | Dom Train Acc: 61.63%\n",
      "           λ = 1.000\n",
      "Epoch 39 | Train Loss: 61.7973 | Val RMSE: 8.9183 | Dom Train Acc: 62.39%\n",
      "           λ = 1.000\n",
      "Epoch 40 | Train Loss: 61.0756 | Val RMSE: 9.0577 | Dom Train Acc: 60.90%\n",
      "           λ = 1.000\n",
      "Epoch 41 | Train Loss: 62.1436 | Val RMSE: 8.5208 | Dom Train Acc: 62.48%\n",
      "           λ = 1.000\n",
      "Epoch 42 | Train Loss: 59.8782 | Val RMSE: 8.8022 | Dom Train Acc: 62.08%\n",
      "           λ = 1.000\n",
      "Epoch 43 | Train Loss: 59.9344 | Val RMSE: 8.6132 | Dom Train Acc: 61.98%\n",
      "           λ = 1.000\n",
      "Epoch 44 | Train Loss: 59.6793 | Val RMSE: 8.4603 | Dom Train Acc: 64.83%\n",
      "           λ = 1.000\n",
      "Epoch 45 | Train Loss: 57.8149 | Val RMSE: 8.2761 | Dom Train Acc: 64.56%\n",
      "           λ = 1.000\n",
      "Epoch 46 | Train Loss: 58.3801 | Val RMSE: 8.4496 | Dom Train Acc: 64.00%\n",
      "           λ = 1.000\n",
      "Epoch 47 | Train Loss: 57.2071 | Val RMSE: 8.7961 | Dom Train Acc: 64.06%\n",
      "           λ = 1.000\n",
      "Epoch 48 | Train Loss: 58.3334 | Val RMSE: 8.8647 | Dom Train Acc: 63.79%\n",
      "           λ = 1.000\n",
      "Epoch 49 | Train Loss: 56.3708 | Val RMSE: 8.7050 | Dom Train Acc: 65.51%\n",
      "           λ = 1.000\n",
      "Epoch 50 | Train Loss: 56.1432 | Val RMSE: 8.6339 | Dom Train Acc: 64.64%\n",
      "           λ = 1.000\n",
      "\n",
      "=== Fold 5 | Train on [0, 1, 2, 3], Validate on 4 ===\n",
      "Epoch 01 | Train Loss: 692.8903 | Val RMSE: 21.1296 | Dom Train Acc: 43.50%\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train Loss: 633.6905 | Val RMSE: 18.4385 | Dom Train Acc: 37.98%\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train Loss: 565.1464 | Val RMSE: 19.5167 | Dom Train Acc: 45.30%\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train Loss: 495.2904 | Val RMSE: 17.4931 | Dom Train Acc: 45.32%\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train Loss: 428.6652 | Val RMSE: 18.8435 | Dom Train Acc: 41.90%\n",
      "           λ = 0.500\n",
      "Epoch 06 | Train Loss: 365.1964 | Val RMSE: 14.5382 | Dom Train Acc: 47.10%\n",
      "           λ = 0.600\n",
      "Epoch 07 | Train Loss: 308.8874 | Val RMSE: 14.8104 | Dom Train Acc: 49.22%\n",
      "           λ = 0.700\n",
      "Epoch 08 | Train Loss: 262.7914 | Val RMSE: 13.5394 | Dom Train Acc: 47.07%\n",
      "           λ = 0.800\n",
      "Epoch 09 | Train Loss: 222.3249 | Val RMSE: 13.5902 | Dom Train Acc: 49.00%\n",
      "           λ = 0.900\n",
      "Epoch 10 | Train Loss: 190.4585 | Val RMSE: 12.2817 | Dom Train Acc: 50.87%\n",
      "           λ = 1.000\n",
      "Epoch 11 | Train Loss: 164.2293 | Val RMSE: 11.8767 | Dom Train Acc: 51.41%\n",
      "           λ = 1.000\n",
      "Epoch 12 | Train Loss: 144.1671 | Val RMSE: 11.6190 | Dom Train Acc: 48.42%\n",
      "           λ = 1.000\n",
      "Epoch 13 | Train Loss: 128.7102 | Val RMSE: 10.7117 | Dom Train Acc: 51.73%\n",
      "           λ = 1.000\n",
      "Epoch 14 | Train Loss: 115.3156 | Val RMSE: 10.2625 | Dom Train Acc: 52.70%\n",
      "           λ = 1.000\n",
      "Epoch 15 | Train Loss: 107.7406 | Val RMSE: 10.5272 | Dom Train Acc: 52.01%\n",
      "           λ = 1.000\n",
      "Epoch 16 | Train Loss: 100.1594 | Val RMSE: 10.1162 | Dom Train Acc: 53.35%\n",
      "           λ = 1.000\n",
      "Epoch 17 | Train Loss: 93.1275 | Val RMSE: 10.2770 | Dom Train Acc: 54.35%\n",
      "           λ = 1.000\n",
      "Epoch 18 | Train Loss: 89.3975 | Val RMSE: 10.2071 | Dom Train Acc: 54.13%\n",
      "           λ = 1.000\n",
      "Epoch 19 | Train Loss: 85.2416 | Val RMSE: 9.5116 | Dom Train Acc: 54.68%\n",
      "           λ = 1.000\n",
      "Epoch 20 | Train Loss: 82.3751 | Val RMSE: 9.7162 | Dom Train Acc: 53.82%\n",
      "           λ = 1.000\n",
      "Epoch 21 | Train Loss: 80.7395 | Val RMSE: 9.5450 | Dom Train Acc: 55.21%\n",
      "           λ = 1.000\n",
      "Epoch 22 | Train Loss: 79.1883 | Val RMSE: 9.1754 | Dom Train Acc: 56.33%\n",
      "           λ = 1.000\n",
      "Epoch 23 | Train Loss: 77.2134 | Val RMSE: 11.3905 | Dom Train Acc: 56.61%\n",
      "           λ = 1.000\n",
      "Epoch 24 | Train Loss: 76.4575 | Val RMSE: 9.1530 | Dom Train Acc: 55.86%\n",
      "           λ = 1.000\n",
      "Epoch 25 | Train Loss: 73.9971 | Val RMSE: 9.1371 | Dom Train Acc: 55.69%\n",
      "           λ = 1.000\n",
      "Epoch 26 | Train Loss: 71.8639 | Val RMSE: 8.8724 | Dom Train Acc: 56.61%\n",
      "           λ = 1.000\n",
      "Epoch 27 | Train Loss: 72.7583 | Val RMSE: 10.4309 | Dom Train Acc: 56.95%\n",
      "           λ = 1.000\n",
      "Epoch 28 | Train Loss: 70.7985 | Val RMSE: 9.4293 | Dom Train Acc: 56.73%\n",
      "           λ = 1.000\n",
      "Epoch 29 | Train Loss: 69.3392 | Val RMSE: 9.2017 | Dom Train Acc: 57.84%\n",
      "           λ = 1.000\n",
      "Epoch 30 | Train Loss: 68.7863 | Val RMSE: 8.8479 | Dom Train Acc: 58.14%\n",
      "           λ = 1.000\n",
      "Epoch 31 | Train Loss: 69.0547 | Val RMSE: 9.1535 | Dom Train Acc: 57.63%\n",
      "           λ = 1.000\n",
      "Epoch 32 | Train Loss: 67.5888 | Val RMSE: 8.8844 | Dom Train Acc: 56.41%\n",
      "           λ = 1.000\n",
      "Epoch 33 | Train Loss: 66.6803 | Val RMSE: 8.9336 | Dom Train Acc: 58.31%\n",
      "           λ = 1.000\n",
      "Epoch 34 | Train Loss: 65.0573 | Val RMSE: 11.5313 | Dom Train Acc: 56.78%\n",
      "           λ = 1.000\n",
      "Epoch 35 | Train Loss: 66.7484 | Val RMSE: 8.7579 | Dom Train Acc: 58.40%\n",
      "           λ = 1.000\n",
      "Epoch 36 | Train Loss: 64.8894 | Val RMSE: 8.9344 | Dom Train Acc: 59.19%\n",
      "           λ = 1.000\n",
      "Epoch 37 | Train Loss: 63.4839 | Val RMSE: 8.8586 | Dom Train Acc: 59.78%\n",
      "           λ = 1.000\n",
      "Epoch 38 | Train Loss: 62.7348 | Val RMSE: 10.1496 | Dom Train Acc: 59.56%\n",
      "           λ = 1.000\n",
      "Epoch 39 | Train Loss: 61.5066 | Val RMSE: 8.9229 | Dom Train Acc: 61.22%\n",
      "           λ = 1.000\n",
      "Epoch 40 | Train Loss: 61.6621 | Val RMSE: 9.5650 | Dom Train Acc: 60.68%\n",
      "           λ = 1.000\n",
      "Epoch 41 | Train Loss: 62.1606 | Val RMSE: 8.9927 | Dom Train Acc: 58.67%\n",
      "           λ = 1.000\n",
      "Epoch 42 | Train Loss: 60.2704 | Val RMSE: 12.3719 | Dom Train Acc: 56.91%\n",
      "           λ = 1.000\n",
      "Epoch 43 | Train Loss: 61.2278 | Val RMSE: 8.8311 | Dom Train Acc: 60.79%\n",
      "           λ = 1.000\n",
      "Epoch 44 | Train Loss: 61.9081 | Val RMSE: 9.0171 | Dom Train Acc: 59.92%\n",
      "           λ = 1.000\n",
      "Epoch 45 | Train Loss: 61.2652 | Val RMSE: 8.7446 | Dom Train Acc: 55.35%\n",
      "           λ = 1.000\n",
      "Epoch 46 | Train Loss: 58.5025 | Val RMSE: 8.5630 | Dom Train Acc: 59.68%\n",
      "           λ = 1.000\n",
      "Epoch 47 | Train Loss: 59.2038 | Val RMSE: 8.8946 | Dom Train Acc: 60.84%\n",
      "           λ = 1.000\n",
      "Epoch 48 | Train Loss: 58.6969 | Val RMSE: 9.2153 | Dom Train Acc: 58.69%\n",
      "           λ = 1.000\n",
      "Epoch 49 | Train Loss: 58.1855 | Val RMSE: 9.2706 | Dom Train Acc: 60.79%\n",
      "           λ = 1.000\n",
      "Epoch 50 | Train Loss: 57.9625 | Val RMSE: 9.0645 | Dom Train Acc: 59.82%\n",
      "           λ = 1.000\n",
      "\n",
      "=== Cross-validated RMSE: 10.0164 ± 1.7799 ===\n"
     ]
    }
   ],
   "source": [
    "rmse_scores = cross_validate_dann(X_windows, Y_labels, tensor_dataset=DANNWindowDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, y_val):\n",
    "    return (np.sum((y_val - y_pred) ** 2) / np.prod(y_val.shape)) ** 0.5\n",
    "\n",
    "def NMSE(y_pred, y_val):\n",
    "    num = np.sum((y_val - y_pred) ** 2)\n",
    "    den = np.sum((y_val - np.mean(y_val, axis=0)) ** 2)\n",
    "    return num / den\n",
    "\n",
    "class DANNTrainer2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        train_sessions: list,\n",
    "        val_session: int,\n",
    "        lambda_grl: float = 0.1,\n",
    "        gamma_entropy=0.5,\n",
    "        batch_size: int = 512,\n",
    "        max_epochs: int = 50,\n",
    "        patience: int = 10,\n",
    "        learning_rate: float = 1e-3,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        tensor_dataset=None\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.lambda_grl = lambda_grl\n",
    "        self.gamma_entropy = gamma_entropy\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.reg_loss = nn.MSELoss()\n",
    "        self.dom_loss = nn.CrossEntropyLoss()\n",
    "        self.train_loader = None  # to be assigned externally\n",
    "\n",
    "    def train(self, validate=True):\n",
    "        best_val_rmse = float(\"inf\")\n",
    "        best_weights = None\n",
    "        patience_counter = 0\n",
    "\n",
    "        def compute_lambda(epoch, max_lambda=1.0, warmup_epochs=10):\n",
    "            return min(max_lambda, epoch / warmup_epochs)\n",
    "\n",
    "        for epoch in range(1, self.max_epochs + 1):\n",
    "            self.model.train()\n",
    "            train_y_true, train_y_pred = [], []\n",
    "            epoch_losses = []\n",
    "\n",
    "            lambda_grl = compute_lambda(epoch, max_lambda=1.0, warmup_epochs=10)\n",
    "\n",
    "            for x, y, sid in self.train_loader:\n",
    "                x, y, sid = x.to(self.device), y.to(self.device), sid.to(self.device)\n",
    "                y_pred, dom_pred = self.model(x, lambda_grl=lambda_grl)\n",
    "\n",
    "                loss = (\n",
    "                    self.reg_loss(y_pred, y) +\n",
    "                    lambda_grl * self.dom_loss(dom_pred, sid)\n",
    "                )\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_losses.append(loss.item())\n",
    "\n",
    "                train_y_true.append(y.detach().cpu().numpy())\n",
    "                train_y_pred.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "            train_y_true = np.vstack(train_y_true)\n",
    "            train_y_pred = np.vstack(train_y_pred)\n",
    "            train_rmse = RMSE(train_y_pred, train_y_true)\n",
    "            train_nmse = NMSE(train_y_pred, train_y_true)\n",
    "\n",
    "            if validate:\n",
    "                val_rmse = self.evaluate()\n",
    "                domain_train_acc = self.evaluate_domain_on_train()\n",
    "                print(f\"Epoch {epoch:02d} | Train RMSE: {train_rmse:.4f} | Train NMSE: {train_nmse:.4f} | Val RMSE: {val_rmse:.4f} | Dom Train Acc: {domain_train_acc:.2%}\")\n",
    "                print(f\"           λ = {lambda_grl:.3f}\")\n",
    "\n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_weights = self.model.state_dict()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch:02d} | Train RMSE: {train_rmse:.4f} | Train NMSE: {train_nmse:.4f}\")\n",
    "                print(f\"           λ = {lambda_grl:.3f}\")\n",
    "\n",
    "        if validate and best_weights is not None:\n",
    "            self.model.load_state_dict(best_weights)\n",
    "        return best_val_rmse if validate else self.model\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for x, *_ in data_loader:\n",
    "                x = x.to(self.device)\n",
    "                preds = self.model(x)[0]  # get y_pred only\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "        return np.vstack(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train RMSE: 23.2175 | Train NMSE: 3.3718\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train RMSE: 16.1987 | Train NMSE: 1.6413\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train RMSE: 11.5368 | Train NMSE: 0.8325\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train RMSE: 9.8828 | Train NMSE: 0.6109\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train RMSE: 9.3169 | Train NMSE: 0.5430\n",
      "           λ = 0.500\n",
      "Epoch 06 | Train RMSE: 9.0003 | Train NMSE: 0.5067\n",
      "           λ = 0.600\n",
      "Epoch 07 | Train RMSE: 8.8032 | Train NMSE: 0.4847\n",
      "           λ = 0.700\n",
      "Epoch 08 | Train RMSE: 8.6630 | Train NMSE: 0.4694\n",
      "           λ = 0.800\n",
      "Epoch 09 | Train RMSE: 8.5286 | Train NMSE: 0.4550\n",
      "           λ = 0.900\n",
      "Epoch 10 | Train RMSE: 8.3746 | Train NMSE: 0.4387\n",
      "           λ = 1.000\n",
      "Epoch 11 | Train RMSE: 8.2355 | Train NMSE: 0.4242\n",
      "           λ = 1.000\n",
      "Epoch 12 | Train RMSE: 8.1317 | Train NMSE: 0.4136\n",
      "           λ = 1.000\n",
      "Epoch 13 | Train RMSE: 8.0285 | Train NMSE: 0.4032\n",
      "           λ = 1.000\n",
      "Epoch 14 | Train RMSE: 7.9746 | Train NMSE: 0.3978\n",
      "           λ = 1.000\n",
      "Epoch 15 | Train RMSE: 7.8789 | Train NMSE: 0.3883\n",
      "           λ = 1.000\n",
      "Epoch 16 | Train RMSE: 7.8390 | Train NMSE: 0.3844\n",
      "           λ = 1.000\n",
      "Epoch 17 | Train RMSE: 7.7791 | Train NMSE: 0.3785\n",
      "           λ = 1.000\n",
      "Epoch 18 | Train RMSE: 7.6447 | Train NMSE: 0.3656\n",
      "           λ = 1.000\n",
      "Epoch 19 | Train RMSE: 7.6009 | Train NMSE: 0.3614\n",
      "           λ = 1.000\n",
      "Epoch 20 | Train RMSE: 7.5626 | Train NMSE: 0.3577\n",
      "           λ = 1.000\n",
      "Epoch 21 | Train RMSE: 7.4946 | Train NMSE: 0.3513\n",
      "           λ = 1.000\n",
      "Epoch 22 | Train RMSE: 7.4171 | Train NMSE: 0.3441\n",
      "           λ = 1.000\n",
      "Epoch 23 | Train RMSE: 7.3857 | Train NMSE: 0.3412\n",
      "           λ = 1.000\n",
      "Epoch 24 | Train RMSE: 7.3790 | Train NMSE: 0.3406\n",
      "           λ = 1.000\n",
      "Epoch 25 | Train RMSE: 7.3083 | Train NMSE: 0.3341\n",
      "           λ = 1.000\n",
      "Epoch 26 | Train RMSE: 7.3086 | Train NMSE: 0.3341\n",
      "           λ = 1.000\n",
      "Epoch 27 | Train RMSE: 7.2493 | Train NMSE: 0.3287\n",
      "           λ = 1.000\n",
      "Epoch 28 | Train RMSE: 7.2211 | Train NMSE: 0.3262\n",
      "           λ = 1.000\n",
      "Epoch 29 | Train RMSE: 7.2171 | Train NMSE: 0.3258\n",
      "           λ = 1.000\n",
      "Epoch 30 | Train RMSE: 7.1258 | Train NMSE: 0.3176\n",
      "           λ = 1.000\n",
      "Epoch 31 | Train RMSE: 7.1407 | Train NMSE: 0.3189\n",
      "           λ = 1.000\n",
      "Epoch 32 | Train RMSE: 7.0444 | Train NMSE: 0.3104\n",
      "           λ = 1.000\n",
      "Epoch 33 | Train RMSE: 7.0952 | Train NMSE: 0.3149\n",
      "           λ = 1.000\n",
      "Epoch 34 | Train RMSE: 7.0361 | Train NMSE: 0.3097\n",
      "           λ = 1.000\n",
      "Epoch 35 | Train RMSE: 7.0101 | Train NMSE: 0.3074\n",
      "           λ = 1.000\n",
      "Epoch 36 | Train RMSE: 7.0102 | Train NMSE: 0.3074\n",
      "           λ = 1.000\n",
      "Epoch 37 | Train RMSE: 6.9842 | Train NMSE: 0.3051\n",
      "           λ = 1.000\n",
      "Epoch 38 | Train RMSE: 6.9428 | Train NMSE: 0.3015\n",
      "           λ = 1.000\n",
      "Epoch 39 | Train RMSE: 6.9203 | Train NMSE: 0.2996\n",
      "           λ = 1.000\n",
      "Epoch 40 | Train RMSE: 6.9519 | Train NMSE: 0.3023\n",
      "           λ = 1.000\n",
      "Epoch 41 | Train RMSE: 6.9111 | Train NMSE: 0.2988\n",
      "           λ = 1.000\n",
      "Epoch 42 | Train RMSE: 6.8944 | Train NMSE: 0.2973\n",
      "           λ = 1.000\n",
      "Epoch 43 | Train RMSE: 6.8353 | Train NMSE: 0.2922\n",
      "           λ = 1.000\n",
      "Epoch 44 | Train RMSE: 6.8368 | Train NMSE: 0.2924\n",
      "           λ = 1.000\n",
      "Epoch 45 | Train RMSE: 6.8312 | Train NMSE: 0.2919\n",
      "           λ = 1.000\n",
      "Epoch 46 | Train RMSE: 6.8134 | Train NMSE: 0.2904\n",
      "           λ = 1.000\n",
      "Epoch 47 | Train RMSE: 6.8169 | Train NMSE: 0.2907\n",
      "           λ = 1.000\n",
      "Epoch 48 | Train RMSE: 6.8115 | Train NMSE: 0.2902\n",
      "           λ = 1.000\n",
      "Epoch 49 | Train RMSE: 6.7849 | Train NMSE: 0.2880\n",
      "           λ = 1.000\n",
      "Epoch 50 | Train RMSE: 6.7973 | Train NMSE: 0.2890\n",
      "           λ = 1.000\n",
      "Epoch 51 | Train RMSE: 6.7795 | Train NMSE: 0.2875\n",
      "           λ = 1.000\n",
      "Epoch 52 | Train RMSE: 6.7667 | Train NMSE: 0.2864\n",
      "           λ = 1.000\n",
      "Epoch 53 | Train RMSE: 6.7222 | Train NMSE: 0.2827\n",
      "           λ = 1.000\n",
      "Epoch 54 | Train RMSE: 6.7037 | Train NMSE: 0.2811\n",
      "           λ = 1.000\n",
      "Epoch 55 | Train RMSE: 6.7138 | Train NMSE: 0.2819\n",
      "           λ = 1.000\n",
      "Epoch 56 | Train RMSE: 6.7182 | Train NMSE: 0.2823\n",
      "           λ = 1.000\n",
      "Epoch 57 | Train RMSE: 6.6990 | Train NMSE: 0.2807\n",
      "           λ = 1.000\n",
      "Epoch 58 | Train RMSE: 6.6966 | Train NMSE: 0.2805\n",
      "           λ = 1.000\n",
      "Epoch 59 | Train RMSE: 6.6264 | Train NMSE: 0.2747\n",
      "           λ = 1.000\n",
      "Epoch 60 | Train RMSE: 6.6489 | Train NMSE: 0.2765\n",
      "           λ = 1.000\n",
      "Epoch 61 | Train RMSE: 6.6355 | Train NMSE: 0.2754\n",
      "           λ = 1.000\n",
      "Epoch 62 | Train RMSE: 6.6341 | Train NMSE: 0.2753\n",
      "           λ = 1.000\n",
      "Epoch 63 | Train RMSE: 6.6143 | Train NMSE: 0.2736\n",
      "           λ = 1.000\n",
      "Epoch 64 | Train RMSE: 6.5945 | Train NMSE: 0.2720\n",
      "           λ = 1.000\n",
      "Epoch 65 | Train RMSE: 6.5888 | Train NMSE: 0.2715\n",
      "           λ = 1.000\n",
      "Epoch 66 | Train RMSE: 6.6244 | Train NMSE: 0.2745\n",
      "           λ = 1.000\n",
      "Epoch 67 | Train RMSE: 6.6081 | Train NMSE: 0.2731\n",
      "           λ = 1.000\n",
      "Epoch 68 | Train RMSE: 6.5481 | Train NMSE: 0.2682\n",
      "           λ = 1.000\n",
      "Epoch 69 | Train RMSE: 6.5939 | Train NMSE: 0.2720\n",
      "           λ = 1.000\n",
      "Epoch 70 | Train RMSE: 6.5774 | Train NMSE: 0.2706\n",
      "           λ = 1.000\n",
      "Epoch 71 | Train RMSE: 6.5438 | Train NMSE: 0.2679\n",
      "           λ = 1.000\n",
      "Epoch 72 | Train RMSE: 6.5348 | Train NMSE: 0.2671\n",
      "           λ = 1.000\n",
      "Epoch 73 | Train RMSE: 6.5460 | Train NMSE: 0.2680\n",
      "           λ = 1.000\n",
      "Epoch 74 | Train RMSE: 6.4939 | Train NMSE: 0.2638\n",
      "           λ = 1.000\n",
      "Epoch 75 | Train RMSE: 6.5187 | Train NMSE: 0.2658\n",
      "           λ = 1.000\n",
      "Epoch 76 | Train RMSE: 6.5024 | Train NMSE: 0.2645\n",
      "           λ = 1.000\n",
      "Epoch 77 | Train RMSE: 6.5183 | Train NMSE: 0.2658\n",
      "           λ = 1.000\n",
      "Epoch 78 | Train RMSE: 6.4587 | Train NMSE: 0.2609\n",
      "           λ = 1.000\n",
      "Epoch 79 | Train RMSE: 6.5033 | Train NMSE: 0.2645\n",
      "           λ = 1.000\n",
      "Epoch 80 | Train RMSE: 6.4320 | Train NMSE: 0.2588\n",
      "           λ = 1.000\n",
      "Epoch 81 | Train RMSE: 6.4488 | Train NMSE: 0.2601\n",
      "           λ = 1.000\n",
      "Epoch 82 | Train RMSE: 6.4833 | Train NMSE: 0.2629\n",
      "           λ = 1.000\n",
      "Epoch 83 | Train RMSE: 6.4582 | Train NMSE: 0.2609\n",
      "           λ = 1.000\n",
      "Epoch 84 | Train RMSE: 6.4384 | Train NMSE: 0.2593\n",
      "           λ = 1.000\n",
      "Epoch 85 | Train RMSE: 6.3971 | Train NMSE: 0.2560\n",
      "           λ = 1.000\n",
      "Epoch 86 | Train RMSE: 6.4206 | Train NMSE: 0.2579\n",
      "           λ = 1.000\n",
      "Epoch 87 | Train RMSE: 6.4439 | Train NMSE: 0.2597\n",
      "           λ = 1.000\n",
      "Epoch 88 | Train RMSE: 6.4090 | Train NMSE: 0.2569\n",
      "           λ = 1.000\n",
      "Epoch 89 | Train RMSE: 6.4039 | Train NMSE: 0.2565\n",
      "           λ = 1.000\n",
      "Epoch 90 | Train RMSE: 6.3939 | Train NMSE: 0.2557\n",
      "           λ = 1.000\n",
      "Epoch 91 | Train RMSE: 6.4237 | Train NMSE: 0.2581\n",
      "           λ = 1.000\n",
      "Epoch 92 | Train RMSE: 6.3481 | Train NMSE: 0.2521\n",
      "           λ = 1.000\n",
      "Epoch 93 | Train RMSE: 6.3679 | Train NMSE: 0.2536\n",
      "           λ = 1.000\n",
      "Epoch 94 | Train RMSE: 6.3461 | Train NMSE: 0.2519\n",
      "           λ = 1.000\n",
      "Epoch 95 | Train RMSE: 6.3374 | Train NMSE: 0.2512\n",
      "           λ = 1.000\n",
      "Epoch 96 | Train RMSE: 6.3727 | Train NMSE: 0.2540\n",
      "           λ = 1.000\n",
      "Epoch 97 | Train RMSE: 6.3308 | Train NMSE: 0.2507\n",
      "           λ = 1.000\n",
      "Epoch 98 | Train RMSE: 6.2981 | Train NMSE: 0.2481\n",
      "           λ = 1.000\n",
      "Epoch 99 | Train RMSE: 6.3114 | Train NMSE: 0.2492\n",
      "           λ = 1.000\n",
      "Epoch 100 | Train RMSE: 6.3277 | Train NMSE: 0.2504\n",
      "           λ = 1.000\n",
      "Epoch 101 | Train RMSE: 6.3061 | Train NMSE: 0.2487\n",
      "           λ = 1.000\n",
      "Epoch 102 | Train RMSE: 6.3530 | Train NMSE: 0.2525\n",
      "           λ = 1.000\n",
      "Epoch 103 | Train RMSE: 6.3096 | Train NMSE: 0.2490\n",
      "           λ = 1.000\n",
      "Epoch 104 | Train RMSE: 6.2816 | Train NMSE: 0.2468\n",
      "           λ = 1.000\n",
      "Epoch 105 | Train RMSE: 6.3099 | Train NMSE: 0.2490\n",
      "           λ = 1.000\n",
      "Epoch 106 | Train RMSE: 6.2690 | Train NMSE: 0.2458\n",
      "           λ = 1.000\n",
      "Epoch 107 | Train RMSE: 6.3092 | Train NMSE: 0.2490\n",
      "           λ = 1.000\n",
      "Epoch 108 | Train RMSE: 6.2487 | Train NMSE: 0.2442\n",
      "           λ = 1.000\n",
      "Epoch 109 | Train RMSE: 6.2627 | Train NMSE: 0.2453\n",
      "           λ = 1.000\n",
      "Epoch 110 | Train RMSE: 6.2780 | Train NMSE: 0.2465\n",
      "           λ = 1.000\n",
      "Epoch 111 | Train RMSE: 6.2441 | Train NMSE: 0.2439\n",
      "           λ = 1.000\n",
      "Epoch 112 | Train RMSE: 6.2941 | Train NMSE: 0.2478\n",
      "           λ = 1.000\n",
      "Epoch 113 | Train RMSE: 6.2318 | Train NMSE: 0.2429\n",
      "           λ = 1.000\n",
      "Epoch 114 | Train RMSE: 6.2329 | Train NMSE: 0.2430\n",
      "           λ = 1.000\n",
      "Epoch 115 | Train RMSE: 6.2277 | Train NMSE: 0.2426\n",
      "           λ = 1.000\n",
      "Epoch 116 | Train RMSE: 6.2554 | Train NMSE: 0.2448\n",
      "           λ = 1.000\n",
      "Epoch 117 | Train RMSE: 6.2542 | Train NMSE: 0.2447\n",
      "           λ = 1.000\n",
      "Epoch 118 | Train RMSE: 6.2556 | Train NMSE: 0.2448\n",
      "           λ = 1.000\n",
      "Epoch 119 | Train RMSE: 6.1694 | Train NMSE: 0.2381\n",
      "           λ = 1.000\n",
      "Epoch 120 | Train RMSE: 6.2071 | Train NMSE: 0.2410\n",
      "           λ = 1.000\n",
      "Epoch 121 | Train RMSE: 6.2214 | Train NMSE: 0.2421\n",
      "           λ = 1.000\n",
      "Epoch 122 | Train RMSE: 6.2300 | Train NMSE: 0.2428\n",
      "           λ = 1.000\n",
      "Epoch 123 | Train RMSE: 6.1488 | Train NMSE: 0.2365\n",
      "           λ = 1.000\n",
      "Epoch 124 | Train RMSE: 6.1764 | Train NMSE: 0.2386\n",
      "           λ = 1.000\n",
      "Epoch 125 | Train RMSE: 6.2018 | Train NMSE: 0.2406\n",
      "           λ = 1.000\n",
      "Epoch 126 | Train RMSE: 6.2163 | Train NMSE: 0.2417\n",
      "           λ = 1.000\n",
      "Epoch 127 | Train RMSE: 6.2029 | Train NMSE: 0.2407\n",
      "           λ = 1.000\n",
      "Epoch 128 | Train RMSE: 6.1230 | Train NMSE: 0.2345\n",
      "           λ = 1.000\n",
      "Epoch 129 | Train RMSE: 6.1642 | Train NMSE: 0.2377\n",
      "           λ = 1.000\n",
      "Epoch 130 | Train RMSE: 6.1573 | Train NMSE: 0.2371\n",
      "           λ = 1.000\n",
      "Epoch 131 | Train RMSE: 6.1598 | Train NMSE: 0.2373\n",
      "           λ = 1.000\n",
      "Epoch 132 | Train RMSE: 6.1270 | Train NMSE: 0.2348\n",
      "           λ = 1.000\n",
      "Epoch 133 | Train RMSE: 6.1340 | Train NMSE: 0.2354\n",
      "           λ = 1.000\n",
      "Epoch 134 | Train RMSE: 6.1497 | Train NMSE: 0.2366\n",
      "           λ = 1.000\n",
      "Epoch 135 | Train RMSE: 6.1437 | Train NMSE: 0.2361\n",
      "           λ = 1.000\n",
      "Epoch 136 | Train RMSE: 6.1522 | Train NMSE: 0.2367\n",
      "           λ = 1.000\n",
      "Epoch 137 | Train RMSE: 6.1262 | Train NMSE: 0.2348\n",
      "           λ = 1.000\n",
      "Epoch 138 | Train RMSE: 6.1036 | Train NMSE: 0.2330\n",
      "           λ = 1.000\n",
      "Epoch 139 | Train RMSE: 6.1158 | Train NMSE: 0.2340\n",
      "           λ = 1.000\n",
      "Epoch 140 | Train RMSE: 6.1303 | Train NMSE: 0.2351\n",
      "           λ = 1.000\n",
      "Epoch 141 | Train RMSE: 6.0682 | Train NMSE: 0.2303\n",
      "           λ = 1.000\n",
      "Epoch 142 | Train RMSE: 6.0562 | Train NMSE: 0.2294\n",
      "           λ = 1.000\n",
      "Epoch 143 | Train RMSE: 6.0736 | Train NMSE: 0.2307\n",
      "           λ = 1.000\n",
      "Epoch 144 | Train RMSE: 6.0652 | Train NMSE: 0.2301\n",
      "           λ = 1.000\n",
      "Epoch 145 | Train RMSE: 6.0989 | Train NMSE: 0.2327\n",
      "           λ = 1.000\n",
      "Epoch 146 | Train RMSE: 6.0935 | Train NMSE: 0.2323\n",
      "           λ = 1.000\n",
      "Epoch 147 | Train RMSE: 6.0660 | Train NMSE: 0.2302\n",
      "           λ = 1.000\n",
      "Epoch 148 | Train RMSE: 6.0895 | Train NMSE: 0.2319\n",
      "           λ = 1.000\n",
      "Epoch 149 | Train RMSE: 6.0761 | Train NMSE: 0.2309\n",
      "           λ = 1.000\n",
      "Epoch 150 | Train RMSE: 6.0552 | Train NMSE: 0.2293\n",
      "           λ = 1.000\n",
      "Epoch 151 | Train RMSE: 6.0494 | Train NMSE: 0.2289\n",
      "           λ = 1.000\n",
      "Epoch 152 | Train RMSE: 6.0654 | Train NMSE: 0.2301\n",
      "           λ = 1.000\n",
      "Epoch 153 | Train RMSE: 6.0335 | Train NMSE: 0.2277\n",
      "           λ = 1.000\n",
      "Epoch 154 | Train RMSE: 6.0157 | Train NMSE: 0.2264\n",
      "           λ = 1.000\n",
      "Epoch 155 | Train RMSE: 6.0306 | Train NMSE: 0.2275\n",
      "           λ = 1.000\n",
      "Epoch 156 | Train RMSE: 6.0788 | Train NMSE: 0.2311\n",
      "           λ = 1.000\n",
      "Epoch 157 | Train RMSE: 6.0468 | Train NMSE: 0.2287\n",
      "           λ = 1.000\n",
      "Epoch 158 | Train RMSE: 6.0552 | Train NMSE: 0.2293\n",
      "           λ = 1.000\n",
      "Epoch 159 | Train RMSE: 6.0308 | Train NMSE: 0.2275\n",
      "           λ = 1.000\n",
      "Epoch 160 | Train RMSE: 6.0154 | Train NMSE: 0.2263\n",
      "           λ = 1.000\n",
      "Epoch 161 | Train RMSE: 6.0219 | Train NMSE: 0.2268\n",
      "           λ = 1.000\n",
      "Epoch 162 | Train RMSE: 6.0509 | Train NMSE: 0.2290\n",
      "           λ = 1.000\n",
      "Epoch 163 | Train RMSE: 6.0014 | Train NMSE: 0.2253\n",
      "           λ = 1.000\n",
      "Epoch 164 | Train RMSE: 6.0111 | Train NMSE: 0.2260\n",
      "           λ = 1.000\n",
      "Epoch 165 | Train RMSE: 6.0641 | Train NMSE: 0.2300\n",
      "           λ = 1.000\n",
      "Epoch 166 | Train RMSE: 6.0012 | Train NMSE: 0.2253\n",
      "           λ = 1.000\n",
      "Epoch 167 | Train RMSE: 5.9942 | Train NMSE: 0.2247\n",
      "           λ = 1.000\n",
      "Epoch 168 | Train RMSE: 6.0132 | Train NMSE: 0.2262\n",
      "           λ = 1.000\n",
      "Epoch 169 | Train RMSE: 5.9876 | Train NMSE: 0.2243\n",
      "           λ = 1.000\n",
      "Epoch 170 | Train RMSE: 6.0174 | Train NMSE: 0.2265\n",
      "           λ = 1.000\n",
      "Epoch 171 | Train RMSE: 5.9788 | Train NMSE: 0.2236\n",
      "           λ = 1.000\n",
      "Epoch 172 | Train RMSE: 5.9444 | Train NMSE: 0.2210\n",
      "           λ = 1.000\n",
      "Epoch 173 | Train RMSE: 5.9614 | Train NMSE: 0.2223\n",
      "           λ = 1.000\n",
      "Epoch 174 | Train RMSE: 5.9789 | Train NMSE: 0.2236\n",
      "           λ = 1.000\n",
      "Epoch 175 | Train RMSE: 5.9467 | Train NMSE: 0.2212\n",
      "           λ = 1.000\n",
      "Epoch 176 | Train RMSE: 5.9613 | Train NMSE: 0.2223\n",
      "           λ = 1.000\n",
      "Epoch 177 | Train RMSE: 5.9970 | Train NMSE: 0.2250\n",
      "           λ = 1.000\n",
      "Epoch 178 | Train RMSE: 5.9261 | Train NMSE: 0.2197\n",
      "           λ = 1.000\n",
      "Epoch 179 | Train RMSE: 5.9407 | Train NMSE: 0.2208\n",
      "           λ = 1.000\n",
      "Epoch 180 | Train RMSE: 5.9319 | Train NMSE: 0.2201\n",
      "           λ = 1.000\n",
      "Epoch 181 | Train RMSE: 5.9433 | Train NMSE: 0.2209\n",
      "           λ = 1.000\n",
      "Epoch 182 | Train RMSE: 5.9507 | Train NMSE: 0.2215\n",
      "           λ = 1.000\n",
      "Epoch 183 | Train RMSE: 6.0041 | Train NMSE: 0.2255\n",
      "           λ = 1.000\n",
      "Epoch 184 | Train RMSE: 5.9214 | Train NMSE: 0.2193\n",
      "           λ = 1.000\n",
      "Epoch 185 | Train RMSE: 5.9334 | Train NMSE: 0.2202\n",
      "           λ = 1.000\n",
      "Epoch 186 | Train RMSE: 5.8835 | Train NMSE: 0.2165\n",
      "           λ = 1.000\n",
      "Epoch 187 | Train RMSE: 5.9088 | Train NMSE: 0.2184\n",
      "           λ = 1.000\n",
      "Epoch 188 | Train RMSE: 5.9373 | Train NMSE: 0.2205\n",
      "           λ = 1.000\n",
      "Epoch 189 | Train RMSE: 5.8827 | Train NMSE: 0.2165\n",
      "           λ = 1.000\n",
      "Epoch 190 | Train RMSE: 5.9471 | Train NMSE: 0.2212\n",
      "           λ = 1.000\n",
      "Epoch 191 | Train RMSE: 5.9056 | Train NMSE: 0.2182\n",
      "           λ = 1.000\n",
      "Epoch 192 | Train RMSE: 5.9614 | Train NMSE: 0.2223\n",
      "           λ = 1.000\n",
      "Epoch 193 | Train RMSE: 5.9062 | Train NMSE: 0.2182\n",
      "           λ = 1.000\n",
      "Epoch 194 | Train RMSE: 5.9010 | Train NMSE: 0.2178\n",
      "           λ = 1.000\n",
      "Epoch 195 | Train RMSE: 5.9162 | Train NMSE: 0.2189\n",
      "           λ = 1.000\n",
      "Epoch 196 | Train RMSE: 5.8821 | Train NMSE: 0.2164\n",
      "           λ = 1.000\n",
      "Epoch 197 | Train RMSE: 5.8878 | Train NMSE: 0.2168\n",
      "           λ = 1.000\n",
      "Epoch 198 | Train RMSE: 5.8762 | Train NMSE: 0.2160\n",
      "           λ = 1.000\n",
      "Epoch 199 | Train RMSE: 5.9094 | Train NMSE: 0.2184\n",
      "           λ = 1.000\n",
      "Epoch 200 | Train RMSE: 5.9003 | Train NMSE: 0.2178\n",
      "           λ = 1.000\n",
      "Epoch 201 | Train RMSE: 5.8948 | Train NMSE: 0.2174\n",
      "           λ = 1.000\n",
      "Epoch 202 | Train RMSE: 5.9376 | Train NMSE: 0.2205\n",
      "           λ = 1.000\n",
      "Epoch 203 | Train RMSE: 5.8914 | Train NMSE: 0.2171\n",
      "           λ = 1.000\n",
      "Epoch 204 | Train RMSE: 5.8544 | Train NMSE: 0.2144\n",
      "           λ = 1.000\n",
      "Epoch 205 | Train RMSE: 5.8657 | Train NMSE: 0.2152\n",
      "           λ = 1.000\n",
      "Epoch 206 | Train RMSE: 5.9007 | Train NMSE: 0.2178\n",
      "           λ = 1.000\n",
      "Epoch 207 | Train RMSE: 5.8640 | Train NMSE: 0.2151\n",
      "           λ = 1.000\n",
      "Epoch 208 | Train RMSE: 5.8405 | Train NMSE: 0.2134\n",
      "           λ = 1.000\n",
      "Epoch 209 | Train RMSE: 5.8587 | Train NMSE: 0.2147\n",
      "           λ = 1.000\n",
      "Epoch 210 | Train RMSE: 5.8904 | Train NMSE: 0.2170\n",
      "           λ = 1.000\n",
      "Epoch 211 | Train RMSE: 5.8711 | Train NMSE: 0.2156\n",
      "           λ = 1.000\n",
      "Epoch 212 | Train RMSE: 5.8890 | Train NMSE: 0.2169\n",
      "           λ = 1.000\n",
      "Epoch 213 | Train RMSE: 5.8491 | Train NMSE: 0.2140\n",
      "           λ = 1.000\n",
      "Epoch 214 | Train RMSE: 5.8683 | Train NMSE: 0.2154\n",
      "           λ = 1.000\n",
      "Epoch 215 | Train RMSE: 5.8341 | Train NMSE: 0.2129\n",
      "           λ = 1.000\n",
      "Epoch 216 | Train RMSE: 5.8427 | Train NMSE: 0.2135\n",
      "           λ = 1.000\n",
      "Epoch 217 | Train RMSE: 5.8498 | Train NMSE: 0.2140\n",
      "           λ = 1.000\n",
      "Epoch 218 | Train RMSE: 5.8201 | Train NMSE: 0.2119\n",
      "           λ = 1.000\n",
      "Epoch 219 | Train RMSE: 5.8043 | Train NMSE: 0.2107\n",
      "           λ = 1.000\n",
      "Epoch 220 | Train RMSE: 5.8023 | Train NMSE: 0.2106\n",
      "           λ = 1.000\n",
      "Epoch 221 | Train RMSE: 5.8255 | Train NMSE: 0.2123\n",
      "           λ = 1.000\n",
      "Epoch 222 | Train RMSE: 5.8505 | Train NMSE: 0.2141\n",
      "           λ = 1.000\n",
      "Epoch 223 | Train RMSE: 5.8253 | Train NMSE: 0.2123\n",
      "           λ = 1.000\n",
      "Epoch 224 | Train RMSE: 5.8033 | Train NMSE: 0.2107\n",
      "           λ = 1.000\n",
      "Epoch 225 | Train RMSE: 5.8137 | Train NMSE: 0.2114\n",
      "           λ = 1.000\n",
      "Epoch 226 | Train RMSE: 5.8077 | Train NMSE: 0.2110\n",
      "           λ = 1.000\n",
      "Epoch 227 | Train RMSE: 5.8219 | Train NMSE: 0.2120\n",
      "           λ = 1.000\n",
      "Epoch 228 | Train RMSE: 5.7915 | Train NMSE: 0.2098\n",
      "           λ = 1.000\n",
      "Epoch 229 | Train RMSE: 5.7944 | Train NMSE: 0.2100\n",
      "           λ = 1.000\n",
      "Epoch 230 | Train RMSE: 5.8535 | Train NMSE: 0.2143\n",
      "           λ = 1.000\n",
      "Epoch 231 | Train RMSE: 5.7590 | Train NMSE: 0.2075\n",
      "           λ = 1.000\n",
      "Epoch 232 | Train RMSE: 5.8362 | Train NMSE: 0.2131\n",
      "           λ = 1.000\n",
      "Epoch 233 | Train RMSE: 5.8095 | Train NMSE: 0.2111\n",
      "           λ = 1.000\n",
      "Epoch 234 | Train RMSE: 5.8245 | Train NMSE: 0.2122\n",
      "           λ = 1.000\n",
      "Epoch 235 | Train RMSE: 5.7824 | Train NMSE: 0.2091\n",
      "           λ = 1.000\n",
      "Epoch 236 | Train RMSE: 5.7578 | Train NMSE: 0.2074\n",
      "           λ = 1.000\n",
      "Epoch 237 | Train RMSE: 5.7690 | Train NMSE: 0.2082\n",
      "           λ = 1.000\n",
      "Epoch 238 | Train RMSE: 5.7791 | Train NMSE: 0.2089\n",
      "           λ = 1.000\n",
      "Epoch 239 | Train RMSE: 5.7969 | Train NMSE: 0.2102\n",
      "           λ = 1.000\n",
      "Epoch 240 | Train RMSE: 5.7880 | Train NMSE: 0.2095\n",
      "           λ = 1.000\n",
      "Epoch 241 | Train RMSE: 5.7723 | Train NMSE: 0.2084\n",
      "           λ = 1.000\n",
      "Epoch 242 | Train RMSE: 5.7793 | Train NMSE: 0.2089\n",
      "           λ = 1.000\n",
      "Epoch 243 | Train RMSE: 5.7618 | Train NMSE: 0.2077\n",
      "           λ = 1.000\n",
      "Epoch 244 | Train RMSE: 5.7455 | Train NMSE: 0.2065\n",
      "           λ = 1.000\n",
      "Epoch 245 | Train RMSE: 5.7499 | Train NMSE: 0.2068\n",
      "           λ = 1.000\n",
      "Epoch 246 | Train RMSE: 5.7885 | Train NMSE: 0.2096\n",
      "           λ = 1.000\n",
      "Epoch 247 | Train RMSE: 5.7633 | Train NMSE: 0.2078\n",
      "           λ = 1.000\n",
      "Epoch 248 | Train RMSE: 5.7725 | Train NMSE: 0.2084\n",
      "           λ = 1.000\n",
      "Epoch 249 | Train RMSE: 5.7488 | Train NMSE: 0.2067\n",
      "           λ = 1.000\n",
      "Epoch 250 | Train RMSE: 5.7526 | Train NMSE: 0.2070\n",
      "           λ = 1.000\n",
      "Epoch 251 | Train RMSE: 5.7813 | Train NMSE: 0.2091\n",
      "           λ = 1.000\n",
      "Epoch 252 | Train RMSE: 5.7742 | Train NMSE: 0.2085\n",
      "           λ = 1.000\n",
      "Epoch 253 | Train RMSE: 5.7541 | Train NMSE: 0.2071\n",
      "           λ = 1.000\n",
      "Epoch 254 | Train RMSE: 5.7361 | Train NMSE: 0.2058\n",
      "           λ = 1.000\n",
      "Epoch 255 | Train RMSE: 5.7429 | Train NMSE: 0.2063\n",
      "           λ = 1.000\n",
      "Epoch 256 | Train RMSE: 5.7234 | Train NMSE: 0.2049\n",
      "           λ = 1.000\n",
      "Epoch 257 | Train RMSE: 5.7634 | Train NMSE: 0.2078\n",
      "           λ = 1.000\n",
      "Epoch 258 | Train RMSE: 5.7242 | Train NMSE: 0.2050\n",
      "           λ = 1.000\n",
      "Epoch 259 | Train RMSE: 5.7116 | Train NMSE: 0.2041\n",
      "           λ = 1.000\n",
      "Epoch 260 | Train RMSE: 5.7413 | Train NMSE: 0.2062\n",
      "           λ = 1.000\n",
      "Epoch 261 | Train RMSE: 5.7254 | Train NMSE: 0.2050\n",
      "           λ = 1.000\n",
      "Epoch 262 | Train RMSE: 5.7058 | Train NMSE: 0.2036\n",
      "           λ = 1.000\n",
      "Epoch 263 | Train RMSE: 5.7085 | Train NMSE: 0.2038\n",
      "           λ = 1.000\n",
      "Epoch 264 | Train RMSE: 5.7277 | Train NMSE: 0.2052\n",
      "           λ = 1.000\n",
      "Epoch 265 | Train RMSE: 5.6910 | Train NMSE: 0.2026\n",
      "           λ = 1.000\n",
      "Epoch 266 | Train RMSE: 5.6883 | Train NMSE: 0.2024\n",
      "           λ = 1.000\n",
      "Epoch 267 | Train RMSE: 5.6779 | Train NMSE: 0.2017\n",
      "           λ = 1.000\n",
      "Epoch 268 | Train RMSE: 5.7148 | Train NMSE: 0.2043\n",
      "           λ = 1.000\n",
      "Epoch 269 | Train RMSE: 5.7053 | Train NMSE: 0.2036\n",
      "           λ = 1.000\n",
      "Epoch 270 | Train RMSE: 5.7060 | Train NMSE: 0.2037\n",
      "           λ = 1.000\n",
      "Epoch 271 | Train RMSE: 5.7344 | Train NMSE: 0.2057\n",
      "           λ = 1.000\n",
      "Epoch 272 | Train RMSE: 5.7205 | Train NMSE: 0.2047\n",
      "           λ = 1.000\n",
      "Epoch 273 | Train RMSE: 5.7541 | Train NMSE: 0.2071\n",
      "           λ = 1.000\n",
      "Epoch 274 | Train RMSE: 5.6824 | Train NMSE: 0.2020\n",
      "           λ = 1.000\n",
      "Epoch 275 | Train RMSE: 5.7251 | Train NMSE: 0.2050\n",
      "           λ = 1.000\n",
      "Epoch 276 | Train RMSE: 5.6972 | Train NMSE: 0.2030\n",
      "           λ = 1.000\n",
      "Epoch 277 | Train RMSE: 5.6817 | Train NMSE: 0.2019\n",
      "           λ = 1.000\n",
      "Epoch 278 | Train RMSE: 5.6591 | Train NMSE: 0.2003\n",
      "           λ = 1.000\n",
      "Epoch 279 | Train RMSE: 5.7004 | Train NMSE: 0.2033\n",
      "           λ = 1.000\n",
      "Epoch 280 | Train RMSE: 5.6854 | Train NMSE: 0.2022\n",
      "           λ = 1.000\n",
      "Epoch 281 | Train RMSE: 5.6596 | Train NMSE: 0.2004\n",
      "           λ = 1.000\n",
      "Epoch 282 | Train RMSE: 5.6910 | Train NMSE: 0.2026\n",
      "           λ = 1.000\n",
      "Epoch 283 | Train RMSE: 5.6985 | Train NMSE: 0.2031\n",
      "           λ = 1.000\n",
      "Epoch 284 | Train RMSE: 5.6603 | Train NMSE: 0.2004\n",
      "           λ = 1.000\n",
      "Epoch 285 | Train RMSE: 5.6926 | Train NMSE: 0.2027\n",
      "           λ = 1.000\n",
      "Epoch 286 | Train RMSE: 5.6639 | Train NMSE: 0.2007\n",
      "           λ = 1.000\n",
      "Epoch 287 | Train RMSE: 5.6625 | Train NMSE: 0.2006\n",
      "           λ = 1.000\n",
      "Epoch 288 | Train RMSE: 5.6497 | Train NMSE: 0.1997\n",
      "           λ = 1.000\n",
      "Epoch 289 | Train RMSE: 5.6529 | Train NMSE: 0.1999\n",
      "           λ = 1.000\n",
      "Epoch 290 | Train RMSE: 5.6669 | Train NMSE: 0.2009\n",
      "           λ = 1.000\n",
      "Epoch 291 | Train RMSE: 5.6705 | Train NMSE: 0.2011\n",
      "           λ = 1.000\n",
      "Epoch 292 | Train RMSE: 5.6611 | Train NMSE: 0.2005\n",
      "           λ = 1.000\n",
      "Epoch 293 | Train RMSE: 5.6404 | Train NMSE: 0.1990\n",
      "           λ = 1.000\n",
      "Epoch 294 | Train RMSE: 5.6805 | Train NMSE: 0.2018\n",
      "           λ = 1.000\n",
      "Epoch 295 | Train RMSE: 5.6949 | Train NMSE: 0.2029\n",
      "           λ = 1.000\n",
      "Epoch 296 | Train RMSE: 5.6601 | Train NMSE: 0.2004\n",
      "           λ = 1.000\n",
      "Epoch 297 | Train RMSE: 5.6543 | Train NMSE: 0.2000\n",
      "           λ = 1.000\n",
      "Epoch 298 | Train RMSE: 5.6107 | Train NMSE: 0.1969\n",
      "           λ = 1.000\n",
      "Epoch 299 | Train RMSE: 5.6500 | Train NMSE: 0.1997\n",
      "           λ = 1.000\n",
      "Epoch 300 | Train RMSE: 5.6549 | Train NMSE: 0.2000\n",
      "           λ = 1.000\n",
      "Epoch 301 | Train RMSE: 5.6361 | Train NMSE: 0.1987\n",
      "           λ = 1.000\n",
      "Epoch 302 | Train RMSE: 5.6095 | Train NMSE: 0.1968\n",
      "           λ = 1.000\n",
      "Epoch 303 | Train RMSE: 5.6043 | Train NMSE: 0.1965\n",
      "           λ = 1.000\n",
      "Epoch 304 | Train RMSE: 5.6251 | Train NMSE: 0.1979\n",
      "           λ = 1.000\n",
      "Epoch 305 | Train RMSE: 5.6406 | Train NMSE: 0.1990\n",
      "           λ = 1.000\n",
      "Epoch 306 | Train RMSE: 5.6316 | Train NMSE: 0.1984\n",
      "           λ = 1.000\n",
      "Epoch 307 | Train RMSE: 5.6571 | Train NMSE: 0.2002\n",
      "           λ = 1.000\n",
      "Epoch 308 | Train RMSE: 5.6013 | Train NMSE: 0.1963\n",
      "           λ = 1.000\n",
      "Epoch 309 | Train RMSE: 5.5898 | Train NMSE: 0.1954\n",
      "           λ = 1.000\n",
      "Epoch 310 | Train RMSE: 5.6250 | Train NMSE: 0.1979\n",
      "           λ = 1.000\n",
      "Epoch 311 | Train RMSE: 5.6259 | Train NMSE: 0.1980\n",
      "           λ = 1.000\n",
      "Epoch 312 | Train RMSE: 5.6205 | Train NMSE: 0.1976\n",
      "           λ = 1.000\n",
      "Epoch 313 | Train RMSE: 5.6026 | Train NMSE: 0.1963\n",
      "           λ = 1.000\n",
      "Epoch 314 | Train RMSE: 5.6509 | Train NMSE: 0.1997\n",
      "           λ = 1.000\n",
      "Epoch 315 | Train RMSE: 5.5914 | Train NMSE: 0.1956\n",
      "           λ = 1.000\n",
      "Epoch 316 | Train RMSE: 5.6030 | Train NMSE: 0.1964\n",
      "           λ = 1.000\n",
      "Epoch 317 | Train RMSE: 5.6382 | Train NMSE: 0.1988\n",
      "           λ = 1.000\n",
      "Epoch 318 | Train RMSE: 5.6210 | Train NMSE: 0.1976\n",
      "           λ = 1.000\n",
      "Epoch 319 | Train RMSE: 5.6235 | Train NMSE: 0.1978\n",
      "           λ = 1.000\n",
      "Epoch 320 | Train RMSE: 5.6141 | Train NMSE: 0.1971\n",
      "           λ = 1.000\n",
      "Epoch 321 | Train RMSE: 5.5832 | Train NMSE: 0.1950\n",
      "           λ = 1.000\n",
      "Epoch 322 | Train RMSE: 5.6016 | Train NMSE: 0.1963\n",
      "           λ = 1.000\n",
      "Epoch 323 | Train RMSE: 5.5992 | Train NMSE: 0.1961\n",
      "           λ = 1.000\n",
      "Epoch 324 | Train RMSE: 5.6097 | Train NMSE: 0.1968\n",
      "           λ = 1.000\n",
      "Epoch 325 | Train RMSE: 5.5750 | Train NMSE: 0.1944\n",
      "           λ = 1.000\n",
      "Epoch 326 | Train RMSE: 5.6189 | Train NMSE: 0.1975\n",
      "           λ = 1.000\n",
      "Epoch 327 | Train RMSE: 5.6169 | Train NMSE: 0.1973\n",
      "           λ = 1.000\n",
      "Epoch 328 | Train RMSE: 5.5949 | Train NMSE: 0.1958\n",
      "           λ = 1.000\n",
      "Epoch 329 | Train RMSE: 5.6188 | Train NMSE: 0.1975\n",
      "           λ = 1.000\n",
      "Epoch 330 | Train RMSE: 5.6055 | Train NMSE: 0.1965\n",
      "           λ = 1.000\n",
      "Epoch 331 | Train RMSE: 5.5854 | Train NMSE: 0.1951\n",
      "           λ = 1.000\n",
      "Epoch 332 | Train RMSE: 5.5663 | Train NMSE: 0.1938\n",
      "           λ = 1.000\n",
      "Epoch 333 | Train RMSE: 5.5684 | Train NMSE: 0.1940\n",
      "           λ = 1.000\n",
      "Epoch 334 | Train RMSE: 5.5962 | Train NMSE: 0.1959\n",
      "           λ = 1.000\n",
      "Epoch 335 | Train RMSE: 5.5692 | Train NMSE: 0.1940\n",
      "           λ = 1.000\n",
      "Epoch 336 | Train RMSE: 5.5665 | Train NMSE: 0.1938\n",
      "           λ = 1.000\n",
      "Epoch 337 | Train RMSE: 5.5798 | Train NMSE: 0.1947\n",
      "           λ = 1.000\n",
      "Epoch 338 | Train RMSE: 5.5730 | Train NMSE: 0.1943\n",
      "           λ = 1.000\n",
      "Epoch 339 | Train RMSE: 5.5605 | Train NMSE: 0.1934\n",
      "           λ = 1.000\n",
      "Epoch 340 | Train RMSE: 5.5743 | Train NMSE: 0.1944\n",
      "           λ = 1.000\n",
      "Epoch 341 | Train RMSE: 5.6149 | Train NMSE: 0.1972\n",
      "           λ = 1.000\n",
      "Epoch 342 | Train RMSE: 5.6041 | Train NMSE: 0.1964\n",
      "           λ = 1.000\n",
      "Epoch 343 | Train RMSE: 5.6151 | Train NMSE: 0.1972\n",
      "           λ = 1.000\n",
      "Epoch 344 | Train RMSE: 5.5551 | Train NMSE: 0.1930\n",
      "           λ = 1.000\n",
      "Epoch 345 | Train RMSE: 5.5606 | Train NMSE: 0.1934\n",
      "           λ = 1.000\n",
      "Epoch 346 | Train RMSE: 5.5579 | Train NMSE: 0.1932\n",
      "           λ = 1.000\n",
      "Epoch 347 | Train RMSE: 5.5661 | Train NMSE: 0.1938\n",
      "           λ = 1.000\n",
      "Epoch 348 | Train RMSE: 5.5714 | Train NMSE: 0.1942\n",
      "           λ = 1.000\n",
      "Epoch 349 | Train RMSE: 5.5513 | Train NMSE: 0.1928\n",
      "           λ = 1.000\n",
      "Epoch 350 | Train RMSE: 5.5562 | Train NMSE: 0.1931\n",
      "           λ = 1.000\n",
      "Epoch 351 | Train RMSE: 5.5663 | Train NMSE: 0.1938\n",
      "           λ = 1.000\n",
      "Epoch 352 | Train RMSE: 5.5776 | Train NMSE: 0.1946\n",
      "           λ = 1.000\n",
      "Epoch 353 | Train RMSE: 5.5761 | Train NMSE: 0.1945\n",
      "           λ = 1.000\n",
      "Epoch 354 | Train RMSE: 5.5189 | Train NMSE: 0.1905\n",
      "           λ = 1.000\n",
      "Epoch 355 | Train RMSE: 5.5895 | Train NMSE: 0.1954\n",
      "           λ = 1.000\n",
      "Epoch 356 | Train RMSE: 5.5816 | Train NMSE: 0.1949\n",
      "           λ = 1.000\n",
      "Epoch 357 | Train RMSE: 5.5637 | Train NMSE: 0.1936\n",
      "           λ = 1.000\n",
      "Epoch 358 | Train RMSE: 5.5655 | Train NMSE: 0.1937\n",
      "           λ = 1.000\n",
      "Epoch 359 | Train RMSE: 5.5453 | Train NMSE: 0.1923\n",
      "           λ = 1.000\n",
      "Epoch 360 | Train RMSE: 5.5588 | Train NMSE: 0.1933\n",
      "           λ = 1.000\n",
      "Epoch 361 | Train RMSE: 5.5536 | Train NMSE: 0.1929\n",
      "           λ = 1.000\n",
      "Epoch 362 | Train RMSE: 5.5758 | Train NMSE: 0.1945\n",
      "           λ = 1.000\n",
      "Epoch 363 | Train RMSE: 5.5586 | Train NMSE: 0.1933\n",
      "           λ = 1.000\n",
      "Epoch 364 | Train RMSE: 5.5348 | Train NMSE: 0.1916\n",
      "           λ = 1.000\n",
      "Epoch 365 | Train RMSE: 5.5520 | Train NMSE: 0.1928\n",
      "           λ = 1.000\n",
      "Epoch 366 | Train RMSE: 5.5452 | Train NMSE: 0.1923\n",
      "           λ = 1.000\n",
      "Epoch 367 | Train RMSE: 5.5126 | Train NMSE: 0.1901\n",
      "           λ = 1.000\n",
      "Epoch 368 | Train RMSE: 5.6009 | Train NMSE: 0.1962\n",
      "           λ = 1.000\n",
      "Epoch 369 | Train RMSE: 5.5441 | Train NMSE: 0.1923\n",
      "           λ = 1.000\n",
      "Epoch 370 | Train RMSE: 5.5453 | Train NMSE: 0.1923\n",
      "           λ = 1.000\n",
      "Epoch 371 | Train RMSE: 5.5220 | Train NMSE: 0.1907\n",
      "           λ = 1.000\n",
      "Epoch 372 | Train RMSE: 5.5352 | Train NMSE: 0.1916\n",
      "           λ = 1.000\n",
      "Epoch 373 | Train RMSE: 5.5248 | Train NMSE: 0.1909\n",
      "           λ = 1.000\n",
      "Epoch 374 | Train RMSE: 5.5262 | Train NMSE: 0.1910\n",
      "           λ = 1.000\n",
      "Epoch 375 | Train RMSE: 5.5303 | Train NMSE: 0.1913\n",
      "           λ = 1.000\n",
      "Epoch 376 | Train RMSE: 5.4970 | Train NMSE: 0.1890\n",
      "           λ = 1.000\n",
      "Epoch 377 | Train RMSE: 5.5274 | Train NMSE: 0.1911\n",
      "           λ = 1.000\n",
      "Epoch 378 | Train RMSE: 5.5316 | Train NMSE: 0.1914\n",
      "           λ = 1.000\n",
      "Epoch 379 | Train RMSE: 5.5224 | Train NMSE: 0.1908\n",
      "           λ = 1.000\n",
      "Epoch 380 | Train RMSE: 5.4963 | Train NMSE: 0.1890\n",
      "           λ = 1.000\n",
      "Epoch 381 | Train RMSE: 5.5177 | Train NMSE: 0.1904\n",
      "           λ = 1.000\n",
      "Epoch 382 | Train RMSE: 5.5193 | Train NMSE: 0.1905\n",
      "           λ = 1.000\n",
      "Epoch 383 | Train RMSE: 5.5391 | Train NMSE: 0.1919\n",
      "           λ = 1.000\n",
      "Epoch 384 | Train RMSE: 5.5264 | Train NMSE: 0.1910\n",
      "           λ = 1.000\n",
      "Epoch 385 | Train RMSE: 5.5320 | Train NMSE: 0.1914\n",
      "           λ = 1.000\n",
      "Epoch 386 | Train RMSE: 5.5417 | Train NMSE: 0.1921\n",
      "           λ = 1.000\n",
      "Epoch 387 | Train RMSE: 5.5283 | Train NMSE: 0.1912\n",
      "           λ = 1.000\n",
      "Epoch 388 | Train RMSE: 5.5173 | Train NMSE: 0.1904\n",
      "           λ = 1.000\n",
      "Epoch 389 | Train RMSE: 5.4845 | Train NMSE: 0.1882\n",
      "           λ = 1.000\n",
      "Epoch 390 | Train RMSE: 5.4933 | Train NMSE: 0.1888\n",
      "           λ = 1.000\n",
      "Epoch 391 | Train RMSE: 5.5273 | Train NMSE: 0.1911\n",
      "           λ = 1.000\n",
      "Epoch 392 | Train RMSE: 5.5220 | Train NMSE: 0.1907\n",
      "           λ = 1.000\n",
      "Epoch 393 | Train RMSE: 5.5098 | Train NMSE: 0.1899\n",
      "           λ = 1.000\n",
      "Epoch 394 | Train RMSE: 5.5284 | Train NMSE: 0.1912\n",
      "           λ = 1.000\n",
      "Epoch 395 | Train RMSE: 5.4966 | Train NMSE: 0.1890\n",
      "           λ = 1.000\n",
      "Epoch 396 | Train RMSE: 5.5232 | Train NMSE: 0.1908\n",
      "           λ = 1.000\n",
      "Epoch 397 | Train RMSE: 5.5235 | Train NMSE: 0.1908\n",
      "           λ = 1.000\n",
      "Epoch 398 | Train RMSE: 5.4974 | Train NMSE: 0.1890\n",
      "           λ = 1.000\n",
      "Epoch 399 | Train RMSE: 5.5013 | Train NMSE: 0.1893\n",
      "           λ = 1.000\n",
      "Epoch 400 | Train RMSE: 5.5235 | Train NMSE: 0.1908\n",
      "           λ = 1.000\n",
      "Epoch 401 | Train RMSE: 5.5192 | Train NMSE: 0.1905\n",
      "           λ = 1.000\n",
      "Epoch 402 | Train RMSE: 5.5081 | Train NMSE: 0.1898\n",
      "           λ = 1.000\n",
      "Epoch 403 | Train RMSE: 5.4940 | Train NMSE: 0.1888\n",
      "           λ = 1.000\n",
      "Epoch 404 | Train RMSE: 5.5100 | Train NMSE: 0.1899\n",
      "           λ = 1.000\n",
      "Epoch 405 | Train RMSE: 5.5349 | Train NMSE: 0.1916\n",
      "           λ = 1.000\n",
      "Epoch 406 | Train RMSE: 5.4838 | Train NMSE: 0.1881\n",
      "           λ = 1.000\n",
      "Epoch 407 | Train RMSE: 5.4942 | Train NMSE: 0.1888\n",
      "           λ = 1.000\n",
      "Epoch 408 | Train RMSE: 5.5130 | Train NMSE: 0.1901\n",
      "           λ = 1.000\n",
      "Epoch 409 | Train RMSE: 5.4947 | Train NMSE: 0.1889\n",
      "           λ = 1.000\n",
      "Epoch 410 | Train RMSE: 5.4650 | Train NMSE: 0.1868\n",
      "           λ = 1.000\n",
      "Epoch 411 | Train RMSE: 5.4919 | Train NMSE: 0.1887\n",
      "           λ = 1.000\n",
      "Epoch 412 | Train RMSE: 5.5173 | Train NMSE: 0.1904\n",
      "           λ = 1.000\n",
      "Epoch 413 | Train RMSE: 5.4581 | Train NMSE: 0.1863\n",
      "           λ = 1.000\n",
      "Epoch 414 | Train RMSE: 5.4758 | Train NMSE: 0.1876\n",
      "           λ = 1.000\n",
      "Epoch 415 | Train RMSE: 5.4939 | Train NMSE: 0.1888\n",
      "           λ = 1.000\n",
      "Epoch 416 | Train RMSE: 5.4849 | Train NMSE: 0.1882\n",
      "           λ = 1.000\n",
      "Epoch 417 | Train RMSE: 5.4689 | Train NMSE: 0.1871\n",
      "           λ = 1.000\n",
      "Epoch 418 | Train RMSE: 5.5151 | Train NMSE: 0.1903\n",
      "           λ = 1.000\n",
      "Epoch 419 | Train RMSE: 5.4689 | Train NMSE: 0.1871\n",
      "           λ = 1.000\n",
      "Epoch 420 | Train RMSE: 5.4763 | Train NMSE: 0.1876\n",
      "           λ = 1.000\n",
      "Epoch 421 | Train RMSE: 5.4913 | Train NMSE: 0.1886\n",
      "           λ = 1.000\n",
      "Epoch 422 | Train RMSE: 5.4831 | Train NMSE: 0.1881\n",
      "           λ = 1.000\n",
      "Epoch 423 | Train RMSE: 5.5069 | Train NMSE: 0.1897\n",
      "           λ = 1.000\n",
      "Epoch 424 | Train RMSE: 5.4498 | Train NMSE: 0.1858\n",
      "           λ = 1.000\n",
      "Epoch 425 | Train RMSE: 5.4948 | Train NMSE: 0.1889\n",
      "           λ = 1.000\n",
      "Epoch 426 | Train RMSE: 5.4994 | Train NMSE: 0.1892\n",
      "           λ = 1.000\n",
      "Epoch 427 | Train RMSE: 5.4631 | Train NMSE: 0.1867\n",
      "           λ = 1.000\n",
      "Epoch 428 | Train RMSE: 5.4972 | Train NMSE: 0.1890\n",
      "           λ = 1.000\n",
      "Epoch 429 | Train RMSE: 5.4809 | Train NMSE: 0.1879\n",
      "           λ = 1.000\n",
      "Epoch 430 | Train RMSE: 5.4719 | Train NMSE: 0.1873\n",
      "           λ = 1.000\n",
      "Epoch 431 | Train RMSE: 5.4541 | Train NMSE: 0.1861\n",
      "           λ = 1.000\n",
      "Epoch 432 | Train RMSE: 5.4868 | Train NMSE: 0.1883\n",
      "           λ = 1.000\n",
      "Epoch 433 | Train RMSE: 5.4970 | Train NMSE: 0.1890\n",
      "           λ = 1.000\n",
      "Epoch 434 | Train RMSE: 5.4848 | Train NMSE: 0.1882\n",
      "           λ = 1.000\n",
      "Epoch 435 | Train RMSE: 5.5165 | Train NMSE: 0.1904\n",
      "           λ = 1.000\n",
      "Epoch 436 | Train RMSE: 5.4532 | Train NMSE: 0.1860\n",
      "           λ = 1.000\n",
      "Epoch 437 | Train RMSE: 5.4613 | Train NMSE: 0.1866\n",
      "           λ = 1.000\n",
      "Epoch 438 | Train RMSE: 5.5099 | Train NMSE: 0.1899\n",
      "           λ = 1.000\n",
      "Epoch 439 | Train RMSE: 5.4733 | Train NMSE: 0.1874\n",
      "           λ = 1.000\n",
      "Epoch 440 | Train RMSE: 5.4546 | Train NMSE: 0.1861\n",
      "           λ = 1.000\n",
      "Epoch 441 | Train RMSE: 5.4376 | Train NMSE: 0.1849\n",
      "           λ = 1.000\n",
      "Epoch 442 | Train RMSE: 5.4550 | Train NMSE: 0.1861\n",
      "           λ = 1.000\n",
      "Epoch 443 | Train RMSE: 5.4583 | Train NMSE: 0.1864\n",
      "           λ = 1.000\n",
      "Epoch 444 | Train RMSE: 5.4578 | Train NMSE: 0.1863\n",
      "           λ = 1.000\n",
      "Epoch 445 | Train RMSE: 5.4869 | Train NMSE: 0.1883\n",
      "           λ = 1.000\n",
      "Epoch 446 | Train RMSE: 5.5082 | Train NMSE: 0.1898\n",
      "           λ = 1.000\n",
      "Epoch 447 | Train RMSE: 5.4821 | Train NMSE: 0.1880\n",
      "           λ = 1.000\n",
      "Epoch 448 | Train RMSE: 5.4818 | Train NMSE: 0.1880\n",
      "           λ = 1.000\n",
      "Epoch 449 | Train RMSE: 5.4638 | Train NMSE: 0.1867\n",
      "           λ = 1.000\n",
      "Epoch 450 | Train RMSE: 5.4729 | Train NMSE: 0.1874\n",
      "           λ = 1.000\n",
      "Epoch 451 | Train RMSE: 5.4739 | Train NMSE: 0.1874\n",
      "           λ = 1.000\n",
      "Epoch 452 | Train RMSE: 5.4403 | Train NMSE: 0.1851\n",
      "           λ = 1.000\n",
      "Epoch 453 | Train RMSE: 5.4692 | Train NMSE: 0.1871\n",
      "           λ = 1.000\n",
      "Epoch 454 | Train RMSE: 5.4851 | Train NMSE: 0.1882\n",
      "           λ = 1.000\n",
      "Epoch 455 | Train RMSE: 5.4601 | Train NMSE: 0.1865\n",
      "           λ = 1.000\n",
      "Epoch 456 | Train RMSE: 5.4604 | Train NMSE: 0.1865\n",
      "           λ = 1.000\n",
      "Epoch 457 | Train RMSE: 5.4363 | Train NMSE: 0.1849\n",
      "           λ = 1.000\n",
      "Epoch 458 | Train RMSE: 5.5112 | Train NMSE: 0.1900\n",
      "           λ = 1.000\n",
      "Epoch 459 | Train RMSE: 5.4125 | Train NMSE: 0.1832\n",
      "           λ = 1.000\n",
      "Epoch 460 | Train RMSE: 5.4447 | Train NMSE: 0.1854\n",
      "           λ = 1.000\n",
      "Epoch 461 | Train RMSE: 5.4537 | Train NMSE: 0.1860\n",
      "           λ = 1.000\n",
      "Epoch 462 | Train RMSE: 5.4597 | Train NMSE: 0.1864\n",
      "           λ = 1.000\n",
      "Epoch 463 | Train RMSE: 5.4946 | Train NMSE: 0.1888\n",
      "           λ = 1.000\n",
      "Epoch 464 | Train RMSE: 5.4416 | Train NMSE: 0.1852\n",
      "           λ = 1.000\n",
      "Epoch 465 | Train RMSE: 5.4232 | Train NMSE: 0.1840\n",
      "           λ = 1.000\n",
      "Epoch 466 | Train RMSE: 5.4771 | Train NMSE: 0.1876\n",
      "           λ = 1.000\n",
      "Epoch 467 | Train RMSE: 5.4542 | Train NMSE: 0.1861\n",
      "           λ = 1.000\n",
      "Epoch 468 | Train RMSE: 5.4393 | Train NMSE: 0.1851\n",
      "           λ = 1.000\n",
      "Epoch 469 | Train RMSE: 5.4132 | Train NMSE: 0.1833\n",
      "           λ = 1.000\n",
      "Epoch 470 | Train RMSE: 5.4386 | Train NMSE: 0.1850\n",
      "           λ = 1.000\n",
      "Epoch 471 | Train RMSE: 5.4295 | Train NMSE: 0.1844\n",
      "           λ = 1.000\n",
      "Epoch 472 | Train RMSE: 5.4274 | Train NMSE: 0.1842\n",
      "           λ = 1.000\n",
      "Epoch 473 | Train RMSE: 5.4223 | Train NMSE: 0.1839\n",
      "           λ = 1.000\n",
      "Epoch 474 | Train RMSE: 5.4378 | Train NMSE: 0.1850\n",
      "           λ = 1.000\n",
      "Epoch 475 | Train RMSE: 5.4669 | Train NMSE: 0.1869\n",
      "           λ = 1.000\n",
      "Epoch 476 | Train RMSE: 5.4681 | Train NMSE: 0.1870\n",
      "           λ = 1.000\n",
      "Epoch 477 | Train RMSE: 5.4501 | Train NMSE: 0.1858\n",
      "           λ = 1.000\n",
      "Epoch 478 | Train RMSE: 5.4464 | Train NMSE: 0.1855\n",
      "           λ = 1.000\n",
      "Epoch 479 | Train RMSE: 5.4316 | Train NMSE: 0.1845\n",
      "           λ = 1.000\n",
      "Epoch 480 | Train RMSE: 5.4573 | Train NMSE: 0.1863\n",
      "           λ = 1.000\n",
      "Epoch 481 | Train RMSE: 5.4354 | Train NMSE: 0.1848\n",
      "           λ = 1.000\n",
      "Epoch 482 | Train RMSE: 5.4407 | Train NMSE: 0.1852\n",
      "           λ = 1.000\n",
      "Epoch 483 | Train RMSE: 5.4545 | Train NMSE: 0.1861\n",
      "           λ = 1.000\n",
      "Epoch 484 | Train RMSE: 5.4142 | Train NMSE: 0.1834\n",
      "           λ = 1.000\n",
      "Epoch 485 | Train RMSE: 5.4389 | Train NMSE: 0.1850\n",
      "           λ = 1.000\n",
      "Epoch 486 | Train RMSE: 5.4129 | Train NMSE: 0.1833\n",
      "           λ = 1.000\n",
      "Epoch 487 | Train RMSE: 5.4426 | Train NMSE: 0.1853\n",
      "           λ = 1.000\n",
      "Epoch 488 | Train RMSE: 5.4483 | Train NMSE: 0.1857\n",
      "           λ = 1.000\n",
      "Epoch 489 | Train RMSE: 5.4455 | Train NMSE: 0.1855\n",
      "           λ = 1.000\n",
      "Epoch 490 | Train RMSE: 5.4582 | Train NMSE: 0.1863\n",
      "           λ = 1.000\n",
      "Epoch 491 | Train RMSE: 5.4007 | Train NMSE: 0.1824\n",
      "           λ = 1.000\n",
      "Epoch 492 | Train RMSE: 5.4222 | Train NMSE: 0.1839\n",
      "           λ = 1.000\n",
      "Epoch 493 | Train RMSE: 5.4611 | Train NMSE: 0.1865\n",
      "           λ = 1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from config.models import DANNTrainer\n",
    "\n",
    "# --- Load full training set ---\n",
    "PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "DATASET = 'freemoves'\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "tw_extractor = TimeWindowTransformer(size=500, step=50)\n",
    "label_extractor = LabelWindowExtractor(size=500, step=50)\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)\n",
    "\n",
    "# X_all = X_windows.reshape(-1, 8, 500)\n",
    "# Y_all = Y_labels.reshape(-1, 51)\n",
    "\n",
    "session_ids = np.concatenate([\n",
    "    np.full(X_windows[i].shape[0], i) for i in range(len(X_windows))\n",
    "])\n",
    "\n",
    "class DANNWindowDataset2(Dataset):\n",
    "    def __init__(self, X, Y, session_ids):\n",
    "        self.X = X.reshape(-1, *X.shape[2:])  # (N, 8, 500)\n",
    "        self.Y = Y.reshape(-1, Y.shape[-1])   # (N, 51)\n",
    "        self.session_ids = torch.tensor(session_ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.Y[idx], dtype=torch.float32),\n",
    "            self.session_ids[idx],\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = DANNWindowDataset2(X_windows, Y_labels, session_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "dummy_X = np.empty((1, 1, 8, 500), dtype=np.float32)\n",
    "dummy_Y = np.empty((1, 1, 51), dtype=np.float32)\n",
    "dummy_ids = np.zeros((1, 1), dtype=int)\n",
    "\n",
    "\n",
    "# --- Define model ---\n",
    "model = DANNModel(lambda_grl=1.0, num_domains=5, output_dim=51)\n",
    "\n",
    "trainer = DANNTrainer2(\n",
    "    model=model,\n",
    "    X=dummy_X,\n",
    "    Y=dummy_Y,\n",
    "    train_sessions=[0],\n",
    "    val_session=0,\n",
    "    lambda_grl=1.0,\n",
    "    gamma_entropy=0.0,\n",
    "    batch_size=512,\n",
    "    max_epochs=500,\n",
    "    patience=10,\n",
    "    learning_rate=1e-3,\n",
    "    device='cuda',\n",
    "    tensor_dataset=DANNWindowDataset2\n",
    ")\n",
    "\n",
    "\n",
    "# Inject trainer internals\n",
    "trainer.train_loader = train_loader\n",
    "trainer.val_loader = None\n",
    "trainer.reg_loss = torch.nn.MSELoss()\n",
    "trainer.dom_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Train on full data ---\n",
    "trainer.train(validate=False)\n",
    "\n",
    "# --- Load test set ---\n",
    "X_test = np.load(PATH + f\"{DATASET}/{DATASET}_testset_X.npy\")\n",
    "X_test_windows = tw_extractor.transform(X_test).reshape(-1, 8, 500)\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test_windows, dtype=torch.float32)),\n",
    "    batch_size=128, shuffle=False\n",
    ")\n",
    "\n",
    "# --- Predict and save ---\n",
    "predictions = trainer.predict(test_loader)\n",
    "np.save(PATH + f\"{DATASET}_testset_predictions.npy\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

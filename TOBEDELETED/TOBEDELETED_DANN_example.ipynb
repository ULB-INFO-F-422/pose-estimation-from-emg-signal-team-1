{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats import entropy, kurtosis\n",
    "from config.transformers import *\n",
    "from config.models import *\n",
    "from config.loss_functions import *\n",
    "from config.validation import *\n",
    "from config.regressors import *\n",
    "from config.dann import *\n",
    "import numpy as np\n",
    "import pyriemann\n",
    "import pywt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "DATASET = 'freemoves' # change this to guided/freemoves if needed\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "tw_extractor = TimeWindowTransformer(size = 500, step = 50)\n",
    "label_extractor = LabelWindowExtractor(size = 500, step = 50)\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 | Train on [1, 2, 3, 4], Validate on 0 ===\n",
      "Epoch 01 | Train Loss: 659.3960 | Val RMSE: 24.3763 | Dom Train Acc: 46.08%\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train Loss: 600.7444 | Val RMSE: 22.9183 | Dom Train Acc: 46.62%\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train Loss: 536.4845 | Val RMSE: 22.4063 | Dom Train Acc: 50.02%\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train Loss: 470.5566 | Val RMSE: 20.5542 | Dom Train Acc: 49.53%\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train Loss: 403.7856 | Val RMSE: 19.3817 | Dom Train Acc: 51.23%\n",
      "           λ = 0.500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rmse_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_dann\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDANNWindowDataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\pose-estimation-from-emg-signal-team-1\\TOBEDELETED\\..\\config\\models.py:909\u001b[0m, in \u001b[0;36mcross_validate_dann\u001b[1;34m(X, Y, tensor_dataset, lambda_grl, max_epochs, patience, batch_size)\u001b[0m\n\u001b[0;32m    895\u001b[0m     model \u001b[38;5;241m=\u001b[39m DANNModel(lambda_grl\u001b[38;5;241m=\u001b[39mlambda_grl, num_domains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39mY\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    896\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m DANNTrainer(\n\u001b[0;32m    897\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    898\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    906\u001b[0m         tensor_dataset\u001b[38;5;241m=\u001b[39mtensor_dataset,\n\u001b[0;32m    907\u001b[0m     )\n\u001b[1;32m--> 909\u001b[0m     fold_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    910\u001b[0m     rmse_scores\u001b[38;5;241m.\u001b[39mappend(fold_rmse)\n\u001b[0;32m    912\u001b[0m mean_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(rmse_scores)\n",
      "File \u001b[1;32mc:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\pose-estimation-from-emg-signal-team-1\\TOBEDELETED\\..\\config\\models.py:828\u001b[0m, in \u001b[0;36mDANNTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    825\u001b[0m     epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    827\u001b[0m val_rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m--> 828\u001b[0m domain_train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_domain_on_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# self.scheduler.step(val_rmse)\u001b[39;00m\n\u001b[0;32m    831\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(epoch_losses)  \u001b[38;5;66;03m# ✅ needed for print\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\pose-estimation-from-emg-signal-team-1\\TOBEDELETED\\..\\config\\models.py:872\u001b[0m, in \u001b[0;36mDANNTrainer.evaluate_domain_on_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, _, sid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader:\n\u001b[1;32m--> 872\u001b[0m         x, sid \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, sid\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    873\u001b[0m         _, dom_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n\u001b[0;32m    874\u001b[0m         pred_sid \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(dom_out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rmse_scores = cross_validate_dann(X_windows, Y_labels, tensor_dataset=DANNWindowDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y_pred, y_val):\n",
    "    return (np.sum((y_val - y_pred) ** 2) / np.prod(y_val.shape)) ** 0.5\n",
    "\n",
    "def NMSE(y_pred, y_val):\n",
    "    num = np.sum((y_val - y_pred) ** 2)\n",
    "    den = np.sum((y_val - np.mean(y_val, axis=0)) ** 2)\n",
    "    return num / den\n",
    "\n",
    "class DANNTrainer2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        train_sessions: list,\n",
    "        val_session: int,\n",
    "        lambda_grl: float = 0.1,\n",
    "        gamma_entropy=0.5,\n",
    "        batch_size: int = 512,\n",
    "        max_epochs: int = 50,\n",
    "        patience: int = 10,\n",
    "        learning_rate: float = 1e-3,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        tensor_dataset=None\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.lambda_grl = lambda_grl\n",
    "        self.gamma_entropy = gamma_entropy\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.reg_loss = nn.MSELoss()\n",
    "        self.dom_loss = nn.CrossEntropyLoss()\n",
    "        self.train_loader = None  # to be assigned externally\n",
    "\n",
    "    def train(self, validate=True):\n",
    "        best_val_rmse = float(\"inf\")\n",
    "        best_weights = None\n",
    "        patience_counter = 0\n",
    "\n",
    "        def compute_lambda(epoch, max_lambda=1.0, warmup_epochs=10):\n",
    "            return min(max_lambda, epoch / warmup_epochs)\n",
    "\n",
    "        for epoch in range(1, self.max_epochs + 1):\n",
    "            self.model.train()\n",
    "            train_y_true, train_y_pred = [], []\n",
    "            epoch_losses = []\n",
    "\n",
    "            lambda_grl = compute_lambda(epoch, max_lambda=1.0, warmup_epochs=10)\n",
    "\n",
    "            for x, y, sid in self.train_loader:\n",
    "                x, y, sid = x.to(self.device), y.to(self.device), sid.to(self.device)\n",
    "                y_pred, dom_pred = self.model(x, lambda_grl=lambda_grl)\n",
    "\n",
    "                loss = (\n",
    "                    self.reg_loss(y_pred, y) +\n",
    "                    lambda_grl * self.dom_loss(dom_pred, sid)\n",
    "                )\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_losses.append(loss.item())\n",
    "\n",
    "                train_y_true.append(y.detach().cpu().numpy())\n",
    "                train_y_pred.append(y_pred.detach().cpu().numpy())\n",
    "\n",
    "            train_y_true = np.vstack(train_y_true)\n",
    "            train_y_pred = np.vstack(train_y_pred)\n",
    "            train_rmse = RMSE(train_y_pred, train_y_true)\n",
    "            train_nmse = NMSE(train_y_pred, train_y_true)\n",
    "\n",
    "            if validate:\n",
    "                val_rmse = self.evaluate()\n",
    "                domain_train_acc = self.evaluate_domain_on_train()\n",
    "                print(f\"Epoch {epoch:02d} | Train RMSE: {train_rmse:.4f} | Train NMSE: {train_nmse:.4f} | Val RMSE: {val_rmse:.4f} | Dom Train Acc: {domain_train_acc:.2%}\")\n",
    "                print(f\"           λ = {lambda_grl:.3f}\")\n",
    "\n",
    "                if val_rmse < best_val_rmse:\n",
    "                    best_val_rmse = val_rmse\n",
    "                    best_weights = self.model.state_dict()\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= self.patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            else:\n",
    "                print(f\"Epoch {epoch:02d} | Train RMSE: {train_rmse:.4f} | Train NMSE: {train_nmse:.4f}\")\n",
    "                print(f\"           λ = {lambda_grl:.3f}\")\n",
    "\n",
    "        if validate and best_weights is not None:\n",
    "            self.model.load_state_dict(best_weights)\n",
    "        return best_val_rmse if validate else self.model\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for x, *_ in data_loader:\n",
    "                x = x.to(self.device)\n",
    "                preds = self.model(x)[0]  # get y_pred only\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "        return np.vstack(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train RMSE: 23.1886 | Train NMSE: 3.3634\n",
      "           λ = 0.100\n",
      "Epoch 02 | Train RMSE: 16.4147 | Train NMSE: 1.6854\n",
      "           λ = 0.200\n",
      "Epoch 03 | Train RMSE: 11.9771 | Train NMSE: 0.8973\n",
      "           λ = 0.300\n",
      "Epoch 04 | Train RMSE: 10.4710 | Train NMSE: 0.6858\n",
      "           λ = 0.400\n",
      "Epoch 05 | Train RMSE: 9.9555 | Train NMSE: 0.6199\n",
      "           λ = 0.500\n",
      "Epoch 06 | Train RMSE: 9.7235 | Train NMSE: 0.5914\n",
      "           λ = 0.600\n",
      "Epoch 07 | Train RMSE: 9.5032 | Train NMSE: 0.5649\n",
      "           λ = 0.700\n",
      "Epoch 08 | Train RMSE: 9.3601 | Train NMSE: 0.5480\n",
      "           λ = 0.800\n",
      "Epoch 09 | Train RMSE: 9.2284 | Train NMSE: 0.5327\n",
      "           λ = 0.900\n",
      "Epoch 10 | Train RMSE: 9.1559 | Train NMSE: 0.5244\n",
      "           λ = 1.000\n",
      "Epoch 11 | Train RMSE: 9.0263 | Train NMSE: 0.5096\n",
      "           λ = 1.000\n",
      "Epoch 12 | Train RMSE: 8.9721 | Train NMSE: 0.5035\n",
      "           λ = 1.000\n",
      "Epoch 13 | Train RMSE: 8.8688 | Train NMSE: 0.4920\n",
      "           λ = 1.000\n",
      "Epoch 14 | Train RMSE: 8.7768 | Train NMSE: 0.4818\n",
      "           λ = 1.000\n",
      "Epoch 15 | Train RMSE: 8.7243 | Train NMSE: 0.4761\n",
      "           λ = 1.000\n",
      "Epoch 16 | Train RMSE: 8.6456 | Train NMSE: 0.4675\n",
      "           λ = 1.000\n",
      "Epoch 17 | Train RMSE: 8.6028 | Train NMSE: 0.4629\n",
      "           λ = 1.000\n",
      "Epoch 18 | Train RMSE: 8.5712 | Train NMSE: 0.4595\n",
      "           λ = 1.000\n",
      "Epoch 19 | Train RMSE: 8.4813 | Train NMSE: 0.4499\n",
      "           λ = 1.000\n",
      "Epoch 20 | Train RMSE: 8.4431 | Train NMSE: 0.4459\n",
      "           λ = 1.000\n",
      "Epoch 21 | Train RMSE: 8.4208 | Train NMSE: 0.4435\n",
      "           λ = 1.000\n",
      "Epoch 22 | Train RMSE: 8.3502 | Train NMSE: 0.4361\n",
      "           λ = 1.000\n",
      "Epoch 23 | Train RMSE: 8.3495 | Train NMSE: 0.4361\n",
      "           λ = 1.000\n",
      "Epoch 24 | Train RMSE: 8.3373 | Train NMSE: 0.4348\n",
      "           λ = 1.000\n",
      "Epoch 25 | Train RMSE: 8.2708 | Train NMSE: 0.4279\n",
      "           λ = 1.000\n",
      "Epoch 26 | Train RMSE: 8.2652 | Train NMSE: 0.4273\n",
      "           λ = 1.000\n",
      "Epoch 27 | Train RMSE: 8.2008 | Train NMSE: 0.4207\n",
      "           λ = 1.000\n",
      "Epoch 28 | Train RMSE: 8.1853 | Train NMSE: 0.4191\n",
      "           λ = 1.000\n",
      "Epoch 29 | Train RMSE: 8.1350 | Train NMSE: 0.4139\n",
      "           λ = 1.000\n",
      "Epoch 30 | Train RMSE: 8.0956 | Train NMSE: 0.4100\n",
      "           λ = 1.000\n",
      "Epoch 31 | Train RMSE: 8.1074 | Train NMSE: 0.4111\n",
      "           λ = 1.000\n",
      "Epoch 32 | Train RMSE: 8.0414 | Train NMSE: 0.4045\n",
      "           λ = 1.000\n",
      "Epoch 33 | Train RMSE: 8.0034 | Train NMSE: 0.4007\n",
      "           λ = 1.000\n",
      "Epoch 34 | Train RMSE: 8.0117 | Train NMSE: 0.4015\n",
      "           λ = 1.000\n",
      "Epoch 35 | Train RMSE: 8.0265 | Train NMSE: 0.4030\n",
      "           λ = 1.000\n",
      "Epoch 36 | Train RMSE: 7.9639 | Train NMSE: 0.3967\n",
      "           λ = 1.000\n",
      "Epoch 37 | Train RMSE: 7.9362 | Train NMSE: 0.3940\n",
      "           λ = 1.000\n",
      "Epoch 38 | Train RMSE: 7.8924 | Train NMSE: 0.3896\n",
      "           λ = 1.000\n",
      "Epoch 39 | Train RMSE: 7.8801 | Train NMSE: 0.3884\n",
      "           λ = 1.000\n",
      "Epoch 40 | Train RMSE: 7.9227 | Train NMSE: 0.3926\n",
      "           λ = 1.000\n",
      "Epoch 41 | Train RMSE: 7.8497 | Train NMSE: 0.3854\n",
      "           λ = 1.000\n",
      "Epoch 42 | Train RMSE: 7.8337 | Train NMSE: 0.3838\n",
      "           λ = 1.000\n",
      "Epoch 43 | Train RMSE: 7.8410 | Train NMSE: 0.3846\n",
      "           λ = 1.000\n",
      "Epoch 44 | Train RMSE: 7.8019 | Train NMSE: 0.3807\n",
      "           λ = 1.000\n",
      "Epoch 45 | Train RMSE: 7.7908 | Train NMSE: 0.3797\n",
      "           λ = 1.000\n",
      "Epoch 46 | Train RMSE: 7.7464 | Train NMSE: 0.3753\n",
      "           λ = 1.000\n",
      "Epoch 47 | Train RMSE: 7.7750 | Train NMSE: 0.3781\n",
      "           λ = 1.000\n",
      "Epoch 48 | Train RMSE: 7.7497 | Train NMSE: 0.3757\n",
      "           λ = 1.000\n",
      "Epoch 49 | Train RMSE: 7.6980 | Train NMSE: 0.3707\n",
      "           λ = 1.000\n",
      "Epoch 50 | Train RMSE: 7.7287 | Train NMSE: 0.3736\n",
      "           λ = 1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from config.models import DANNTrainer\n",
    "\n",
    "# --- Load full training set ---\n",
    "PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "DATASET = 'freemoves'\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "step_prediction = 50\n",
    "tw_extractor = TimeWindowTransformer(size=500, step=step_prediction)\n",
    "label_extractor = LabelWindowExtractor(size=500, step=step_prediction)\n",
    "\n",
    "# X_transformed = EMGPreprocessor().fit_transform(X)\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)\n",
    "\n",
    "# X_all = X_windows.reshape(-1, 8, 500)\n",
    "# Y_all = Y_labels.reshape(-1, 51)\n",
    "\n",
    "session_ids = np.concatenate([\n",
    "    np.full(X_windows[i].shape[0], i) for i in range(len(X_windows))\n",
    "])\n",
    "\n",
    "class DANNWindowDataset2(Dataset):\n",
    "    def __init__(self, X, Y, session_ids):\n",
    "        self.X = X.reshape(-1, *X.shape[2:])  # (N, 8, 500)\n",
    "        self.Y = Y.reshape(-1, Y.shape[-1])   # (N, 51)\n",
    "        self.session_ids = torch.tensor(session_ids, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.X[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.Y[idx], dtype=torch.float32),\n",
    "            self.session_ids[idx],\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = DANNWindowDataset2(X_windows, Y_labels, session_ids)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "dummy_X = np.empty((1, 1, 8, 500), dtype=np.float32)\n",
    "dummy_Y = np.empty((1, 1, 51), dtype=np.float32)\n",
    "dummy_ids = np.zeros((1, 1), dtype=int)\n",
    "\n",
    "\n",
    "# --- Define model ---\n",
    "model = DANNModel(lambda_grl=1.0, num_domains=5, output_dim=51)\n",
    "\n",
    "trainer = DANNTrainer2(\n",
    "    model=model,\n",
    "    X=dummy_X,\n",
    "    Y=dummy_Y,\n",
    "    train_sessions=[0],\n",
    "    val_session=0,\n",
    "    lambda_grl=1,\n",
    "    gamma_entropy=0.4,\n",
    "    batch_size=128,\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    learning_rate=1e-3,\n",
    "    device='cuda',\n",
    "    tensor_dataset=DANNWindowDataset2\n",
    ")\n",
    "\n",
    "\n",
    "# Inject trainer internals\n",
    "trainer.train_loader = train_loader\n",
    "trainer.val_loader = None\n",
    "trainer.reg_loss = torch.nn.MSELoss()\n",
    "trainer.dom_loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# --- Train on full data ---\n",
    "trainer.train(validate=False)\n",
    "\n",
    "# --- Load test set ---\n",
    "X_test = np.load(PATH + f\"{DATASET}/{DATASET}_testset_X.npy\")\n",
    "X_test_windows = tw_extractor.transform(X_test).reshape(-1, 8, 500)\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test_windows, dtype=torch.float32)),\n",
    "    batch_size=128, shuffle=False\n",
    ")\n",
    "\n",
    "# --- Predict and save ---\n",
    "predictions = trainer.predict(test_loader)\n",
    "np.save('dann_filterAct_epoch_50_steps_' + str(step_prediction) + '_rmse_6-8.npy', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Computes raw, delta, and delta^2 along time dimension of EMG windows,\n",
    "    preserving session structure.\n",
    "\n",
    "    Input shape:  (n_sessions, n_windows, n_channels, time)\n",
    "    Output shape: (n_sessions, n_windows, n_channels * 3, time)\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # X.shape = (n_sessions, n_windows, n_channels, time)\n",
    "        raw = X  # shape: (S, W, C, T)\n",
    "        diff = np.diff(X, axis=-1, prepend=X[..., :1])  # Δ\n",
    "        diff2 = np.diff(diff, axis=-1, prepend=diff[..., :1])  # Δ²\n",
    "\n",
    "        # Stack along a new \"temporal feature\" axis: raw, Δ, Δ²\n",
    "        # Result: (S, W, C, T, 3)\n",
    "        stacked = np.stack([raw, diff, diff2], axis=-1)\n",
    "\n",
    "        # Rearrange to: (S, W, C * 3, T)\n",
    "        S, W, C, T, F = stacked.shape\n",
    "        output = stacked.transpose(0, 1, 2, 4, 3).reshape(S, W, C * F, T)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "step_prediction = 50\n",
    "tw_extractor = TimeWindowTransformer(size=500, step=step_prediction)\n",
    "label_extractor = LabelWindowExtractor(size=500, step=step_prediction)\n",
    "\n",
    "# X_transformed = EMGPreprocessor().fit_transform(X)\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)\n",
    "X_transformed = DeltaFeatureTransformer().fit_transform(X_windows)\n",
    "session_ids = np.repeat(np.arange(5), X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_dann(\n",
    "    X,\n",
    "    Y,\n",
    "    session_ids,\n",
    "    tensor_dataset,\n",
    "    model_class,\n",
    "    lambda_grl=0.3,\n",
    "    max_epochs=50,\n",
    "    patience=20,\n",
    "    batch_size=512,\n",
    "):\n",
    "    import numpy as np\n",
    "\n",
    "    n_sessions = X.shape[0]\n",
    "    rmse_scores = []\n",
    "\n",
    "    # Flatten X and Y *once*\n",
    "    n_sessions, n_windows = X.shape[:2]\n",
    "    X_flat = X.reshape(n_sessions * n_windows, *X.shape[2:])  # (S×W, C, T)\n",
    "    Y_flat = Y.reshape(n_sessions * n_windows, Y.shape[-1])   # (S×W, D)\n",
    "\n",
    "    # Rebuild session_ids if needed\n",
    "    session_ids_flat = np.repeat(np.arange(n_sessions), n_windows)\n",
    "\n",
    "    for val_session in range(n_sessions):\n",
    "        train_sessions = [s for s in range(n_sessions) if s != val_session]\n",
    "        print(f\"\\n=== Fold {val_session + 1} | Train on {train_sessions}, Validate on {val_session} ===\")\n",
    "\n",
    "        model = model_class() \n",
    "\n",
    "        trainer = DANNTrainer(\n",
    "            model=model,\n",
    "            X=None,\n",
    "            Y=None,\n",
    "            train_sessions=[],\n",
    "            val_session=None,\n",
    "            lambda_grl=lambda_grl,\n",
    "            max_epochs=max_epochs,\n",
    "            patience=patience,\n",
    "            batch_size=batch_size,\n",
    "            tensor_dataset=tensor_dataset\n",
    "        )\n",
    "\n",
    "        # Create correct boolean masks\n",
    "        train_mask = session_ids_flat != val_session\n",
    "\n",
    "        # Subset correctly\n",
    "        X_train = X_flat[train_mask]\n",
    "        Y_train = Y_flat[train_mask]\n",
    "        session_ids_train = session_ids_flat[train_mask]\n",
    "\n",
    "        train_dataset = tensor_dataset(X_train, Y_train, session_ids_train)\n",
    "        trainer.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "        val_mask = session_ids_flat == val_session\n",
    "\n",
    "        X_val = X_flat[val_mask]\n",
    "        Y_val = Y_flat[val_mask]\n",
    "        session_ids_val = session_ids_flat[val_mask]\n",
    "        \n",
    "        val_dataset = tensor_dataset(X_val, Y_val, session_ids_val)\n",
    "        trainer.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        rmse = trainer.train()\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    std_rmse = np.std(rmse_scores)\n",
    "\n",
    "    print(f\"\\n=== Cross-validated RMSE: {mean_rmse:.4f} ± {std_rmse:.4f} ===\")\n",
    "    return rmse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 | Train on [1, 2, 3, 4], Validate on 0 ===\n",
      "Epoch 01 | Train RMSE: 26.0696 | Train NMSE: 4.0955 | Val RMSE: 24.8626 | Dom Train Acc: 34.57%\n",
      "           λ = 0.100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m build_model() \n\u001b[1;32m---> 12\u001b[0m rmse_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate_dann\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_transformed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mY_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensor_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDANNWindowTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_grl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 63\u001b[0m, in \u001b[0;36mcross_validate_dann\u001b[1;34m(X, Y, session_ids, tensor_dataset, model_class, lambda_grl, max_epochs, patience, batch_size)\u001b[0m\n\u001b[0;32m     60\u001b[0m     val_dataset \u001b[38;5;241m=\u001b[39m tensor_dataset(X_val, Y_val, session_ids_val)\n\u001b[0;32m     61\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mval_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 63\u001b[0m     rmse \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     rmse_scores\u001b[38;5;241m.\u001b[39mappend(rmse)\n\u001b[0;32m     66\u001b[0m mean_rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(rmse_scores)\n",
      "File \u001b[1;32mc:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\pose-estimation-from-emg-signal-team-1\\TOBEDELETED\\..\\config\\dann.py:253\u001b[0m, in \u001b[0;36mDANNTrainer.train\u001b[1;34m(self, validate)\u001b[0m\n\u001b[0;32m    249\u001b[0m epoch_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    251\u001b[0m lambda_grl \u001b[38;5;241m=\u001b[39m compute_lambda(epoch, max_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, warmup_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m--> 253\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdom_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_grl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_grl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:493\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:424\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1171\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1164\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1171\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def build_model():\n",
    "    model = TemporalDANNModel(lambda_grl=0.3, num_domains=5, output_dim=51)\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "model = build_model() \n",
    "\n",
    "rmse_scores = cross_validate_dann(\n",
    "    X_transformed,\n",
    "    Y_labels,\n",
    "    session_ids=session_ids,\n",
    "    tensor_dataset=DANNWindowTensor,\n",
    "    model_class=build_model,\n",
    "    lambda_grl=0.3,\n",
    "    max_epochs=50,\n",
    "    patience=20,\n",
    "    batch_size=2048\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

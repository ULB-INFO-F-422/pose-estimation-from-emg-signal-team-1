{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c9720ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "204f8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from config.regressors import NNRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f5fbd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: [3 → 64 → 32 → 16 → latent_dim]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(4, latent_dim)  # No activation\n",
    "        )\n",
    "        \n",
    "        # Decoder: [latent_dim → 16 → 32 → 64 → 3]\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(4, 8),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(16, 3)  # No activation\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # LeCun normal initialization for SELU\n",
    "                nn.init.normal_(m.weight, 0, std=1. / np.sqrt(m.in_features))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0adeec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(bone, latent_dim):\n",
    "    # ----------------------------\n",
    "    # 1. Configuration\n",
    "    # ----------------------------\n",
    "    device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size     = 4096\n",
    "    learning_rate  = 1e-3\n",
    "    weight_decay   = 1e-4\n",
    "    max_epochs     = 100\n",
    "    patience       = 20\n",
    "    num_workers    = 4  # >0 for parallel loading\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Prepare Data (CPU‑only)\n",
    "    # ----------------------------\n",
    "    # Assume Y is your NumPy array of shape [N, 3*B]\n",
    "    PATH = f'/Users/marco/PROJECTS/data/'\n",
    "    Y = np.load(PATH + 'Y_all.npy')\n",
    "    Y_bone = Y[:, bone*3:(bone+1)*3]\n",
    "\n",
    "    Yb_cpu   = torch.as_tensor(Y_bone, dtype=torch.float32)  # stays on CPU\n",
    "\n",
    "    dataset      = TensorDataset(Yb_cpu, Yb_cpu)\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    val_loader   = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    # instantiate with your chosen latent dimension\n",
    "    model = AutoEncoder(latent_dim).to(device)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Loss & Optimizer\n",
    "    # ----------------------------\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Training Loop w/ Early Stopping\n",
    "    # ----------------------------\n",
    "    best_val_loss     = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        # —— Training —— \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for xb_cpu, yb_cpu in train_loader:\n",
    "            # move batch to MPS (or CUDA) on-the-fly\n",
    "            xb = xb_cpu.to(device, non_blocking=True)\n",
    "            yb = yb_cpu.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)              \n",
    "            loss  = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # —— Validation ——\n",
    "        model.eval()\n",
    "        running_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb_cpu, yb_cpu in val_loader:\n",
    "                xb = xb_cpu.to(device, non_blocking=True)\n",
    "                yb = yb_cpu.to(device, non_blocking=True)\n",
    "                running_val += criterion(model(xb), yb).item()\n",
    "        avg_val_loss = running_val / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: Train MSE = {avg_train_loss:.6f} | Val MSE = {avg_val_loss:.6f}\")\n",
    "\n",
    "        # —— Early Stopping ——\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss     = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f\"best_models/best_model_bone{bone}.pth\")\n",
    "            print(\"  → New best model saved\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "331038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(Y):\n",
    "    latent_dims = {\n",
    "        0: 1, 1: 1, 2: 1,\n",
    "        3: 1, 4: 2, 5: 1,\n",
    "        6: 1, 7: 2, 8: 1,\n",
    "        9: 1, 10: 2, 11: 1,\n",
    "        12: 1, 13: 1, 14: 2,\n",
    "        15: 1, 16: 1,}\n",
    "    \n",
    "    Y_shape = Y.shape\n",
    "    \n",
    "    Y_enc = []\n",
    "    for bone in range(17):\n",
    "        latent_dim = latent_dims[bone]\n",
    "        model = AutoEncoder(latent_dim=latent_dim).to('mps')\n",
    "        model.load_state_dict(torch.load(f'best_models/best_model_bone{bone}.pth'))\n",
    "        \n",
    "        to_encode = Y[..., 3*bone:3*(bone+1)].reshape(-1,3)\n",
    "        to_encode_tensor = torch.tensor(\n",
    "            to_encode,\n",
    "            dtype=torch.float,\n",
    "            device='mps')\n",
    "        encoded_tensor = model.encoder(to_encode_tensor)\n",
    "        Y_enc.append(encoded_tensor.cpu().detach().numpy())\n",
    "\n",
    "    Y_enc = np.hstack(Y_enc)\n",
    "    Y_enc = Y_enc.reshape(*Y_shape[:-1], 21)\n",
    "    \n",
    "    return Y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8c918b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(Y):\n",
    "    latent_dims = {\n",
    "        0: 1, 1: 1, 2: 1,\n",
    "        3: 1, 4: 2, 5: 1,\n",
    "        6: 1, 7: 2, 8: 1,\n",
    "        9: 1, 10: 2, 11: 1,\n",
    "        12: 1, 13: 1, 14: 2,\n",
    "        15: 1, 16: 1,}\n",
    "    \n",
    "    Y_shape = Y.shape\n",
    "    \n",
    "    current = 0\n",
    "    Y_dec = []\n",
    "    for bone in range(17):\n",
    "        latent_dim = latent_dims[bone]\n",
    "        model = AutoEncoder(latent_dim=latent_dim).to('mps')\n",
    "\n",
    "        model.load_state_dict(torch.load(f'best_models/best_model_bone{bone}.pth'))\n",
    "\n",
    "        to_decode = Y[..., current:current + latent_dim].reshape(-1, latent_dim)\n",
    "        current += latent_dim\n",
    "\n",
    "        to_decode_tensor = torch.tensor(\n",
    "            to_decode,\n",
    "            dtype=torch.float,\n",
    "            device='mps')\n",
    "        decoded_tensor = model.decoder(to_decode_tensor)\n",
    "        Y_dec.append(decoded_tensor.cpu().detach().numpy())\n",
    "        \n",
    "    Y_dec = np.hstack(Y_dec)\n",
    "    Y_dec = Y_dec.reshape(*Y_shape[:-1], 51)\n",
    "\n",
    "    return Y_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37428936",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c12aeddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.transformers import TimeDomainTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import pyriemann\n",
    "import pyriemann.regression\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "baseline_kr = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KernelRidge(\n",
    "            alpha = 0.01,\n",
    "            gamma = 0.01,\n",
    "            kernel='laplacian'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_knn = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KNeighborsRegressor(\n",
    "            n_neighbors = 5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_rf = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators = 50,\n",
    "            max_depth = 10,\n",
    "            random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_xgb = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', XGBRegressor())\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem_kr = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KernelRidge(\n",
    "            alpha = 0.01,\n",
    "            gamma = 0.01,\n",
    "            kernel='laplacian'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem_knn = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KNeighborsRegressor(\n",
    "            n_neighbors = 5))\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem_rf = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor(\n",
    "            n_estimators = 50,\n",
    "            max_depth = 10,\n",
    "            random_state=42))\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem_xgb = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', XGBRegressor())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ad2f7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from config.transformers import TimeWindowTransformer, LabelWindowExtractor\n",
    "\n",
    "PATH = f'/Users/marco/PROJECTS/data/'\n",
    "# PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "\n",
    "DATASET = 'guided'\n",
    "X_freemoves = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')         # shape (5, 8, 230000)\n",
    "Y_freemoves = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')         # shape (5, 51, 230000)\n",
    "\n",
    "# define parameters\n",
    "size = 500\n",
    "step = 250\n",
    "\n",
    "# initialize transformers\n",
    "tw_transformer = TimeWindowTransformer(size=size, step=step)\n",
    "label_extractor = LabelWindowExtractor(size=size, step=step)\n",
    "\n",
    "# apply transformations\n",
    "X_freemoves_windows = tw_transformer.transform(X_freemoves)           # shape: (5, n_windows, 8, 500)\n",
    "Y_freemoves_labels = label_extractor.transform(Y_freemoves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3504c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.validation import cross_validate_pipeline, RMSE, NMSE\n",
    "\n",
    "X_folds = np.vstack(X_freemoves_windows[:4])\n",
    "X_test = X_freemoves_windows[4]\n",
    "\n",
    "Y_folds = np.vstack(Y_freemoves_labels[:4])\n",
    "Y_test = Y_freemoves_labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1934725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.regressors import StackingRegressor, VotingRegressor\n",
    "from config.transformers import TimeWindowTransformer, LabelWindowExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3b5cfab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "bced2408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE without autoencoders: 3.354287340881273\n",
      "RMSE with autoencoders: 3.2728642920137223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = StackingRegressor(\n",
    "    estimators = [\n",
    "        baseline_kr,\n",
    "        baseline_knn,\n",
    "        baseline_rf,\n",
    "        # baseline_xgb,\n",
    "        # riem_kr,\n",
    "        riem_knn,\n",
    "        # riem_rf,\n",
    "        # riem_xgb\n",
    "    ],\n",
    "    end_estimator = KNeighborsRegressor(\n",
    "        n_neighbors=7,\n",
    "        p=1)\n",
    "    # end_estimator = RadiusNeighborsRegressor(radius=12)\n",
    ")\n",
    "\n",
    "# pipeline = VotingRegressor(\n",
    "#     estimators = [\n",
    "#         baseline_kr,\n",
    "#         baseline_knn,\n",
    "#         baseline_rf,\n",
    "#         riem_knn,\n",
    "#         riem_rf\n",
    "#     ]\n",
    "#     # end_estimator = RandomForestRegressor(\n",
    "#     #         n_estimators = 50,\n",
    "#     #         max_depth = 10)\n",
    "# )\n",
    "\n",
    "# feature scaler\n",
    "targ_scal = StandardScaler()\n",
    "\n",
    "# actual regression\n",
    "Y_folds = np.vstack(Y_freemoves_labels[:4])\n",
    "\n",
    "Y_folds = targ_scal.fit_transform(Y_folds) # scaling targets\n",
    "\n",
    "pipeline.fit(X_folds, Y_folds)\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "Y_pred = targ_scal.inverse_transform(Y_pred) # scaling targets back\n",
    "\n",
    "print('RMSE without autoencoders:', RMSE(Y_pred, Y_test))\n",
    "\n",
    "# encoded regression\n",
    "Y_folds = np.vstack(Y_freemoves_labels[:4])\n",
    "\n",
    "Y_folds_enc = encode(Y_folds) # shape (..., 21) rather than (..., 51)\n",
    "Y_folds_enc = targ_scal.fit_transform(Y_folds_enc) # scaling targets\n",
    "\n",
    "pipeline.fit(X_folds, Y_folds_enc)\n",
    "Y_enc_pred = pipeline.predict(X_test) # shape (..., 21) rather than (..., 51)\n",
    "\n",
    "Y_enc_pred = targ_scal.inverse_transform(Y_enc_pred) # scaling targets back\n",
    "Y_dec_pred = decode(Y_enc_pred) # back to shape (..., 51)\n",
    "\n",
    "\n",
    "print('RMSE with autoencoders:', RMSE(Y_dec_pred, Y_test))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad576cbf",
   "metadata": {},
   "source": [
    "## Prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1453d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "99974912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding...\n",
      "Encoded.\n",
      "Training...\n",
      "Trained.\n",
      "Predicting...\n",
      "Predicted.\n",
      "Decoding...\n",
      "Decoded.\n"
     ]
    }
   ],
   "source": [
    "model = StackingRegressor(\n",
    "    estimators = [\n",
    "        baseline_kr,\n",
    "        baseline_knn,\n",
    "        baseline_rf,\n",
    "        # riem_kr,\n",
    "        riem_knn,\n",
    "        # riem_rf\n",
    "    ],\n",
    "    end_estimator = KNeighborsRegressor(\n",
    "        n_neighbors=7,\n",
    "        p=1\n",
    "    )\n",
    ")\n",
    "\n",
    "confirmation = input('Have you inserted the correct model name and the expected RMSE?')\n",
    "\n",
    "model_name = 'prova_autoencoders'\n",
    "expected_rmse = '1000'\n",
    "\n",
    "step_prediction = 50 # step for prediction\n",
    "\n",
    "# preparing the training data\n",
    "tw_extractor = TimeWindowTransformer(size = 500, step = step_prediction)\n",
    "label_extractor = LabelWindowExtractor(size = 500, step = step_prediction)\n",
    "\n",
    "DATASET = 'guided'\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "DATASET = 'freemoves'\n",
    "XX = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "YY = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "XX = XX[..., -230000:]\n",
    "YY = YY[..., -230000:]\n",
    "\n",
    "X = np.vstack([X, XX])\n",
    "Y = np.vstack([Y, YY])\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)\n",
    "\n",
    "# encoding\n",
    "print('Encoding...')\n",
    "Y_labels = encode(Y_labels)\n",
    "print('Encoded.')\n",
    "\n",
    "# stacking the sessions \n",
    "X_train = X_windows.reshape(-1, *X_windows.shape[2:])\n",
    "Y_train = Y_labels.reshape(-1, *Y_labels.shape[2:])\n",
    "\n",
    "# scaling the target\n",
    "targ_scal = StandardScaler()\n",
    "Y_train = targ_scal.fit_transform(Y_train)\n",
    "\n",
    "# training\n",
    "print('Training...')\n",
    "model.fit(X_train, Y_train)\n",
    "print('Trained.')\n",
    "\n",
    "# predicting\n",
    "DATASET = 'guided'\n",
    "X_test = np.load(PATH + f'{DATASET}/{DATASET}_testset_X.npy')\n",
    "X_test = X_test.reshape(-1, *X_windows.shape[2:])\n",
    "print('Predicting...')\n",
    "Y_pred = model.predict(X_test)\n",
    "print('Predicted.')\n",
    "\n",
    "# scaling back\n",
    "Y_pred = targ_scal.inverse_transform(Y_pred)\n",
    "\n",
    "# decoding\n",
    "print('Decoding...')\n",
    "Y_pred = decode(Y_pred)\n",
    "print('Decoded.')\n",
    "\n",
    "# saving\n",
    "file_name = 'guided_encoding_scaling_meta_50.npy'\n",
    "\n",
    "np.save(file_name, Y_pred)\n",
    "\n",
    "Y_pred_guided = np.load('guided_encoding_scaling_meta_50.npy')\n",
    "Y_pred_freemoves = np.load('dann_steps_50_rmse_10.npy')\n",
    "\n",
    "Y_pred = np.vstack([Y_pred_guided, Y_pred_freemoves])\n",
    "Y_pred_df = pd.DataFrame(Y_pred)\n",
    "Y_pred_df.to_csv('very_last_try.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

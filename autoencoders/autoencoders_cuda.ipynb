{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9720ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204f8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from config.regressors import *\n",
    "from config.models import *\n",
    "from config.loss_functions import *\n",
    "from config.transformers import *\n",
    "from config.validation import *\n",
    "from config.dann import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5fbd872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder: [3 → 64 → 32 → 16 → latent_dim]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 16),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(4, latent_dim)  # No activation\n",
    "        )\n",
    "        \n",
    "        # Decoder: [latent_dim → 16 → 32 → 64 → 3]\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(4, 8),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(16, 3)  # No activation\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                # LeCun normal initialization for SELU\n",
    "                nn.init.normal_(m.weight, 0, std=1. / np.sqrt(m.in_features))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adeec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(bone, latent_dim):\n",
    "    # ----------------------------\n",
    "    # 1. Configuration\n",
    "    # ----------------------------\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size     = 4096\n",
    "    learning_rate  = 1e-3\n",
    "    weight_decay   = 1e-4\n",
    "    max_epochs     = 100\n",
    "    patience       = 20\n",
    "    num_workers    = 4  # >0 for parallel loading\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Prepare Data (CPU‑only)\n",
    "    # ----------------------------\n",
    "    # Assume Y is your NumPy array of shape [N, 3*B]\n",
    "    PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "    Y = np.load(PATH + 'Y_all.npy')\n",
    "    Y_bone = Y[:, bone*3:(bone+1)*3]\n",
    "    Y_bone = Y[:, bone*3:(bone+1)*3]\n",
    "\n",
    "    Yb_cpu   = torch.as_tensor(Y_bone, dtype=torch.float32)  # stays on CPU\n",
    "\n",
    "    dataset      = TensorDataset(Yb_cpu, Yb_cpu)\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    val_loader   = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "\n",
    "    # instantiate with your chosen latent dimension\n",
    "    model = AutoEncoder(latent_dim).to(device)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Loss & Optimizer\n",
    "    # ----------------------------\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Training Loop w/ Early Stopping\n",
    "    # ----------------------------\n",
    "    best_val_loss     = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        # —— Training —— \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for xb_cpu, yb_cpu in train_loader:\n",
    "            # move batch to MPS (or CUDA) on-the-fly\n",
    "            xb = xb_cpu.to(device, non_blocking=True)\n",
    "            yb = yb_cpu.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)              \n",
    "            loss  = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # —— Validation ——\n",
    "        model.eval()\n",
    "        running_val = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb_cpu, yb_cpu in val_loader:\n",
    "                xb = xb_cpu.to(device, non_blocking=True)\n",
    "                yb = yb_cpu.to(device, non_blocking=True)\n",
    "                running_val += criterion(model(xb), yb).item()\n",
    "        avg_val_loss = running_val / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}: Train MSE = {avg_train_loss:.6f} | Val MSE = {avg_val_loss:.6f}\")\n",
    "\n",
    "        # —— Early Stopping ——\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss     = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), f\"best_models_cuda/best_model_bone{bone}.pth\")\n",
    "            print(\"  → New best model saved\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d2e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 01: Train MSE = 201.669550 | Val MSE = 45.854745\n",
      "  → New best model saved\n",
      "Epoch 02: Train MSE = 44.262359 | Val MSE = 42.299581\n",
      "  → New best model saved\n",
      "Epoch 03: Train MSE = 39.035336 | Val MSE = 34.545258\n",
      "  → New best model saved\n",
      "Epoch 04: Train MSE = 23.843714 | Val MSE = 9.010370\n",
      "  → New best model saved\n",
      "Epoch 05: Train MSE = 2.457708 | Val MSE = 0.443657\n",
      "  → New best model saved\n",
      "Epoch 06: Train MSE = 0.300545 | Val MSE = 0.237245\n",
      "  → New best model saved\n",
      "Epoch 07: Train MSE = 0.228174 | Val MSE = 0.223350\n",
      "  → New best model saved\n",
      "Epoch 08: Train MSE = 0.224349 | Val MSE = 0.224297\n",
      "Epoch 09: Train MSE = 0.224494 | Val MSE = 0.222168\n",
      "  → New best model saved\n",
      "Epoch 10: Train MSE = 0.224661 | Val MSE = 0.223163\n",
      "Epoch 11: Train MSE = 0.224339 | Val MSE = 0.236498\n",
      "Epoch 12: Train MSE = 0.224620 | Val MSE = 0.224000\n",
      "Epoch 13: Train MSE = 0.224116 | Val MSE = 0.219629\n",
      "  → New best model saved\n",
      "Epoch 14: Train MSE = 0.212893 | Val MSE = 0.200944\n",
      "  → New best model saved\n",
      "Epoch 15: Train MSE = 0.194342 | Val MSE = 0.183289\n",
      "  → New best model saved\n",
      "Epoch 16: Train MSE = 0.163855 | Val MSE = 0.147087\n",
      "  → New best model saved\n",
      "Epoch 17: Train MSE = 0.116184 | Val MSE = 0.090614\n",
      "  → New best model saved\n",
      "Epoch 18: Train MSE = 0.078964 | Val MSE = 0.068528\n",
      "  → New best model saved\n",
      "Epoch 19: Train MSE = 0.065369 | Val MSE = 0.060677\n",
      "  → New best model saved\n",
      "Epoch 20: Train MSE = 0.062640 | Val MSE = 0.059214\n",
      "  → New best model saved\n",
      "Epoch 21: Train MSE = 0.061389 | Val MSE = 0.065364\n",
      "Epoch 22: Train MSE = 0.061240 | Val MSE = 0.058429\n",
      "  → New best model saved\n",
      "Epoch 23: Train MSE = 0.060897 | Val MSE = 0.060117\n",
      "Epoch 24: Train MSE = 0.060660 | Val MSE = 0.059083\n",
      "Epoch 25: Train MSE = 0.060303 | Val MSE = 0.060914\n",
      "Epoch 26: Train MSE = 0.060232 | Val MSE = 0.066817\n",
      "Epoch 27: Train MSE = 0.059514 | Val MSE = 0.059986\n",
      "Epoch 28: Train MSE = 0.059146 | Val MSE = 0.057989\n",
      "  → New best model saved\n",
      "Epoch 29: Train MSE = 0.057857 | Val MSE = 0.055434\n",
      "  → New best model saved\n",
      "Epoch 30: Train MSE = 0.057112 | Val MSE = 0.071308\n",
      "Epoch 31: Train MSE = 0.055467 | Val MSE = 0.053243\n",
      "  → New best model saved\n",
      "Epoch 32: Train MSE = 0.054168 | Val MSE = 0.052865\n",
      "  → New best model saved\n",
      "Epoch 33: Train MSE = 0.051999 | Val MSE = 0.048661\n",
      "  → New best model saved\n",
      "Epoch 34: Train MSE = 0.048397 | Val MSE = 0.044568\n",
      "  → New best model saved\n",
      "Epoch 35: Train MSE = 0.044207 | Val MSE = 0.042021\n",
      "  → New best model saved\n",
      "Epoch 36: Train MSE = 0.040095 | Val MSE = 0.037117\n",
      "  → New best model saved\n",
      "Epoch 37: Train MSE = 0.037503 | Val MSE = 0.034360\n",
      "  → New best model saved\n",
      "Epoch 38: Train MSE = 0.036223 | Val MSE = 0.039059\n",
      "Epoch 39: Train MSE = 0.034462 | Val MSE = 0.033382\n",
      "  → New best model saved\n",
      "Epoch 40: Train MSE = 0.034612 | Val MSE = 0.035349\n",
      "Epoch 41: Train MSE = 0.033226 | Val MSE = 0.034522\n",
      "Epoch 42: Train MSE = 0.033446 | Val MSE = 0.030752\n",
      "  → New best model saved\n",
      "Epoch 43: Train MSE = 0.032627 | Val MSE = 0.030653\n",
      "  → New best model saved\n",
      "Epoch 44: Train MSE = 0.032634 | Val MSE = 0.030950\n",
      "Epoch 45: Train MSE = 0.032328 | Val MSE = 0.030188\n",
      "  → New best model saved\n",
      "Epoch 46: Train MSE = 0.032123 | Val MSE = 0.037033\n",
      "Epoch 47: Train MSE = 0.031946 | Val MSE = 0.038145\n",
      "Epoch 48: Train MSE = 0.031787 | Val MSE = 0.038168\n",
      "Epoch 49: Train MSE = 0.031668 | Val MSE = 0.037815\n",
      "Epoch 50: Train MSE = 0.031511 | Val MSE = 0.034881\n",
      "Epoch 51: Train MSE = 0.031735 | Val MSE = 0.036415\n",
      "Epoch 52: Train MSE = 0.031401 | Val MSE = 0.030305\n",
      "Epoch 53: Train MSE = 0.030967 | Val MSE = 0.033852\n",
      "Epoch 54: Train MSE = 0.030960 | Val MSE = 0.030168\n",
      "  → New best model saved\n",
      "Epoch 55: Train MSE = 0.030928 | Val MSE = 0.028572\n",
      "  → New best model saved\n",
      "Epoch 56: Train MSE = 0.030699 | Val MSE = 0.032798\n",
      "Epoch 57: Train MSE = 0.030678 | Val MSE = 0.031390\n",
      "Epoch 58: Train MSE = 0.030345 | Val MSE = 0.035646\n",
      "Epoch 59: Train MSE = 0.030542 | Val MSE = 0.028144\n",
      "  → New best model saved\n",
      "Epoch 60: Train MSE = 0.030246 | Val MSE = 0.030447\n",
      "Epoch 61: Train MSE = 0.030056 | Val MSE = 0.030711\n",
      "Epoch 62: Train MSE = 0.030156 | Val MSE = 0.029684\n",
      "Epoch 63: Train MSE = 0.029969 | Val MSE = 0.028610\n",
      "Epoch 64: Train MSE = 0.030049 | Val MSE = 0.029033\n",
      "Epoch 65: Train MSE = 0.029535 | Val MSE = 0.028203\n",
      "Epoch 66: Train MSE = 0.029939 | Val MSE = 0.034552\n",
      "Epoch 67: Train MSE = 0.029647 | Val MSE = 0.027541\n",
      "  → New best model saved\n",
      "Epoch 68: Train MSE = 0.029626 | Val MSE = 0.028956\n",
      "Epoch 69: Train MSE = 0.029558 | Val MSE = 0.027839\n",
      "Epoch 70: Train MSE = 0.029418 | Val MSE = 0.029200\n",
      "Epoch 71: Train MSE = 0.029271 | Val MSE = 0.030868\n",
      "Epoch 72: Train MSE = 0.029213 | Val MSE = 0.028866\n",
      "Epoch 73: Train MSE = 0.029727 | Val MSE = 0.035674\n",
      "Epoch 74: Train MSE = 0.029221 | Val MSE = 0.029613\n",
      "Epoch 75: Train MSE = 0.029000 | Val MSE = 0.027127\n",
      "  → New best model saved\n",
      "Epoch 76: Train MSE = 0.029171 | Val MSE = 0.027949\n",
      "Epoch 77: Train MSE = 0.028908 | Val MSE = 0.028679\n",
      "Epoch 78: Train MSE = 0.028986 | Val MSE = 0.026772\n",
      "  → New best model saved\n",
      "Epoch 79: Train MSE = 0.028806 | Val MSE = 0.026695\n",
      "  → New best model saved\n",
      "Epoch 80: Train MSE = 0.028503 | Val MSE = 0.032166\n",
      "Epoch 81: Train MSE = 0.028711 | Val MSE = 0.027431\n",
      "Epoch 82: Train MSE = 0.028590 | Val MSE = 0.026925\n",
      "Epoch 83: Train MSE = 0.028385 | Val MSE = 0.026371\n",
      "  → New best model saved\n",
      "Epoch 84: Train MSE = 0.028553 | Val MSE = 0.028606\n",
      "Epoch 85: Train MSE = 0.028393 | Val MSE = 0.029924\n",
      "Epoch 86: Train MSE = 0.028259 | Val MSE = 0.027763\n",
      "Epoch 87: Train MSE = 0.028159 | Val MSE = 0.026181\n",
      "  → New best model saved\n",
      "Epoch 88: Train MSE = 0.028193 | Val MSE = 0.026144\n",
      "  → New best model saved\n",
      "Epoch 89: Train MSE = 0.028279 | Val MSE = 0.029240\n",
      "Epoch 90: Train MSE = 0.027929 | Val MSE = 0.026356\n",
      "Epoch 91: Train MSE = 0.028105 | Val MSE = 0.029281\n",
      "Epoch 92: Train MSE = 0.028046 | Val MSE = 0.027854\n",
      "Epoch 93: Train MSE = 0.028010 | Val MSE = 0.027171\n",
      "Epoch 94: Train MSE = 0.027743 | Val MSE = 0.026450\n",
      "Epoch 95: Train MSE = 0.027975 | Val MSE = 0.028686\n",
      "Epoch 96: Train MSE = 0.027741 | Val MSE = 0.029255\n",
      "Epoch 97: Train MSE = 0.027659 | Val MSE = 0.026380\n",
      "Epoch 98: Train MSE = 0.027880 | Val MSE = 0.029635\n",
      "Epoch 99: Train MSE = 0.027683 | Val MSE = 0.028705\n",
      "Epoch 100: Train MSE = 0.027485 | Val MSE = 0.029172\n",
      "Using device: cuda\n",
      "Epoch 01: Train MSE = 63.066778 | Val MSE = 35.205069\n",
      "  → New best model saved\n",
      "Epoch 02: Train MSE = 31.377638 | Val MSE = 21.718739\n",
      "  → New best model saved\n",
      "Epoch 03: Train MSE = 8.972460 | Val MSE = 1.872172\n",
      "  → New best model saved\n",
      "Epoch 04: Train MSE = 0.507821 | Val MSE = 0.111460\n",
      "  → New best model saved\n",
      "Epoch 05: Train MSE = 0.105100 | Val MSE = 0.102133\n",
      "  → New best model saved\n",
      "Epoch 06: Train MSE = 0.100746 | Val MSE = 0.097540\n",
      "  → New best model saved\n",
      "Epoch 07: Train MSE = 0.086268 | Val MSE = 0.071054\n",
      "  → New best model saved\n",
      "Epoch 08: Train MSE = 0.060507 | Val MSE = 0.052545\n",
      "  → New best model saved\n",
      "Epoch 09: Train MSE = 0.049303 | Val MSE = 0.045039\n",
      "  → New best model saved\n",
      "Epoch 10: Train MSE = 0.043216 | Val MSE = 0.039755\n",
      "  → New best model saved\n",
      "Epoch 11: Train MSE = 0.038730 | Val MSE = 0.037772\n",
      "  → New best model saved\n",
      "Epoch 12: Train MSE = 0.035205 | Val MSE = 0.032197\n",
      "  → New best model saved\n",
      "Epoch 13: Train MSE = 0.032011 | Val MSE = 0.029446\n",
      "  → New best model saved\n",
      "Epoch 14: Train MSE = 0.029374 | Val MSE = 0.026632\n",
      "  → New best model saved\n",
      "Epoch 15: Train MSE = 0.026018 | Val MSE = 0.022660\n",
      "  → New best model saved\n",
      "Epoch 16: Train MSE = 0.022001 | Val MSE = 0.023316\n",
      "Epoch 17: Train MSE = 0.019070 | Val MSE = 0.016565\n",
      "  → New best model saved\n",
      "Epoch 18: Train MSE = 0.016871 | Val MSE = 0.014177\n",
      "  → New best model saved\n",
      "Epoch 19: Train MSE = 0.015026 | Val MSE = 0.012629\n",
      "  → New best model saved\n",
      "Epoch 20: Train MSE = 0.013892 | Val MSE = 0.014757\n",
      "Epoch 21: Train MSE = 0.013256 | Val MSE = 0.011173\n",
      "  → New best model saved\n",
      "Epoch 22: Train MSE = 0.012777 | Val MSE = 0.010618\n",
      "  → New best model saved\n",
      "Epoch 23: Train MSE = 0.012220 | Val MSE = 0.010785\n",
      "Epoch 24: Train MSE = 0.011893 | Val MSE = 0.017277\n",
      "Epoch 25: Train MSE = 0.011811 | Val MSE = 0.010118\n",
      "  → New best model saved\n",
      "Epoch 26: Train MSE = 0.011745 | Val MSE = 0.013401\n",
      "Epoch 27: Train MSE = 0.011312 | Val MSE = 0.009785\n",
      "  → New best model saved\n",
      "Epoch 28: Train MSE = 0.011132 | Val MSE = 0.015402\n",
      "Epoch 29: Train MSE = 0.011061 | Val MSE = 0.010893\n",
      "Epoch 30: Train MSE = 0.010924 | Val MSE = 0.010123\n",
      "Epoch 31: Train MSE = 0.010241 | Val MSE = 0.008115\n",
      "  → New best model saved\n",
      "Epoch 32: Train MSE = 0.009944 | Val MSE = 0.008238\n",
      "Epoch 33: Train MSE = 0.009704 | Val MSE = 0.008885\n",
      "Epoch 34: Train MSE = 0.009461 | Val MSE = 0.007298\n",
      "  → New best model saved\n",
      "Epoch 35: Train MSE = 0.009169 | Val MSE = 0.007226\n",
      "  → New best model saved\n",
      "Epoch 36: Train MSE = 0.009137 | Val MSE = 0.012443\n",
      "Epoch 37: Train MSE = 0.009203 | Val MSE = 0.008538\n",
      "Epoch 38: Train MSE = 0.008795 | Val MSE = 0.007393\n",
      "Epoch 39: Train MSE = 0.008799 | Val MSE = 0.008412\n",
      "Epoch 40: Train MSE = 0.008916 | Val MSE = 0.013088\n",
      "Epoch 41: Train MSE = 0.008488 | Val MSE = 0.006722\n",
      "  → New best model saved\n",
      "Epoch 42: Train MSE = 0.008539 | Val MSE = 0.006672\n",
      "  → New best model saved\n",
      "Epoch 43: Train MSE = 0.008623 | Val MSE = 0.009331\n",
      "Epoch 44: Train MSE = 0.008344 | Val MSE = 0.011883\n",
      "Epoch 45: Train MSE = 0.008503 | Val MSE = 0.006669\n",
      "  → New best model saved\n",
      "Epoch 46: Train MSE = 0.008243 | Val MSE = 0.008436\n",
      "Epoch 47: Train MSE = 0.008343 | Val MSE = 0.006166\n",
      "  → New best model saved\n",
      "Epoch 48: Train MSE = 0.008147 | Val MSE = 0.006261\n",
      "Epoch 49: Train MSE = 0.008063 | Val MSE = 0.006029\n",
      "  → New best model saved\n",
      "Epoch 50: Train MSE = 0.008274 | Val MSE = 0.006010\n",
      "  → New best model saved\n",
      "Epoch 51: Train MSE = 0.008242 | Val MSE = 0.005955\n",
      "  → New best model saved\n",
      "Epoch 52: Train MSE = 0.007830 | Val MSE = 0.007368\n",
      "Epoch 53: Train MSE = 0.008087 | Val MSE = 0.005903\n",
      "  → New best model saved\n",
      "Epoch 54: Train MSE = 0.007858 | Val MSE = 0.008754\n",
      "Epoch 55: Train MSE = 0.007941 | Val MSE = 0.006419\n",
      "Epoch 56: Train MSE = 0.008108 | Val MSE = 0.008694\n",
      "Epoch 57: Train MSE = 0.007480 | Val MSE = 0.006757\n",
      "Epoch 58: Train MSE = 0.007888 | Val MSE = 0.006316\n",
      "Epoch 59: Train MSE = 0.007893 | Val MSE = 0.006561\n",
      "Epoch 60: Train MSE = 0.007510 | Val MSE = 0.005658\n",
      "  → New best model saved\n",
      "Epoch 61: Train MSE = 0.007672 | Val MSE = 0.018380\n",
      "Epoch 62: Train MSE = 0.007880 | Val MSE = 0.005579\n",
      "  → New best model saved\n",
      "Epoch 63: Train MSE = 0.007596 | Val MSE = 0.005531\n",
      "  → New best model saved\n",
      "Epoch 64: Train MSE = 0.007500 | Val MSE = 0.008254\n",
      "Epoch 65: Train MSE = 0.007507 | Val MSE = 0.005640\n",
      "Epoch 66: Train MSE = 0.007525 | Val MSE = 0.007998\n",
      "Epoch 67: Train MSE = 0.007545 | Val MSE = 0.005759\n",
      "Epoch 68: Train MSE = 0.007598 | Val MSE = 0.005513\n",
      "  → New best model saved\n",
      "Epoch 69: Train MSE = 0.007389 | Val MSE = 0.010584\n",
      "Epoch 70: Train MSE = 0.007366 | Val MSE = 0.005751\n",
      "Epoch 71: Train MSE = 0.007363 | Val MSE = 0.028150\n",
      "Epoch 72: Train MSE = 0.007466 | Val MSE = 0.006121\n",
      "Epoch 73: Train MSE = 0.007352 | Val MSE = 0.008235\n",
      "Epoch 74: Train MSE = 0.007685 | Val MSE = 0.005339\n",
      "  → New best model saved\n",
      "Epoch 75: Train MSE = 0.007200 | Val MSE = 0.005574\n",
      "Epoch 76: Train MSE = 0.007225 | Val MSE = 0.005286\n",
      "  → New best model saved\n",
      "Epoch 77: Train MSE = 0.007213 | Val MSE = 0.005433\n",
      "Epoch 78: Train MSE = 0.007557 | Val MSE = 0.006156\n",
      "Epoch 79: Train MSE = 0.007022 | Val MSE = 0.033501\n",
      "Epoch 80: Train MSE = 0.007158 | Val MSE = 0.005637\n",
      "Epoch 81: Train MSE = 0.007199 | Val MSE = 0.006151\n",
      "Epoch 82: Train MSE = 0.007244 | Val MSE = 0.006666\n",
      "Epoch 83: Train MSE = 0.007247 | Val MSE = 0.005203\n",
      "  → New best model saved\n",
      "Epoch 84: Train MSE = 0.007145 | Val MSE = 0.005730\n",
      "Epoch 85: Train MSE = 0.006988 | Val MSE = 0.005244\n",
      "Epoch 86: Train MSE = 0.007040 | Val MSE = 0.005228\n",
      "Epoch 87: Train MSE = 0.007359 | Val MSE = 0.005309\n",
      "Epoch 88: Train MSE = 0.006852 | Val MSE = 0.006070\n",
      "Epoch 89: Train MSE = 0.007127 | Val MSE = 0.005964\n",
      "Epoch 90: Train MSE = 0.006809 | Val MSE = 0.007441\n",
      "Epoch 91: Train MSE = 0.007197 | Val MSE = 0.005101\n",
      "  → New best model saved\n",
      "Epoch 92: Train MSE = 0.006921 | Val MSE = 0.008153\n",
      "Epoch 93: Train MSE = 0.007047 | Val MSE = 0.004997\n",
      "  → New best model saved\n",
      "Epoch 94: Train MSE = 0.006809 | Val MSE = 0.006915\n",
      "Epoch 95: Train MSE = 0.007043 | Val MSE = 0.010654\n",
      "Epoch 96: Train MSE = 0.006899 | Val MSE = 0.005066\n",
      "Epoch 97: Train MSE = 0.006792 | Val MSE = 0.005671\n",
      "Epoch 98: Train MSE = 0.006967 | Val MSE = 0.005111\n",
      "Epoch 99: Train MSE = 0.006823 | Val MSE = 0.005035\n",
      "Epoch 100: Train MSE = 0.006871 | Val MSE = 0.005190\n",
      "Using device: cuda\n",
      "Epoch 01: Train MSE = 20.712099 | Val MSE = 1.417394\n",
      "  → New best model saved\n",
      "Epoch 02: Train MSE = 0.396828 | Val MSE = 0.194423\n",
      "  → New best model saved\n",
      "Epoch 03: Train MSE = 0.129471 | Val MSE = 0.086064\n",
      "  → New best model saved\n",
      "Epoch 04: Train MSE = 0.064351 | Val MSE = 0.046584\n",
      "  → New best model saved\n",
      "Epoch 05: Train MSE = 0.034180 | Val MSE = 0.023478\n",
      "  → New best model saved\n",
      "Epoch 06: Train MSE = 0.016731 | Val MSE = 0.011506\n",
      "  → New best model saved\n",
      "Epoch 07: Train MSE = 0.008525 | Val MSE = 0.006109\n",
      "  → New best model saved\n",
      "Epoch 08: Train MSE = 0.004936 | Val MSE = 0.003849\n",
      "  → New best model saved\n",
      "Epoch 09: Train MSE = 0.003714 | Val MSE = 0.004156\n",
      "Epoch 10: Train MSE = 0.003494 | Val MSE = 0.003358\n",
      "  → New best model saved\n",
      "Epoch 11: Train MSE = 0.003350 | Val MSE = 0.002899\n",
      "  → New best model saved\n",
      "Epoch 12: Train MSE = 0.003318 | Val MSE = 0.005323\n",
      "Epoch 13: Train MSE = 0.003216 | Val MSE = 0.002839\n",
      "  → New best model saved\n",
      "Epoch 14: Train MSE = 0.003133 | Val MSE = 0.002713\n",
      "  → New best model saved\n",
      "Epoch 15: Train MSE = 0.003073 | Val MSE = 0.002364\n",
      "  → New best model saved\n",
      "Epoch 16: Train MSE = 0.002940 | Val MSE = 0.002673\n",
      "Epoch 17: Train MSE = 0.002832 | Val MSE = 0.003490\n",
      "Epoch 18: Train MSE = 0.002534 | Val MSE = 0.002712\n",
      "Epoch 19: Train MSE = 0.002431 | Val MSE = 0.001756\n",
      "  → New best model saved\n",
      "Epoch 20: Train MSE = 0.001993 | Val MSE = 0.001442\n",
      "  → New best model saved\n",
      "Epoch 21: Train MSE = 0.001803 | Val MSE = 0.001662\n",
      "Epoch 22: Train MSE = 0.001632 | Val MSE = 0.001459\n",
      "Epoch 23: Train MSE = 0.001463 | Val MSE = 0.001277\n",
      "  → New best model saved\n"
     ]
    }
   ],
   "source": [
    "# PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "\n",
    "# DATASET = 'guided'\n",
    "# Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "# Y = Y.transpose(0, 2, 1).reshape(-1, Y.shape[1])\n",
    "\n",
    "# DATASET = 'freemoves'\n",
    "# YY = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "# YY = YY.transpose(0, 2, 1).reshape(-1, YY.shape[1])\n",
    "\n",
    "# Y_all = np.vstack([Y, YY])\n",
    "\n",
    "# np.save(PATH + 'Y_all.npy', Y_all)\n",
    "\n",
    "latent_dims = {\n",
    "        0: 1, 1: 1, 2: 1,\n",
    "        3: 1, 4: 2, 5: 1,\n",
    "        6: 1, 7: 2, 8: 1,\n",
    "        9: 1, 10: 2, 11: 1,\n",
    "        12: 1, 13: 1, 14: 2,\n",
    "        15: 1, 16: 1,}\n",
    "\n",
    "for bone in range(17):\n",
    "    trainer(bone, latent_dims[bone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331038a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(Y):\n",
    "    latent_dims = {\n",
    "        0: 1, 1: 1, 2: 1,\n",
    "        3: 1, 4: 2, 5: 1,\n",
    "        6: 1, 7: 2, 8: 1,\n",
    "        9: 1, 10: 2, 11: 1,\n",
    "        12: 1, 13: 1, 14: 2,\n",
    "        15: 1, 16: 1,}\n",
    "    \n",
    "    Y_shape = Y.shape\n",
    "    \n",
    "    Y_enc = []\n",
    "    for bone in range(17):\n",
    "        latent_dim = latent_dims[bone]\n",
    "        model = AutoEncoder(latent_dim=latent_dim).to('cuda')\n",
    "        model.load_state_dict(torch.load(f'best_models/best_model_bone{bone}.pth'))\n",
    "        \n",
    "        to_encode = Y[..., 3*bone:3*(bone+1)].reshape(-1,3)\n",
    "        to_encode_tensor = torch.tensor(\n",
    "            to_encode,\n",
    "            dtype=torch.float,\n",
    "            device='cuda')\n",
    "        encoded_tensor = model.encoder(to_encode_tensor)\n",
    "        Y_enc.append(encoded_tensor.cpu().detach().numpy())\n",
    "\n",
    "    Y_enc = np.hstack(Y_enc)\n",
    "    Y_enc = Y_enc.reshape(*Y_shape[:-1], 21)\n",
    "    \n",
    "    return Y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c918b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(Y):\n",
    "    latent_dims = {\n",
    "        0: 1, 1: 1, 2: 1,\n",
    "        3: 1, 4: 2, 5: 1,\n",
    "        6: 1, 7: 2, 8: 1,\n",
    "        9: 1, 10: 2, 11: 1,\n",
    "        12: 1, 13: 1, 14: 2,\n",
    "        15: 1, 16: 1,}\n",
    "    \n",
    "    Y_shape = Y.shape\n",
    "    \n",
    "    current = 0\n",
    "    Y_dec = []\n",
    "    for bone in range(17):\n",
    "        latent_dim = latent_dims[bone]\n",
    "        model = AutoEncoder(latent_dim=latent_dim).to('cuda')\n",
    "\n",
    "        model.load_state_dict(torch.load(f'best_models/best_model_bone{bone}.pth'))\n",
    "\n",
    "        to_decode = Y[..., current:current + latent_dim].reshape(-1, latent_dim)\n",
    "        current += latent_dim\n",
    "\n",
    "        to_decode_tensor = torch.tensor(\n",
    "            to_decode,\n",
    "            dtype=torch.float,\n",
    "            device='cuda')\n",
    "        decoded_tensor = model.decoder(to_decode_tensor)\n",
    "        Y_dec.append(decoded_tensor.cpu().detach().numpy())\n",
    "        \n",
    "    Y_dec = np.hstack(Y_dec)\n",
    "    Y_dec = Y_dec.reshape(*Y_shape[:-1], 51)\n",
    "\n",
    "    return Y_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad576cbf",
   "metadata": {},
   "source": [
    "## Prediction generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99974912",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackingRegressor(\n",
    "    estimators = [\n",
    "        baseline_kr,\n",
    "        baseline_knn,\n",
    "        baseline_rf,\n",
    "        riem_rf\n",
    "    ],\n",
    "    end_estimator = KNeighborsRegressor(n_neighbors=100)\n",
    ")\n",
    "\n",
    "confirmation = input('Have you inserted the correct model name and the expected RMSE?')\n",
    "\n",
    "model_name = 'prova_autoencoders'\n",
    "expected_rmse = '1000'\n",
    "\n",
    "step_prediction = 50 # step for prediction\n",
    "\n",
    "# preparing the training data\n",
    "tw_extractor = TimeWindowTransformer(size = 500, step = step_prediction)\n",
    "label_extractor = LabelWindowExtractor(size = 500, step = step_prediction)\n",
    "\n",
    "DATASET = 'guided'\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "DATASET = 'freemoves'\n",
    "XX = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "YY = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "XX = XX[..., -230000:]\n",
    "YY = YY[..., -230000:]\n",
    "\n",
    "X = np.vstack([X, XX])\n",
    "Y = np.vstack([Y, YY])\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)\n",
    "\n",
    "# encoding\n",
    "Y_labels_enc = encode(Y_labels)\n",
    "\n",
    "# stacking the sessions \n",
    "X_train = X_windows.reshape(-1, *X_windows.shape[2:])\n",
    "Y_train = Y_labels_enc.reshape(-1, *Y_labels_enc.shape[2:])\n",
    "\n",
    "# training\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# predicting\n",
    "DATASET = 'guided'\n",
    "X_test = np.load(PATH + f'{DATASET}/{DATASET}_testset_X.npy')\n",
    "X_test = X_test.reshape(-1, *X_windows.shape[2:])\n",
    "Y_pred_enc = model.predict(X_test)\n",
    "\n",
    "# decoding\n",
    "Y_pred_dec = decode(Y_pred_enc)\n",
    "\n",
    "# saving\n",
    "file_name = 'prova_encoded_decoded.npy'\n",
    "\n",
    "np.save(file_name, Y_pred_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e749f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.UntypedStorage(): Storage device not recognized: mps",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 52\u001b[0m\n\u001b[0;32m     41\u001b[0m regressor \u001b[38;5;241m=\u001b[39m DANNRegressor(\n\u001b[0;32m     42\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     43\u001b[0m     session_ids\u001b[38;5;241m=\u001b[39mtrain_session_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# encoding\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m Y_labels_enc \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# stacking the sessions \u001b[39;00m\n\u001b[0;32m     55\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_windows\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mX_windows\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(Y)\u001b[0m\n\u001b[0;32m     14\u001b[0m latent_dim \u001b[38;5;241m=\u001b[39m latent_dims[bone]\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoEncoder(latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_models/best_model_bone\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbone\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m to_encode \u001b[38;5;241m=\u001b[39m Y[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mbone:\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m(bone\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     19\u001b[0m to_encode_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[0;32m     20\u001b[0m     to_encode,\n\u001b[0;32m     21\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat,\n\u001b[0;32m     22\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\serialization.py:1516\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1516\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1520\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1523\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\serialization.py:2114\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   2112\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   2113\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 2114\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2115\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2117\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\_weights_only_unpickler.py:532\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     ):\n\u001b[0;32m    529\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[0;32m    530\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    531\u001b[0m         )\n\u001b[1;32m--> 532\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m    534\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\serialization.py:2078\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2077\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 2078\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2079\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2080\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\serialization.py:2044\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   2040\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mdetect_fake_mode(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2044\u001b[0m     wrap_storage \u001b[38;5;241m=\u001b[39m \u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2046\u001b[0m     storage\u001b[38;5;241m.\u001b[39m_fake_device \u001b[38;5;241m=\u001b[39m location\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\serialization.py:698\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 698\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\serialization.py:564\u001b[0m, in \u001b[0;36m_mps_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mps_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 564\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gianm\\miniforge3\\envs\\test\\Lib\\site-packages\\torch\\storage.py:268\u001b[0m, in \u001b[0;36m_StorageBase.mps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a MPS copy of this storage if it's not already on the MPS.\"\"\"\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.UntypedStorage(): Storage device not recognized: mps"
     ]
    }
   ],
   "source": [
    "PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "\n",
    "model_name = 'prova_autoencoders_dann'\n",
    "expected_rmse = '1000'\n",
    "\n",
    "step_prediction = 50 # step for prediction\n",
    "\n",
    "# preparing the training data\n",
    "tw_extractor = TimeWindowTransformer(size = 500, step = step_prediction)\n",
    "label_extractor = LabelWindowExtractor(size = 500, step = step_prediction)\n",
    "\n",
    "DATASET = 'guided'\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "DATASET = 'freemoves'\n",
    "\n",
    "XX = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "YY = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')\n",
    "\n",
    "XX = XX[..., -230000:]\n",
    "YY = YY[..., -230000:]\n",
    "\n",
    "X = np.vstack([X, XX])\n",
    "Y = np.vstack([Y, YY])\n",
    "\n",
    "X_windows = tw_extractor.transform(X)\n",
    "Y_labels = label_extractor.transform(Y)\n",
    "\n",
    "# Load Model and session labels (indices)\n",
    "train_session_ids = np.concatenate([np.full(X_windows[:9][s].shape[0], s) for s in range(9)])\n",
    "\n",
    "model = DANNModel(\n",
    "    lambda_grl=1.0,\n",
    "    num_domains=10,\n",
    "    output_dim=51,\n",
    ")\n",
    "\n",
    "# Wrap into scikit-learn-style regressor\n",
    "regressor = DANNRegressor(\n",
    "    model=model,\n",
    "    session_ids=train_session_ids,\n",
    "    dataset_class=DANNWindowTensor,\n",
    "    gamma_entropy=0.1,\n",
    "    max_epochs=50,\n",
    "    patience=10,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# encoding\n",
    "Y_labels_enc = encode(Y_labels)\n",
    "\n",
    "# stacking the sessions \n",
    "X_train = X_windows.reshape(-1, *X_windows.shape[2:])\n",
    "Y_train = Y_labels_enc.reshape(-1, *Y_labels_enc.shape[2:])\n",
    "\n",
    "# training\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "# # predicting\n",
    "# DATASET = 'guided'\n",
    "# X_test = np.load(PATH + f'{DATASET}/{DATASET}_testset_X.npy')\n",
    "# X_test = X_test.reshape(-1, *X_windows.shape[2:])\n",
    "# Y_pred_enc = model.predict(X_test)\n",
    "\n",
    "# # decoding\n",
    "# Y_pred_dec = decode(Y_pred_enc)\n",
    "\n",
    "# # saving\n",
    "# file_name = 'prova_encoded_decoded.npy'\n",
    "\n",
    "# np.save(file_name, Y_pred_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading predictions  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Y_pred_guided = np.load('prova_encoded_decoded.npy')\n",
    "Y_pred_freemoves = np.load('dann_steps_50_rmse_10.npy')\n",
    "\n",
    "# saving in csv file\n",
    "SUBMISSION_FOLDER = 'submissions/'\n",
    "fname_csv = SUBMISSION_FOLDER + 'submission_ibelieveinu9_rmse_' + expected_rmse_str + '.csv'\n",
    "\n",
    "Y_pred = np.vstack([Y_pred_guided, Y_pred_freemoves])\n",
    "Y_pred_df = pd.DataFrame(Y_pred)\n",
    "Y_pred_df.to_csv('best_with_autoencoders.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

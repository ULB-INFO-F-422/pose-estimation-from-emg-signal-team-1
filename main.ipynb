{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92cf27c",
   "metadata": {},
   "source": [
    "# Statistical foundation of machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prep import TimeWindowTransformer, LabelWindowExtractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib ipympl\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# adjust import if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d401a7",
   "metadata": {},
   "source": [
    "## Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e218cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading raw data\n",
    "PATH = f'/Users/marco/PROJECTS/data/'\n",
    "# PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "DATASET = 'guided' # change this to guided/freemoves if needed\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3adc425",
   "metadata": {},
   "source": [
    "## (Optional) Signal filtering\n",
    "\n",
    "if you plan to filter your sEMG signals, it is recommended to perform\n",
    "this preprocessing step directly on the continuous raw data prior to window extraction or feature\n",
    "computation. Note that this step is completely optional but may improve your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093990ea",
   "metadata": {},
   "source": [
    "## (0.5 point) Dataset preparation and augmentation through overlapping windows\n",
    "\n",
    "You should first segment your sEMG signals into smaller windows of fixed size k = 500. These windows should be created with a chosen degree of overlap, which you can adjust based on the computational and memory resources available to you. Keep in mind that a larger overlap results in a greater number of samples and thus a larger dataset to train your models but to the cost of increasing computational demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a311f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "size = 500\n",
    "step = 250\n",
    "\n",
    "# Initialize transformers\n",
    "tw_transformer = TimeWindowTransformer(size=size, step=step)\n",
    "label_extractor = LabelWindowExtractor(size=size, step=step)\n",
    "\n",
    "# Apply transformations\n",
    "X_windows = tw_transformer.transform(X)     # shape: (5, n_windows, 8, 500)\n",
    "Y_labels = label_extractor.transform(Y)     # shape: (5, n_windows, 51)\n",
    "\n",
    "# Inspect shapes\n",
    "print(\"X_windows shape:\", X_windows.shape)\n",
    "print(\"Y_labels shape:\", Y_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6921d1",
   "metadata": {},
   "source": [
    "## (1 point) Cross-validation strategy\n",
    "\n",
    "Determine and implement an adequate cross-validation strategy to validate your regression models, specifying how you organized your data partitions for training and validation. Provide a detailed justification showing that your validation sets remain completely independent from the training set. Include reasoning or evidence demonstrating explicitly that your chosen partitioning strategy prevents data leakage or bias, ensuring the reliability and generalizability of your model performance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_folds = X_windows[:4]\n",
    "Y_train_val_folds = Y_labels[:4]\n",
    "X_test = X_windows[4]\n",
    "Y_test = Y_labels[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261336d",
   "metadata": {},
   "source": [
    "## (3 points) Baseline approach\n",
    "\n",
    "Create a custom class inheriting from scikit-learnâ€™s `BaseEstimator`\n",
    "and `TransformerMixin` that implements the extraction of common time-domain features described\n",
    "in section 3.1. Note that the features described in Section 3.1 represent the minimal required set. We\n",
    "encourage you to include additional features or preprocessing steps if you would like to further improve your model performances. Select at least two different regression models, compare their cross-validated performance, and evaluate their feature importances. For both models, perform feature selection to determine the optimal subset of features minimizing the Root Mean Squared Error (RMSE).\n",
    "Clearly document this process in your notebook, discussing the outcomes in detail. Finally, create a\n",
    "scikit-learn `Pipeline` that integrates your custom feature extraction class, the optimal feature selection step, and the best-performing regression model identified from your cross-validation results.\n",
    "Using visualizations and tables to illustrate your findings, and employing formulas or pseudo-code\n",
    "to explain the feature selection procedure, is strongly encouraged. Note that one-third of the score\n",
    "will depend on the quality and clarity of your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90722601",
   "metadata": {},
   "source": [
    "**Disclaimer:** EVERY model/pipeline we create MUST take in input a numpy array of shape (N, 8, 500) and return a numpy array of shape (N, 51). The usage is:\n",
    "\n",
    "model.fit(X, y)  --> Here X is of shape (N, 8, 500), y is of shape (N, 51) ; we are not supposed to use the return of this method, the method changes the model in place.\n",
    "\n",
    "model.predict(X) --> Here X is of shape (N, 8, 500), and the return is of shape (N, 51)\n",
    "\n",
    "The transformers have a fit_transform() method, which is the only one used. They take (N, 8, 500) and return something else of shape (N,...) depending on which transformer is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824ad33",
   "metadata": {},
   "source": [
    "#### Baseline models - guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import TimeDomainTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from validation import RMSE, NMSE, cross_validate_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline1 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KernelRidge(\n",
    "            alpha = 0.001,\n",
    "            kernel='laplacian'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline2 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KNeighborsRegressor())\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline3 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "metric_fns = {'RMSE': RMSE, 'NMSE': NMSE}\n",
    "models = {\n",
    "    'Time domain features + Kernel Ridge': baseline1,\n",
    "    'Time domain features + KNN': baseline2,\n",
    "    'Time domain features + Random Forests': baseline3\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    result = cross_validate_pipeline(model, X_train_val_folds, Y_train_val_folds, metric_fns, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a5122",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "from validation import parameter_selection\n",
    "\n",
    "model = baseline1\n",
    "metric_fns = {'RMSE': RMSE, 'NMSE': NMSE}\n",
    "param_grid = {\n",
    "    'regressor__alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "    'regressor__kernel': ['rbf', 'linear', 'laplacian']\n",
    "}\n",
    "\n",
    "sweep_results = parameter_selection(model, param_grid, X_train_val_folds, Y_train_val_folds, metric_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = min(sweep_results, key=lambda d: d['avg_val_RMSE'])\n",
    "\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(best['params'])\n",
    "print(f\"Val RMSE: {best['avg_val_RMSE']:.4f}, Val NMSE: {best['avg_val_NMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a57768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "\n",
    "# Extract the alpha values and metrics\n",
    "alphas = [res['params']['regressor__alpha'] for res in sweep_results]\n",
    "rmse_vals = [res['avg_val_RMSE'] for res in sweep_results]\n",
    "nrmse_vals = [res['avg_val_NMSE'] for res in sweep_results]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogx(alphas, rmse_vals, marker='o')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.title('Validation RMSE vs Alpha')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogx(alphas, nrmse_vals, marker='o')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('Validation NRMSE')\n",
    "plt.title('Validation NRMSE vs Alpha')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b464e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Flatten and extract time domain features\n",
    "n_sessions, n_windows, n_channels, window_size = X_train_val_folds.shape\n",
    "X_flat = X_train_val_folds.reshape(-1, n_channels, window_size)\n",
    "y_flat = Y_train_val_folds.reshape(-1, 51)[:, 0]  # predict first joint for now\n",
    "\n",
    "# Apply time-domain transformer manually for feature selection\n",
    "td_transformer = TimeDomainTransformer()\n",
    "X_feat = td_transformer.transform(X_flat).reshape(X_flat.shape[0], -1)\n",
    "X_df = pd.DataFrame(X_feat)\n",
    "y_series = pd.Series(y_flat)\n",
    "n = X_df.shape[1]\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Mutual information approximation via correlation\n",
    "def mutual_info_corr(X, Y):\n",
    "    c = np.corrcoef(X, Y)[0, 1]\n",
    "    if abs(c) == 1:\n",
    "        c = 0.999999\n",
    "    return -0.5 * np.log(1 - c**2)\n",
    "\n",
    "def compute_mi_vector(X_tr, Y_tr):\n",
    "    mis = []\n",
    "    for col in X_tr.columns:\n",
    "        mi = mutual_info_corr(X_tr[col].values, Y_tr)\n",
    "        mis.append(mi)\n",
    "    return np.array(mis)\n",
    "\n",
    "# Correlation Ranking - ignores redundancy between features \n",
    "correlations = np.abs(X_df.corrwith(y_series))\n",
    "ranking_corr_idx = correlations.sort_values(ascending=False).index.tolist()\n",
    "CV_err_corr = np.zeros((n, 4))\n",
    "\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(X_df)):\n",
    "    X_tr, X_ts = X_df.iloc[train_index], X_df.iloc[test_index]\n",
    "    Y_tr, Y_ts = y_series[train_index], y_series[test_index]\n",
    "\n",
    "    for nb_features in range(1, n + 1):\n",
    "        selected_features = ranking_corr_idx[:nb_features]\n",
    "        model = KernelRidge()\n",
    "        model.fit(X_tr[selected_features], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[selected_features])\n",
    "        CV_err_corr[nb_features - 1, fold_id] = NMSE(Y_hat_ts, Y_ts)\n",
    "\n",
    "print(\"\\nCorrelation Ranking\")\n",
    "for i in range(n):\n",
    "    print(f\"#Features: {i+1}; CV error = {CV_err_corr[i,:].mean():.4f}; std dev = {CV_err_corr[i,:].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4de39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mRMR Ranking - requires mutual information computation\n",
    "CV_err_mrmr = np.zeros((n, 4))\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(X_df)):\n",
    "    X_tr, X_ts = X_df.iloc[train_index], X_df.iloc[test_index]\n",
    "    Y_tr, Y_ts = y_series[train_index], y_series[test_index]\n",
    "\n",
    "    mutual_info_values = compute_mi_vector(X_tr, Y_tr)\n",
    "    selected = []\n",
    "    candidates = list(range(n))\n",
    "\n",
    "    for j in range(n):\n",
    "        redundancy_score = np.zeros(len(candidates))\n",
    "        if len(selected) > 0:\n",
    "            for cidx in candidates:\n",
    "                col_c = X_tr.iloc[:, cidx]\n",
    "                mis_c = []\n",
    "                for sidx in selected:\n",
    "                    col_s = X_tr.iloc[:, sidx]\n",
    "                    cc = np.corrcoef(col_s, col_c)[0, 1]\n",
    "                    if abs(cc) == 1:\n",
    "                        cc = 0.999999\n",
    "                    mis_c.append(-0.5 * np.log(1 - cc**2))\n",
    "                redundancy_score[candidates.index(cidx)] = np.mean(mis_c)\n",
    "        mRMR_score = mutual_info_values[candidates] - redundancy_score\n",
    "        best_idx = candidates[np.argmax(mRMR_score)]\n",
    "        selected.append(best_idx)\n",
    "        candidates.remove(best_idx)\n",
    "\n",
    "    for nb_features in range(1, n + 1):\n",
    "        features_to_use = [X_df.columns[i] for i in selected[:nb_features]]\n",
    "        model = KernelRidge()\n",
    "        model.fit(X_tr[features_to_use], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[features_to_use])\n",
    "        CV_err_mrmr[nb_features - 1, fold_id] = NMSE(Y_hat_ts, Y_ts)\n",
    "\n",
    "print(\"\\nmRMR Ranking\")\n",
    "for i in range(n):\n",
    "    print(f\"#Features: {i+1}; CV error = {CV_err_mrmr[i,:].mean():.4f}; std dev = {CV_err_mrmr[i,:].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f39e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - produces unstructured components (not original features)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_df)\n",
    "CV_err_pca = np.zeros((n, 4))\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(X_pca)):\n",
    "    X_tr, X_ts = X_pca[train_index], X_pca[test_index]\n",
    "    Y_tr, Y_ts = y_series[train_index], y_series[test_index]\n",
    "\n",
    "    for nb_components in range(1, n + 1):\n",
    "        model = KernelRidge()\n",
    "        model.fit(X_tr[:, :nb_components], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[:, :nb_components])\n",
    "        CV_err_pca[nb_components - 1, fold_id] = NMSE(Y_hat_ts, Y_ts)\n",
    "\n",
    "print(\"\\nPCA Components\")\n",
    "for i in range(n):\n",
    "    print(f\"#Components: {i+1}; CV error = {CV_err_pca[i,:].mean():.4f}; std dev = {CV_err_pca[i,:].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb85ea",
   "metadata": {},
   "source": [
    "### Riemannian geometry pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ebcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyriemann\n",
    "import pyriemann.regression\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----- Riemannian geometry of covariance matrices ----- #\n",
    "riem1 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('regressor', KernelRidge(\n",
    "            kernel='laplacian'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem2 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('regressor', KNeighborsRegressor())\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem3 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('regressor', RandomForestRegressor())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08205eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "metric_fns = {'RMSE': RMSE, 'NMSE': NMSE}\n",
    "models = {\n",
    "    'riem1': riem1,\n",
    "    'riem2': riem2,\n",
    "    'riem3': riem3\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    result = cross_validate_pipeline(model, X_train_val_folds, Y_train_val_folds, metric_fns, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5e732",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a9328",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234b6e9",
   "metadata": {},
   "source": [
    "### Ensemble regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31beb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble import VotingRegressor\n",
    "\n",
    "ensemble_estimator = VotingRegressor(\n",
    "    estimators = [baseline2, riem2]\n",
    ")\n",
    "\n",
    "result = cross_validate_pipeline(ensemble_estimator, X_train_val_folds, Y_train_val_folds, metric_fns, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32c4c8",
   "metadata": {},
   "source": [
    "### Estimator validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a49a0",
   "metadata": {},
   "source": [
    "### Visualizing predictions to unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_flat = X_train_val_folds.reshape(-1, *X_train_val_folds.shape[2:])\n",
    "Y_train_val_flat = Y_train_val_folds.reshape(-1, *Y_train_val_folds.shape[2:])\n",
    "\n",
    "model = ensemble_estimator\n",
    "\n",
    "model.fit(X_train_val_flat, Y_train_val_flat)\n",
    "Y_train_pred = model.predict(X_train_val_flat)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"train RMSE:\\t{RMSE(Y_train_pred, Y_train_val_flat):.4f}\\ttrain NMSE:\\t{NMSE(Y_train_pred, Y_train_val_flat):.4f}\")\n",
    "print(f\"test RMSE:\\t{RMSE(Y_test_pred, Y_test):.4f}\\ttest NMSE:\\t{NMSE(Y_test_pred, Y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624edfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization_tools import scatter_3d_points\n",
    "\n",
    "bone = 6\n",
    "Y_true_bone = Y_test[:,3*bone:3*(bone+1)]\n",
    "Y_pred_bone = Y_test_pred[:,3*bone:3*(bone+1)]\n",
    "\n",
    "print(f'RMSE for bone {bone}:', RMSE(Y_pred_bone, Y_true_bone))\n",
    "print(f'NMSE for bone {bone}:', NMSE(Y_pred_bone, Y_true_bone))\n",
    "\n",
    "ax = scatter_3d_points(Y_true_bone, color = 'b')\n",
    "scatter_3d_points(Y_pred_bone, color = 'r', ax = ax)\n",
    "# ax.set_xlim3d(-50, 50)\n",
    "# ax.set_ylim3d(-50, 50)\n",
    "# ax.set_zlim3d(-50, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125f432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

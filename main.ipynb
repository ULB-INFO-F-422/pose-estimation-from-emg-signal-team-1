{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92cf27c",
   "metadata": {},
   "source": [
    "# Statistical foundation of machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e0a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prep import TimeWindowTransformer, LabelWindowExtractor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# adjust import if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d401a7",
   "metadata": {},
   "source": [
    "## Loading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e218cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading raw data\n",
    "PATH = f'/Users/marco/PROJECTS/data/'\n",
    "# PATH = r'C:\\Users\\gianm\\Documents\\Uni\\Big Data\\F422\\project\\data\\\\'\n",
    "DATASET = 'guided' # change this to guided/freemoves if needed\n",
    "\n",
    "X = np.load(PATH + f'{DATASET}/{DATASET}_dataset_X.npy')\n",
    "Y = np.load(PATH + f'{DATASET}/{DATASET}_dataset_Y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3adc425",
   "metadata": {},
   "source": [
    "## (Optional) Signal filtering\n",
    "\n",
    "if you plan to filter your sEMG signals, it is recommended to perform\n",
    "this preprocessing step directly on the continuous raw data prior to window extraction or feature\n",
    "computation. Note that this step is completely optional but may improve your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093990ea",
   "metadata": {},
   "source": [
    "## (0.5 point) Dataset preparation and augmentation through overlapping windows\n",
    "\n",
    "You should first segment your sEMG signals into smaller windows of fixed size k = 500. These windows should be created with a chosen degree of overlap, which you can adjust based on the computational and memory resources available to you. Keep in mind that a larger overlap results in a greater number of samples and thus a larger dataset to train your models but to the cost of increasing computational demands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ad92c",
   "metadata": {},
   "source": [
    "### \n",
    "To prepare our dataset for regression tasks, we segment the continuous surface electromyography (sEMG) signals into smaller overlapping windows of fixed size. This preprocessing step is crucial as it enables the learning models to capture localized temporal patterns in muscle activity and link them to corresponding hand pose estimations.\n",
    "\n",
    "We chose a **window size `k = 500` samples**, which corresponds to roughly **0.49 seconds** of sEMG data, given the sampling rate of **1024 Hz**. This window size strikes a balance between capturing enough signal dynamics and ensuring real-time usability for prosthesis control.\n",
    "\n",
    "We implemented a **50% overlap**, meaning each window starts 250 samples after the previous one. This overlap increases the number of training samples without excessively inflating computational costs. Larger overlaps (e.g., 75%) generate even more samples but demand significantly more memory and processing time, which may not scale efficiently depending on available resources.\n",
    "\n",
    "This approach is implemented using two custom transformer classes:\n",
    "- `TimeWindowTransformer`: Applies sliding windows to the raw sEMG signals.\n",
    "- `LabelWindowExtractor`: Aligns the corresponding joint angle labels by sampling at the end of each time window.\n",
    "\n",
    "This transformation ensures consistent alignment between each input window and its output label, facilitating supervised learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a311f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_windows shape: (5, 919, 8, 500)\n",
      "Y_labels shape: (5, 919, 51)\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "size = 500\n",
    "step = 250\n",
    "\n",
    "# Initialize transformers\n",
    "tw_transformer = TimeWindowTransformer(size=size, step=step)\n",
    "label_extractor = LabelWindowExtractor(size=size, step=step)\n",
    "\n",
    "# Apply transformations\n",
    "X_windows = tw_transformer.transform(X)     # shape: (5, n_windows, 8, 500)\n",
    "Y_labels = label_extractor.transform(Y)     # shape: (5, n_windows, 51)\n",
    "\n",
    "# Inspect shapes\n",
    "print(\"X_windows shape:\", X_windows.shape)\n",
    "print(\"Y_labels shape:\", Y_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6921d1",
   "metadata": {},
   "source": [
    "## (1 point) Cross-validation strategy\n",
    "\n",
    "Determine and implement an adequate cross-validation strategy to validate your regression models, specifying how you organized your data partitions for training and validation. Provide a detailed justification showing that your validation sets remain completely independent from the training set. Include reasoning or evidence demonstrating explicitly that your chosen partitioning strategy prevents data leakage or bias, ensuring the reliability and generalizability of your model performance estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988124e5",
   "metadata": {},
   "source": [
    "###\n",
    "To rigorously evaluate the performance and generalization capabilities of our regression models, we implemented a **cross-validation strategy** that ensures independence between training and validation data and avoids data leakage.\n",
    "\n",
    "The available data consists of 5 recording sessions. We use the first **4 sessions** (`X_train_val_folds`, `Y_train_val_folds`) for training and validation, and **reserve the 5th session** (`X_test`, `Y_test`) as a completely unseen test set. This separation allows us to simulate real-world scenarios where the model encounters entirely new data from the same participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d5c4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_folds = X_windows[:4]\n",
    "Y_train_val_folds = Y_labels[:4]\n",
    "X_test = X_windows[4]\n",
    "Y_test = Y_labels[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e89e9",
   "metadata": {},
   "source": [
    "Within the 4 training sessions, we assess the estimator performance using a by training it on 75% of the training set and evaluating it on the remaining 25%, representing a “holdout\" portion of the data that the model has never seen. To prevent potential bias due to specificities in portions of the training set, we use a stratified **4-fold cross-validation** and report the root mean squared error (**RMSE**) and normalized mean squared error (**NMSE**) across all. This strategy ensures:\n",
    "\n",
    "- **Complete independence** between training and validation data in each fold\n",
    "- Robust estimation of the model's performance across different recording sessions\n",
    "- Reduced risk of overfitting to session-specific artifacts\n",
    "\n",
    "This strategy provides insights into how well the model generalizes to other sessions from the same participant. However, it does not provide any information on the estimator’s ability to generalize to data from different participants.\n",
    "\n",
    "We implemented this strategy creating a 'cross_validate_pipeline' utility function to handle the fold-based evaluation of any sklearn-compatible pipeline:\n",
    "\n",
    "- Trains on 3 sessions and validates on the 4th (rotating fold)\n",
    "- Reports both training and validation scores for each fold\n",
    "- Computes mean performance across all folds for better interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261336d",
   "metadata": {},
   "source": [
    "## (3 points) Baseline approach\n",
    "\n",
    "Create a custom class inheriting from scikit-learn’s `BaseEstimator`\n",
    "and `TransformerMixin` that implements the extraction of common time-domain features described\n",
    "in section 3.1. Note that the features described in Section 3.1 represent the minimal required set. We\n",
    "encourage you to include additional features or preprocessing steps if you would like to further improve your model performances. Select at least two different regression models, compare their cross-validated performance, and evaluate their feature importances. For both models, perform feature selection to determine the optimal subset of features minimizing the Root Mean Squared Error (RMSE).\n",
    "Clearly document this process in your notebook, discussing the outcomes in detail. Finally, create a\n",
    "scikit-learn `Pipeline` that integrates your custom feature extraction class, the optimal feature selection step, and the best-performing regression model identified from your cross-validation results.\n",
    "Using visualizations and tables to illustrate your findings, and employing formulas or pseudo-code\n",
    "to explain the feature selection procedure, is strongly encouraged. Note that one-third of the score\n",
    "will depend on the quality and clarity of your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc0c86",
   "metadata": {},
   "source": [
    "###\n",
    "To establish a robust performance benchmark, we designed a baseline approach that uses standard time-domain features extracted from the raw sEMG signal windows. These features have been widely used in EMG-based regression problems due to their low computational cost and effectiveness in capturing muscle activity dynamics.\n",
    "\n",
    "We created a custom class `TimeDomainTransformer`, inheriting from `BaseEstimator` and `TransformerMixin`, to extract 12 key time-domain features from each EMG channel over a signal window of 500 samples.\n",
    "These features are computed along the time dimension and include:\n",
    "\n",
    "- **MAV** – Mean Absolute Value\n",
    "- **RMS** – Root Mean Square\n",
    "- **VAR** – Variance\n",
    "- **STD** – Standard Deviation\n",
    "- **ZC** – Zero Crossing Count\n",
    "- **MPR** – Myopulse Percentage Rate (using a tunable threshold `sigma_mpr`)\n",
    "- **MAA** – Maximum Absolute Amplitude\n",
    "- **WL** – Waveform Length\n",
    "- **SSC** – Slope Sign Changes\n",
    "- **WA** – Wilson Amplitude\n",
    "- **MFL** – Maximum Fractal Length\n",
    "- **KRT** – Kurtosis\n",
    "\n",
    "Each channel of the EMG window (8 in total) produces 12 features, resulting in a final feature vector of size **96** per window.\n",
    "\n",
    "We constructed three baseline pipelines using different regression algorithms to evaluate how well standard time-domain features can predict hand articulation from sEMG signals.\n",
    "\n",
    "Each pipeline follows the same general structure:\n",
    "\n",
    "1. **Feature Extraction**  \n",
    "   Extracts 12 physiological descriptors from each EMG channel using the custom `TimeDomainTransformer`.\n",
    "\n",
    "2. **Standardization**  \n",
    "   Normalizes the extracted features using `StandardScaler` to ensure each feature contributes equally to the regression model.\n",
    "\n",
    "3. **Regression Model**  \n",
    "   Each pipeline uses a different learning algorithm:\n",
    "   - **Kernel Ridge Regression** with a Laplacian kernel\n",
    "   - **K-Nearest Neighbors Regression**\n",
    "   - **Random Forest Regression**\n",
    "\n",
    "To improve the performance of each baseline model, we performed **manual hyperparameter tuning** using an exhaustive grid search strategy. This process is implemented in a dedicated notebook, `tuning_baseline.ipynb`, and applied independently to each pipeline.\n",
    "\n",
    "For each model, we defined a tailored parameter grid and evaluated all combinations using our cross-validation procedure. The objective is to identify the configuration that minimizes the **validation RMSE**, while also tracking **NMSE** for robustness.\n",
    "\n",
    "Below is a summary of the parameter grids explored:\n",
    "\n",
    "- **Time Domain Features + Kernel Ridge**\n",
    "  - `alpha`: regularization strength\n",
    "  - `gamma`: kernel coefficient\n",
    "  - `kernel`: type of kernel (`rbf`, `laplacian`, `poly`)\n",
    "\n",
    "- **Time Domain Features + KNN**\n",
    "  - `n_neighbors`: number of nearest neighbors\n",
    "  - `weights`: uniform or distance-based weighting\n",
    "\n",
    "- **Time Domain Features + Random Forest**\n",
    "  - `n_estimators`: number of trees in the forest\n",
    "  - `max_depth`: depth of each tree\n",
    "  - `min_samples_split`: minimum samples required to split a node\n",
    "  - `min_samples_leaf`: minimum samples at a leaf node\n",
    "\n",
    "Each model is tuned using the following loop:\n",
    "\n",
    "```python\n",
    "for model_name, pipeline in models.items():\n",
    "    print(f\"\\nTuning Model: {model_name}\") \n",
    "    results = []\n",
    "\n",
    "    for params in ParameterGrid(param_grids[model_name]):\n",
    "        pipeline.set_params(**params)\n",
    "        scores = cross_validate_pipeline(pipeline, X_train_val_folds, Y_train_val_folds, metric_fns)\n",
    "\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'mean_train_RMSE': scores['avg_train_RMSE'],\n",
    "            'mean_val_RMSE': scores['avg_val_RMSE'],\n",
    "            'mean_train_NMSE': scores['avg_train_NMSE'],\n",
    "            'mean_val_NMSE': scores['avg_val_NMSE']\n",
    "        })\n",
    "\n",
    "    all_results[model_name] = results\n",
    "```\n",
    "\n",
    "The results are saved in a seperate json file, allowing us to later analyze the best hyperparameter configuration per model. This tuning step ensures that our baselines are not only functional but also reasonably optimized for performance.\n",
    "\n",
    "> **DISCLAIMER**\n",
    "\n",
    "All models and pipelines developed in this project adhere to the following input/output shape requirements to ensure compatibility with the dataset and evaluation routines:\n",
    "\n",
    "- `model.fit(X, y)`  \n",
    "  - **Input:**  \n",
    "    - `X`: NumPy array of shape **(N, 8, 500)** — N windows of raw sEMG signals (8 channels, 500 samples each)  \n",
    "    - `y`: NumPy array of shape **(N, 51)** — Corresponding joint-angle labels\n",
    "  - **Note:** This method fits the model in place; the return value is not used.\n",
    "\n",
    "- `model.predict(X)`  \n",
    "  - **Input:** `X` of shape **(N, 8, 500)**  \n",
    "  - **Output:** NumPy array of shape **(N, 51)** — Predicted joint angles\n",
    "\n",
    "Custom transformers (e.g., for feature extraction) should implement the `fit_transform()` method:\n",
    "- **Input:** `X` of shape **(N, 8, 500)**\n",
    "- **Output:** Transformed feature array of shape **(N, ...)**, depending on the specific transformer logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824ad33",
   "metadata": {},
   "source": [
    "#### Baseline models - guided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85cd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import TimeDomainTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "from validation import RMSE, NMSE, cross_validate_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d21b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline1 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KernelRidge(\n",
    "            alpha = 0.001,\n",
    "            kernel='laplacian'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline2 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer(sigma_mpr=0.3)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', KNeighborsRegressor())\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline3 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', TimeDomainTransformer()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23d99d",
   "metadata": {},
   "source": [
    "To evaluate these baselines, we applied the **4-fold cross-validation** strategy described previously. For each model, we report both:\n",
    "\n",
    "- **Root Mean Squared Error (RMSE)**: Measures the average magnitude of prediction errors.\n",
    "- **Normalized Mean Squared Error (NMSE)**: Indicates performance relative to the variance of the true joint angles.\n",
    "\n",
    "This setup allows us to compare model performance on a consistent, unbiased basis and analyze the robustness of each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605a4a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Time domain features + Kernel Ridge\n",
      "\n",
      "Average Scores across folds:\n",
      "RMSE: train=0.0181, val=5.1703\n",
      "NMSE: train=0.0000, val=0.1351\n",
      "\n",
      "Model: Time domain features + KNN\n",
      "\n",
      "Average Scores across folds:\n",
      "RMSE: train=2.0971, val=4.7569\n",
      "NMSE: train=0.0219, val=0.1144\n",
      "\n",
      "Model: Time domain features + Random Forests\n",
      "\n",
      "Average Scores across folds:\n",
      "RMSE: train=1.2079, val=5.1408\n",
      "NMSE: train=0.0073, val=0.1354\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "metric_fns = {'RMSE': RMSE, 'NMSE': NMSE}\n",
    "models = {\n",
    "    'Time domain features + Kernel Ridge': baseline1,\n",
    "    'Time domain features + KNN': baseline2,\n",
    "    'Time domain features + Random Forests': baseline3\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    result = cross_validate_pipeline(model, X_train_val_folds, Y_train_val_folds, metric_fns, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950a054",
   "metadata": {},
   "source": [
    "### Save CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65456a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_to_csv\n",
    "\n",
    "# choose estimator\n",
    "estimator = baseline2\n",
    "\n",
    "# train on the first four sessions\n",
    "X_train = X_train_val_folds.reshape(-1, *X_train_val_folds.shape[2:])\n",
    "Y_train = Y_train_val_folds.reshape(-1, *Y_train_val_folds.shape[2:])\n",
    "\n",
    "estimator.fit(X_train, Y_train)\n",
    "\n",
    "# generate predictions on fifth fold\n",
    "Y_test_pred = estimator.predict(X_test)\n",
    "\n",
    "# save both ground truth and estimation to csv files\n",
    "save_to_csv(Y_test, 'Y_true.csv')\n",
    "save_to_csv(Y_test_pred, 'Y_predicted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98031411",
   "metadata": {},
   "source": [
    "#### Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b464e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Flatten and extract time domain features\n",
    "n_sessions, n_windows, n_channels, window_size = X_train_val_folds.shape\n",
    "X_flat = X_train_val_folds.reshape(-1, n_channels, window_size)\n",
    "y_flat = Y_train_val_folds.reshape(-1, 51)[:, 0]  # predict first joint for now\n",
    "\n",
    "# Apply time-domain transformer manually for feature selection\n",
    "td_transformer = TimeDomainTransformer()\n",
    "X_feat = td_transformer.transform(X_flat).reshape(X_flat.shape[0], -1)\n",
    "X_df = pd.DataFrame(X_feat)\n",
    "y_series = pd.Series(y_flat)\n",
    "n = X_df.shape[1]\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Mutual information approximation via correlation\n",
    "def mutual_info_corr(X, Y):\n",
    "    c = np.corrcoef(X, Y)[0, 1]\n",
    "    if abs(c) == 1:\n",
    "        c = 0.999999\n",
    "    return -0.5 * np.log(1 - c**2)\n",
    "\n",
    "def compute_mi_vector(X_tr, Y_tr):\n",
    "    mis = []\n",
    "    for col in X_tr.columns:\n",
    "        mi = mutual_info_corr(X_tr[col].values, Y_tr)\n",
    "        mis.append(mi)\n",
    "    return np.array(mis)\n",
    "\n",
    "# Correlation Ranking - ignores redundancy between features \n",
    "correlations = np.abs(X_df.corrwith(y_series))\n",
    "ranking_corr_idx = correlations.sort_values(ascending=False).index.tolist()\n",
    "CV_err_corr = np.zeros((n, 4))\n",
    "\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(X_df)):\n",
    "    X_tr, X_ts = X_df.iloc[train_index], X_df.iloc[test_index]\n",
    "    Y_tr, Y_ts = y_series[train_index], y_series[test_index]\n",
    "\n",
    "    for nb_features in range(1, n + 1):\n",
    "        selected_features = ranking_corr_idx[:nb_features]\n",
    "        model = KernelRidge()\n",
    "        model.fit(X_tr[selected_features], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[selected_features])\n",
    "        CV_err_corr[nb_features - 1, fold_id] = NMSE(Y_hat_ts, Y_ts)\n",
    "\n",
    "print(\"\\nCorrelation Ranking\")\n",
    "for i in range(n):\n",
    "    print(f\"#Features: {i+1}; CV error = {CV_err_corr[i,:].mean():.4f}; std dev = {CV_err_corr[i,:].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4de39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mRMR Ranking - requires mutual information computation\n",
    "CV_err_mrmr = np.zeros((n, 4))\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(X_df)):\n",
    "    X_tr, X_ts = X_df.iloc[train_index], X_df.iloc[test_index]\n",
    "    Y_tr, Y_ts = y_series[train_index], y_series[test_index]\n",
    "\n",
    "    mutual_info_values = compute_mi_vector(X_tr, Y_tr)\n",
    "    selected = []\n",
    "    candidates = list(range(n))\n",
    "\n",
    "    for j in range(n):\n",
    "        redundancy_score = np.zeros(len(candidates))\n",
    "        if len(selected) > 0:\n",
    "            for cidx in candidates:\n",
    "                col_c = X_tr.iloc[:, cidx]\n",
    "                mis_c = []\n",
    "                for sidx in selected:\n",
    "                    col_s = X_tr.iloc[:, sidx]\n",
    "                    cc = np.corrcoef(col_s, col_c)[0, 1]\n",
    "                    if abs(cc) == 1:\n",
    "                        cc = 0.999999\n",
    "                    mis_c.append(-0.5 * np.log(1 - cc**2))\n",
    "                redundancy_score[candidates.index(cidx)] = np.mean(mis_c)\n",
    "        mRMR_score = mutual_info_values[candidates] - redundancy_score\n",
    "        best_idx = candidates[np.argmax(mRMR_score)]\n",
    "        selected.append(best_idx)\n",
    "        candidates.remove(best_idx)\n",
    "\n",
    "    for nb_features in range(1, n + 1):\n",
    "        features_to_use = [X_df.columns[i] for i in selected[:nb_features]]\n",
    "        model = KernelRidge()\n",
    "        model.fit(X_tr[features_to_use], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[features_to_use])\n",
    "        CV_err_mrmr[nb_features - 1, fold_id] = NMSE(Y_hat_ts, Y_ts)\n",
    "\n",
    "print(\"\\nmRMR Ranking\")\n",
    "for i in range(n):\n",
    "    print(f\"#Features: {i+1}; CV error = {CV_err_mrmr[i,:].mean():.4f}; std dev = {CV_err_mrmr[i,:].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f39e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA - produces unstructured components (not original features)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_df)\n",
    "CV_err_pca = np.zeros((n, 4))\n",
    "for fold_id, (train_index, test_index) in enumerate(kf.split(X_pca)):\n",
    "    X_tr, X_ts = X_pca[train_index], X_pca[test_index]\n",
    "    Y_tr, Y_ts = y_series[train_index], y_series[test_index]\n",
    "\n",
    "    for nb_components in range(1, n + 1):\n",
    "        model = KernelRidge()\n",
    "        model.fit(X_tr[:, :nb_components], Y_tr)\n",
    "        Y_hat_ts = model.predict(X_ts[:, :nb_components])\n",
    "        CV_err_pca[nb_components - 1, fold_id] = NMSE(Y_hat_ts, Y_ts)\n",
    "\n",
    "print(\"\\nPCA Components\")\n",
    "for i in range(n):\n",
    "    print(f\"#Components: {i+1}; CV error = {CV_err_pca[i,:].mean():.4f}; std dev = {CV_err_pca[i,:].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb85ea",
   "metadata": {},
   "source": [
    "### Riemannian geometry pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5ebcbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyriemann\n",
    "import pyriemann.regression\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20eb30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riemannian geometry of covariance matrices\n",
    "riem1 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('regressor', KernelRidge(\n",
    "            kernel='laplacian'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem2 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('regressor', KNeighborsRegressor())\n",
    "    ]\n",
    ")\n",
    "\n",
    "riem3 = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', pyriemann.estimation.Covariances()),\n",
    "        ('transformation', pyriemann.tangentspace.TangentSpace(\n",
    "            metric = 'riemann',\n",
    "            tsupdate = True)),\n",
    "        ('regressor', RandomForestRegressor())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08205eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Riemann + Kernel Ridge\n",
      "\n",
      "Average Scores across folds:\n",
      "RMSE: train=4.1274, val=5.6207\n",
      "NMSE: train=0.0848, val=0.1591\n",
      "\n",
      "Model: Riemann + KNN\n",
      "\n",
      "Average Scores across folds:\n",
      "RMSE: train=2.1140, val=5.1833\n",
      "NMSE: train=0.0223, val=0.1358\n",
      "\n",
      "Model: Riemann + Random Forest\n",
      "\n",
      "Average Scores across folds:\n",
      "RMSE: train=1.3251, val=4.9036\n",
      "NMSE: train=0.0087, val=0.1230\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "metric_fns = {'RMSE': RMSE, 'NMSE': NMSE}\n",
    "models = {\n",
    "    'Riemann + Kernel Ridge': riem1,\n",
    "    'Riemann + KNN': riem2,\n",
    "    'Riemann + Random Forest': riem3\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f'\\nModel: {model_name}')\n",
    "    result = cross_validate_pipeline(model, X_train_val_folds, Y_train_val_folds, metric_fns, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc5e732",
   "metadata": {},
   "source": [
    "#### Features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f234b6e9",
   "metadata": {},
   "source": [
    "### Ensemble regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31beb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensemble import VotingRegressor\n",
    "\n",
    "ensemble_estimator = VotingRegressor(\n",
    "    estimators = [baseline2, riem2]\n",
    ")\n",
    "\n",
    "result = cross_validate_pipeline(ensemble_estimator, X_train_val_folds, Y_train_val_folds, metric_fns, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe32c4c8",
   "metadata": {},
   "source": [
    "### Estimator validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a49a0",
   "metadata": {},
   "source": [
    "### Visualizing predictions to unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3dc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_flat = X_train_val_folds.reshape(-1, *X_train_val_folds.shape[2:])\n",
    "Y_train_val_flat = Y_train_val_folds.reshape(-1, *Y_train_val_folds.shape[2:])\n",
    "\n",
    "model = ensemble_estimator\n",
    "\n",
    "model.fit(X_train_val_flat, Y_train_val_flat)\n",
    "Y_train_pred = model.predict(X_train_val_flat)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"train RMSE:\\t{RMSE(Y_train_pred, Y_train_val_flat):.4f}\\ttrain NMSE:\\t{NMSE(Y_train_pred, Y_train_val_flat):.4f}\")\n",
    "print(f\"test RMSE:\\t{RMSE(Y_test_pred, Y_test):.4f}\\ttest NMSE:\\t{NMSE(Y_test_pred, Y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624edfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization_tools import scatter_3d_points\n",
    "\n",
    "bone = 6\n",
    "Y_true_bone = Y_test[:,3*bone:3*(bone+1)]\n",
    "Y_pred_bone = Y_test_pred[:,3*bone:3*(bone+1)]\n",
    "\n",
    "print(f'RMSE for bone {bone}:', RMSE(Y_pred_bone, Y_true_bone))\n",
    "print(f'NMSE for bone {bone}:', NMSE(Y_pred_bone, Y_true_bone))\n",
    "\n",
    "ax = scatter_3d_points(Y_true_bone, color = 'b')\n",
    "scatter_3d_points(Y_pred_bone, color = 'r', ax = ax)\n",
    "# ax.set_xlim3d(-50, 50)\n",
    "# ax.set_ylim3d(-50, 50)\n",
    "# ax.set_zlim3d(-50, 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125f432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
